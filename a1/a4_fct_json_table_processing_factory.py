from pathlib import Path
import re
import time
from datetime import datetime, date
from multiprocessing import Pool, cpu_count, freeze_support
import calendar

import psycopg2
from psycopg2 import sql

# ============================================
# 0) ê¸°ë³¸ ê²½ë¡œ / DB ì„¤ì •
# ============================================

# NAS ê²½ë¡œ
BASE_LOG_DIR = Path(r"\\192.168.108.101\HistoryLog")

TC_FOLDERS = ["TC6", "TC7", "TC8", "TC9"]
TARGET_FOLDERS = ["GoodFile", "BadFile"]

DB_CONFIG = {
    "host": "192.168.108.162",
    "port": 5432,
    "dbname": "postgres",
    "user": "postgres",
    "password": "leejangwoo1!",
}

SCHEMA_NAME = "a4_fct_json_table_processing"
TABLE_NAME = "fct_json_table_processing"

USE_MULTIPROCESSING = True  # ë¬¸ì œ ìƒê¸°ë©´ False ë¡œ ë°”ê¿”ì„œ ë‹¨ì¼í”„ë¡œì„¸ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸

# ê³ ì • ìµœì†Œ ì‹œì‘ì¼ (2025-10-01 ì´ì „ í´ë”ëŠ” ì „ë¶€ ì œì™¸)
FIXED_START_DATE = date(2025, 10, 1)

# í•œ ë²ˆì— DBì— ë„£ì„ ìµœëŒ€ row ìˆ˜ (ë©”ëª¨ë¦¬ ìµœì í™”ìš©)
BATCH_SIZE_ROWS = 50000

# ì‹¤ì‹œê°„ìš©: ìµœê·¼ Nì´ˆ ì´ë‚´ ìˆ˜ì •ëœ íŒŒì¼ë§Œ ì²˜ë¦¬
REALTIME_LOOKBACK_SECONDS = 120  # ì˜ˆ: ìµœê·¼ 2ë¶„


# ============================================
# ë‚ ì§œ ìœˆë„ìš° ê³„ì‚°
# ============================================

def six_months_ago(d: date) -> date:
    """
    ì˜¤ëŠ˜ ê¸°ì¤€ 6ê°œì›” ì „ ë‚ ì§œ ê³„ì‚° (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ì°¸ê³ ìš©ìœ¼ë¡œ ë‚¨ê²¨ë‘ ).
    """
    year = d.year
    month = d.month - 6
    if month <= 0:
        year -= 1
        month += 12

    last_day = calendar.monthrange(year, month)[1]
    day = min(d.day, last_day)
    return date(year, month, day)


def get_window_dates():
    """
    ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 'ì´ë²ˆ ë‹¬ 1ì¼ ~ ì˜¤ëŠ˜' ë²”ìœ„ë¥¼ ë°˜í™˜.
    ì˜ˆ)
      - ì˜¤ëŠ˜ = 2025-12-04 â†’ 2025-12-01 ~ 2025-12-04
      - ì˜¤ëŠ˜ = 2025-12-31 â†’ 2025-12-01 ~ 2025-12-31
      - ì˜¤ëŠ˜ = 2026-01-01 â†’ 2026-01-01 ~ 2026-01-01

    FIXED_START_DATE ì´ì „ì€ ë¬´ì¡°ê±´ ì œì™¸.
    """
    today = date.today()

    # ì´ë²ˆ ë‹¬ 1ì¼
    month_start = today.replace(day=1)

    # ê³ ì • ì‹œì‘ì¼ ì´í›„ë§Œ
    window_start_date = max(month_start, FIXED_START_DATE)
    window_end_date = today
    return window_start_date, window_end_date


# ============================================
# 1) PostgreSQL ê´€ë ¨ í•¨ìˆ˜
# ============================================

def get_connection():
    """PostgreSQL ì»¤ë„¥ì…˜ ìƒì„±."""
    conn = psycopg2.connect(**DB_CONFIG)
    conn.autocommit = True
    return conn


def init_db(conn):
    """ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´)."""
    create_schema_sql = sql.SQL("CREATE SCHEMA IF NOT EXISTS {}").format(
        sql.Identifier(SCHEMA_NAME)
    )

    create_table_sql = sql.SQL("""
        CREATE TABLE IF NOT EXISTS {}.{} (
            id SERIAL PRIMARY KEY,
            file_path TEXT NOT NULL,
            station TEXT,
            barcode_information TEXT,
            step_description TEXT,
            value TEXT,
            min TEXT,
            max TEXT,
            result TEXT,
            created_at TIMESTAMP NOT NULL DEFAULT NOW()
        );
    """).format(sql.Identifier(SCHEMA_NAME), sql.Identifier(TABLE_NAME))

    with conn.cursor() as cur:
        cur.execute(create_schema_sql)
        cur.execute(create_table_sql)


def cleanup_old_data(conn, window_start_date: date):
    """
    window_start_date ì´ì „ DB ë°ì´í„° ì‚­ì œ (DELETE).
    created_at < window_start_date 00:00:00 ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ.

    â€» í˜„ì¬ process_onceì—ì„œëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ.
       í•„ìš” ì‹œ psql ë˜ëŠ” ë³„ë„ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥.
    """
    cutoff_dt = datetime.combine(window_start_date, datetime.min.time())

    delete_sql = sql.SQL("""
        DELETE FROM {}.{}
        WHERE created_at < %s
    """).format(sql.Identifier(SCHEMA_NAME), sql.Identifier(TABLE_NAME))

    with conn.cursor() as cur:
        cur.execute(delete_sql, (cutoff_dt,))
        deleted = cur.rowcount

    print(f"[ì •ë¦¬] window_start ì´ì „ DB ë°ì´í„° ì‚­ì œ ì™„ë£Œ (rows={deleted})")


def get_processed_file_paths(conn, window_start_date: date) -> set:
    """
    ì´ë¯¸ DBì— ì ì¬ëœ file_path ëª©ë¡(set) ì¡°íšŒ.
    - created_at >= window_start_date ê¸°ì¤€ìœ¼ë¡œë§Œ ì¡°íšŒí•´ì„œ
      ì˜¤ë˜ëœ ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ì œì™¸ (ìœˆë„ìš° ë‚´ ì¤‘ë³µë§Œ ë°©ì§€).
    """
    cutoff_dt = datetime.combine(window_start_date, datetime.min.time())

    query = sql.SQL("""
        SELECT DISTINCT file_path
        FROM {}.{}
        WHERE created_at >= %s
    """).format(
        sql.Identifier(SCHEMA_NAME),
        sql.Identifier(TABLE_NAME),
    )

    with conn.cursor() as cur:
        cur.execute(query, (cutoff_dt,))
        rows = cur.fetchall()
    return {r[0] for r in rows}


def insert_records(conn, records: list[dict]) -> int:
    """íŒŒì‹±ëœ ë ˆì½”ë“œë¥¼ DBì— INSERT."""
    if not records:
        return 0

    rows = []
    for r in records:
        rows.append((
            r.get("file_path", ""),
            r.get("Station", ""),
            r.get("Barcode information", ""),
            r.get("step_description", ""),
            r.get("value", ""),
            r.get("min", ""),
            r.get("max", ""),
            r.get("result", ""),
        ))

    insert_sql = sql.SQL("""
        INSERT INTO {}.{} (
            file_path,
            station,
            barcode_information,
            step_description,
            value,
            min,
            max,
            result
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """).format(sql.Identifier(SCHEMA_NAME), sql.Identifier(TABLE_NAME))

    with conn.cursor() as cur:
        cur.executemany(insert_sql, rows)

    return len(rows)


# ============================================
# 2) FCT ë¡œê·¸ íŒŒì‹±ìš© ì •ê·œì‹
# ============================================

STATION_PATTERN = re.compile(r"Station\s*:?\s*(\S+)", re.IGNORECASE)
BARCODE_PATTERN = re.compile(r"Barcode\s+information\s*:?\s*(.+)", re.IGNORECASE)
STEP_PATTERN = re.compile(
    r"^(?P<desc>.+?)\s*,\s*(?P<value>[^,]*),\s*(?P<min>[^,]*),\s*(?P<max>[^,]*),\s*(?P<result>\[[^\]]*\])"
)


def normalize_step_desc(desc: str) -> str:
    """step description: 2ê°œ ì´ìƒ ê³µë°± -> 1ê°œ, ì–‘ë ê³µë°± ì œê±°."""
    return " ".join(desc.split())


def parse_fct_file(file_path: Path) -> list[dict]:
    """
    FCT ë¡œê·¸ í•œ ê°œ íŒŒì¼ì„ ì½ì–´ì„œ
    JSON ë ˆì½”ë“œ(ë”•ì…”ë„ˆë¦¬) ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    key:
      - Station
      - Barcode information
      - step_description
      - value
      - min
      - max
      - result
      + DB ì¤‘ë³µ ì²´í¬ìš© file_path í¬í•¨.
    """
    try:
        with file_path.open("r", encoding="cp949", errors="ignore") as f:
            lines = [line.rstrip("\n") for line in f]
    except UnicodeDecodeError:
        with file_path.open("r", encoding="utf-8", errors="ignore") as f:
            lines = [line.rstrip("\n") for line in f]

    if not lines:
        return []

    station = None
    barcode = None

    # Station (3ë²ˆì§¸ ì¤„ ìš°ì„ )
    if len(lines) >= 3:
        m = STATION_PATTERN.search(lines[2])
        if m:
            station = m.group(1).strip()
    if station is None:
        for line in lines:
            m = STATION_PATTERN.search(line)
            if m:
                station = m.group(1).strip()
                break

    # Barcode information (5ë²ˆì§¸ ì¤„ ìš°ì„ )
    if len(lines) >= 5:
        m = BARCODE_PATTERN.search(lines[4])
        if m:
            barcode = m.group(1).strip()
    if barcode is None:
        for line in lines:
            m = BARCODE_PATTERN.search(line)
            if m:
                barcode = m.group(1).strip()
                break

    records = []
    for line in lines:
        m = STEP_PATTERN.match(line)
        if not m:
            continue

        desc_raw = m.group("desc")
        value_raw = m.group("value")
        min_raw = m.group("min")
        max_raw = m.group("max")
        result_raw = m.group("result")

        step_desc = normalize_step_desc(desc_raw)

        rec = {
            "file_path": str(file_path),
            "Station": station if station is not None else "",
            "Barcode information": barcode if barcode is not None else "",
            "step_description": step_desc,
            "value": str(value_raw).strip(),
            "min": str(min_raw).strip(),
            "max": str(max_raw).strip(),
            "result": str(result_raw).strip(),
        }
        records.append(rec)

    return records


# ============================================
# 3) íŒŒì¼ ìˆ˜ì§‘ (ë‚ ì§œ ìœˆë„ìš° + mtime í•„í„°)
# ============================================

def collect_fct_files(
    base_dir: Path,
    window_start_str: str,
    window_end_str: str,
    cutoff_ts: float,
) -> list[Path]:
    """
    TC6~9 / yyyymmdd / GoodFile/BadFile ì•„ë˜ì˜ ëª¨ë“  *.txt ìˆ˜ì§‘.
    - ë‚ ì§œ í´ë”ëŠ” window_start_str ~ window_end_str ë²”ìœ„ë§Œ ì²˜ë¦¬.
    - íŒŒì¼ mtimeì´ cutoff_ts (ìµœê·¼ REALTIME_LOOKBACK_SECONDSì´ˆ) ì´í›„ì¸ ê²½ìš°ë§Œ ëŒ€ìƒ.
    """
    file_list: list[Path] = []

    for tc in TC_FOLDERS:
        tc_path = base_dir / tc
        if not tc_path.exists():
            continue

        for date_dir in tc_path.iterdir():
            if not date_dir.is_dir():
                continue

            folder_name = date_dir.name.strip()
            # í´ë”ëª… yyyymmdd ê²€ì‚¬
            if not (folder_name.isdigit() and len(folder_name) == 8):
                continue

            # ë‚ ì§œ ìœˆë„ìš° ë²”ìœ„ ì²´í¬
            if not (window_start_str <= folder_name <= window_end_str):
                continue

            # GoodFile / BadFile
            for gb in TARGET_FOLDERS:
                target_dir = date_dir / gb
                if not target_dir.exists():
                    continue

                # .txt ìˆ˜ì§‘
                for f in target_dir.glob("*.txt"):
                    try:
                        if f.stat().st_mtime < cutoff_ts:
                            # ì‹¤ì‹œê°„ ìœˆë„ìš° ë°–ì´ë©´ ê±´ë„ˆëœ€
                            continue
                    except FileNotFoundError:
                        continue

                    file_list.append(f)

    return file_list


# ============================================
# 4) í•œ ë²ˆì˜ ì‚¬ì´í´ì—ì„œ í•  ì¼
# ============================================

def process_once():
    """
    í•œ ë²ˆ ì‚¬ì´í´:
      - ë‚ ì§œ ìœˆë„ìš°(ì´ë²ˆ ë‹¬ 1ì¼ ~ ì˜¤ëŠ˜) ì ìš©
      - ì‹¤ì‹œê°„ mtime ìœˆë„ìš° ì ìš© (ìµœê·¼ Nì´ˆ)
      - ì´ë¯¸ ì²˜ë¦¬ëœ file_path(ìœˆë„ìš° ë‚´ created_at ê¸°ì¤€) ì¡°íšŒ
      - ìƒˆ íŒŒì¼ë§Œ íŒŒì‹± â†’ ë°°ì¹˜ ë‹¨ìœ„ë¡œ DB ì ì¬
    """
    cycle_start = time.time()
    window_start_date, window_end_date = get_window_dates()
    window_start_str = window_start_date.strftime("%Y%m%d")
    window_end_str = window_end_date.strftime("%Y%m%d")

    now_ts = time.time()
    cutoff_ts = now_ts - REALTIME_LOOKBACK_SECONDS

    print("\n==============================================")
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] process_once ì‹œì‘")
    print(f"[ìœˆë„ìš°] í´ë”/ë°ì´í„° ìœ íš¨ ê¸°ê°„: {window_start_date} ~ {window_end_date}")
    print(f"[ì‹¤ì‹œê°„] ìµœê·¼ {REALTIME_LOOKBACK_SECONDS}ì´ˆ ì´ë‚´ ìˆ˜ì •ëœ íŒŒì¼ë§Œ ì²˜ë¦¬ (cutoff_ts={cutoff_ts})")

    conn = get_connection()
    try:
        init_db(conn)

        # âœ… ì‚­ì œëŠ” DB ì¸¡ì—ì„œ ë³„ë„ SQLë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥
        # cleanup_old_data(conn, window_start_date)

        # ìœˆë„ìš° ì´í›„(created_at >= window_start_date) ê¸°ì¤€ìœ¼ë¡œ,
        # ì´ë¯¸ ì²˜ë¦¬ëœ file_path ëª©ë¡
        processed_files = get_processed_file_paths(conn, window_start_date)

        # ì „ì²´ íŒŒì¼ ìŠ¤ìº” (ë‚ ì§œ ìœˆë„ìš° + mtime ìœˆë„ìš° ì ìš©)
        all_files = collect_fct_files(BASE_LOG_DIR, window_start_str, window_end_str, cutoff_ts)
        all_files_str = [str(p) for p in all_files]

        new_files = [Path(p) for p in all_files_str if p not in processed_files]

        print(f"  ì´ íŒŒì¼ ìˆ˜(í´ë”+mtime ìœˆë„ìš° ë‚´): {len(all_files)}ê°œ")
        print(f"  ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜(DB, created_at>=ìœˆë„ìš°): {len(processed_files)}ê°œ")
        print(f"  ì´ë²ˆì— ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(new_files)}ê°œ")

        if not new_files:
            print("  ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼ ì—†ìŒ. ì‚¬ì´í´ ì¢…ë£Œ.")
            return  # ìƒˆ íŒŒì¼ ì—†ìœ¼ë©´ ë

        total_inserted_rows = 0
        batch_records: list[dict] = []

        if USE_MULTIPROCESSING:
            # ğŸ”¥ ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ë¥¼ í•­ìƒ 4ê°œë¡œ ê³ ì •
            n_proc = 4
            print(f"  ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©: í”„ë¡œì„¸ìŠ¤ {n_proc}ê°œ")

            with Pool(processes=n_proc) as pool:
                for idx, recs in enumerate(
                    pool.imap_unordered(parse_fct_file, new_files, chunksize=10), start=1
                ):
                    if recs:
                        batch_records.extend(recs)

                    # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ DB INSERT
                    if len(batch_records) >= BATCH_SIZE_ROWS:
                        inserted = insert_records(conn, batch_records)
                        total_inserted_rows += inserted
                        print(
                            f"    â†’ ë°°ì¹˜ INSERT (rows={inserted}, ëˆ„ì  rows={total_inserted_rows}) "
                            f" at file {idx}/{len(new_files)}"
                        )
                        batch_records.clear()

                    if idx % 1000 == 0 or idx == len(new_files):
                        print(f"    â†’ í˜„ì¬ {idx}/{len(new_files)} íŒŒì¼ íŒŒì‹± ì™„ë£Œ")

        else:
            print("  ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬ (USE_MULTIPROCESSING = False)")
            for idx, f in enumerate(new_files, start=1):
                recs = parse_fct_file(f)
                if recs:
                    batch_records.extend(recs)

                if len(batch_records) >= BATCH_SIZE_ROWS:
                    inserted = insert_records(conn, batch_records)
                    total_inserted_rows += inserted
                    print(
                        f"    â†’ ë°°ì¹˜ INSERT (rows={inserted}, ëˆ„ì  rows={total_inserted_rows}) "
                        f" at file {idx}/{len(new_files)}"
                    )
                    batch_records.clear()

                if idx % 1000 == 0 or idx == len(new_files):
                    print(f"    â†’ í˜„ì¬ {idx}/{len(new_files)} íŒŒì¼ íŒŒì‹± ì™„ë£Œ")

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch_records:
            inserted = insert_records(conn, batch_records)
            total_inserted_rows += inserted
            print(
                f"  ë§ˆì§€ë§‰ ë°°ì¹˜ INSERT (rows={inserted}, ëˆ„ì  rows={total_inserted_rows})"
            )

        cycle_end = time.time()
        print(f"  ì´ INSERTëœ ë ˆì½”ë“œ ìˆ˜: {total_inserted_rows}ê°œ")
        print(f"  DB ì ì¬ ì™„ë£Œ. (ì‚¬ì´í´ ì†Œìš” ì‹œê°„: {cycle_end - cycle_start:.1f}ì´ˆ)")

    finally:
        conn.close()
        print("process_once ì¢…ë£Œ")
        print("==============================================\n")


# ============================================
# 5) ë©”ì¸ ë£¨í”„ (1ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰)
# ============================================

def main_loop():
    print("=== a4_fct_json_table_processing ì‹œì‘ (1ì´ˆë§ˆë‹¤ í´ë§) ===")
    print(f"ê¸°ë³¸ ë¡œê·¸ ê²½ë¡œ: {BASE_LOG_DIR}")
    print(f"DB: {DB_CONFIG['host']}:{DB_CONFIG['port']} / {DB_CONFIG['dbname']} (user={DB_CONFIG['user']})")

    while True:
        try:
            process_once()
        except Exception as e:
            print(f"[ì—ëŸ¬ ë°œìƒ] {e}")
        # 1ì´ˆ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹¤í–‰
        time.sleep(1)


if __name__ == "__main__":
    freeze_support()  # ìœˆë„ìš° / exe ë³€í™˜ ì‹œ ì•ˆì „
    main_loop()
