from pathlib import Path
import re
import time
from datetime import datetime, date
from multiprocessing import Pool, cpu_count, freeze_support
import calendar

import psycopg2
from psycopg2 import sql

# ============================================
# 0) ê¸°ë³¸ ê²½ë¡œ / DB ì„¤ì •
# ============================================

# NAS ê²½ë¡œ
BASE_LOG_DIR = Path(r"\\192.168.108.101\HistoryLog")

TC_FOLDERS = ["TC6", "TC7", "TC8", "TC9"]
TARGET_FOLDERS = ["GoodFile", "BadFile"]

DB_CONFIG = {
    "host": "192.168.108.162",
    "port": 5432,
    "dbname": "postgres",
    "user": "postgres",
    "password": "leejangwoo1!",
}

SCHEMA_NAME = "a4_fct_json_table_processing"
TABLE_NAME = "fct_json_table_processing"

USE_MULTIPROCESSING = True  # ë¬¸ì œ ìƒê¸°ë©´ False ë¡œ ë°”ê¿”ì„œ ë‹¨ì¼í”„ë¡œì„¸ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸

# ê³ ì • ìµœì†Œ ì‹œì‘ì¼ (2025-10-01 ì´ì „ í´ë”ëŠ” ì „ë¶€ ì œì™¸)
FIXED_START_DATE = date(2025, 10, 1)

# í•œ ë²ˆì— DBì— ë„£ì„ ìµœëŒ€ row ìˆ˜ (ë©”ëª¨ë¦¬ ìµœì í™”ìš©)
BATCH_SIZE_ROWS = 50000


# ============================================
# ë‚ ì§œ ìœˆë„ìš° ê³„ì‚°
# ============================================

def six_months_ago(d: date) -> date:
    """
    ì˜¤ëŠ˜ ê¸°ì¤€ 6ê°œì›” ì „ ë‚ ì§œ ê³„ì‚° (relativedelta ì—†ì´ ì§ì ‘ êµ¬í˜„).
    """
    year = d.year
    month = d.month - 6
    if month <= 0:
        year -= 1
        month += 12

    last_day = calendar.monthrange(year, month)[1]
    day = min(d.day, last_day)
    return date(year, month, day)


def get_window_dates():
    """
    - today: ì˜¤ëŠ˜
    - window_start_date: max(FIXED_START_DATE, today-6ê°œì›”)
    - window_end_date: today

    ì˜ˆ)
      ì²˜ìŒì—” 2025-10-01 ~ ì˜¤ëŠ˜
      ì‹œê°„ì´ ì§€ë‚˜ì„œ today-6ê°œì›”ì´ 2026-02-02ë¼ë©´ â†’ 2026-02-02 ~ today
    """
    today = date.today()
    six_before = six_months_ago(today)
    window_start_date = max(FIXED_START_DATE, six_before)
    window_end_date = today
    return window_start_date, window_end_date


# ============================================
# 1) PostgreSQL ê´€ë ¨ í•¨ìˆ˜
# ============================================

def get_connection():
    """PostgreSQL ì»¤ë„¥ì…˜ ìƒì„±."""
    conn = psycopg2.connect(**DB_CONFIG)
    conn.autocommit = True
    return conn


def init_db(conn):
    """ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´)."""
    create_schema_sql = sql.SQL("CREATE SCHEMA IF NOT EXISTS {}").format(
        sql.Identifier(SCHEMA_NAME)
    )

    create_table_sql = sql.SQL("""
        CREATE TABLE IF NOT EXISTS {}.{} (
            id SERIAL PRIMARY KEY,
            file_path TEXT NOT NULL,
            station TEXT,
            barcode_information TEXT,
            step_description TEXT,
            value TEXT,
            min TEXT,
            max TEXT,
            result TEXT,
            created_at TIMESTAMP NOT NULL DEFAULT NOW()
        );
    """).format(sql.Identifier(SCHEMA_NAME), sql.Identifier(TABLE_NAME))

    with conn.cursor() as cur:
        cur.execute(create_schema_sql)
        cur.execute(create_table_sql)


def cleanup_old_data(conn, window_start_date: date):
    """
    í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ 6ê°œì›” ì´ìƒëœ DB ë°ì´í„° ì‚­ì œ (DELETE).
    created_at < window_start_date 00:00:00 ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ.
    """
    cutoff_dt = datetime.combine(window_start_date, datetime.min.time())

    delete_sql = sql.SQL("""
        DELETE FROM {}.{}
        WHERE created_at < %s
    """).format(sql.Identifier(SCHEMA_NAME), sql.Identifier(TABLE_NAME))

    with conn.cursor() as cur:
        cur.execute(delete_sql, (cutoff_dt,))
        deleted = cur.rowcount

    print(f"[ì •ë¦¬] 6ê°œì›” ì´ì „ DB ë°ì´í„° ì‚­ì œ ì™„ë£Œ (rows={deleted})")


def get_processed_file_paths(conn) -> set:
    """ì´ë¯¸ DBì— ì ì¬ëœ file_path ëª©ë¡(set) ì¡°íšŒ."""
    query = sql.SQL("SELECT DISTINCT file_path FROM {}.{}").format(
        sql.Identifier(SCHEMA_NAME),
        sql.Identifier(TABLE_NAME),
    )
    with conn.cursor() as cur:
        cur.execute(query)
        rows = cur.fetchall()
    return {r[0] for r in rows}


def insert_records(conn, records: list[dict]) -> int:
    """íŒŒì‹±ëœ ë ˆì½”ë“œë¥¼ DBì— INSERT."""
    if not records:
        return 0

    rows = []
    for r in records:
        rows.append((
            r.get("file_path", ""),
            r.get("Station", ""),
            r.get("Barcode information", ""),
            r.get("step_description", ""),
            r.get("value", ""),
            r.get("min", ""),
            r.get("max", ""),
            r.get("result", ""),
        ))

    insert_sql = sql.SQL("""
        INSERT INTO {}.{} (
            file_path,
            station,
            barcode_information,
            step_description,
            value,
            min,
            max,
            result
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """).format(sql.Identifier(SCHEMA_NAME), sql.Identifier(TABLE_NAME))

    with conn.cursor() as cur:
        cur.executemany(insert_sql, rows)

    return len(rows)


# ============================================
# 2) FCT ë¡œê·¸ íŒŒì‹±ìš© ì •ê·œì‹
# ============================================

STATION_PATTERN = re.compile(r"Station\s*:?\s*(\S+)", re.IGNORECASE)
BARCODE_PATTERN = re.compile(r"Barcode\s+information\s*:?\s*(.+)", re.IGNORECASE)
STEP_PATTERN = re.compile(
    r"^(?P<desc>.+?)\s*,\s*(?P<value>[^,]*),\s*(?P<min>[^,]*),\s*(?P<max>[^,]*),\s*(?P<result>\[[^\]]*\])"
)


def normalize_step_desc(desc: str) -> str:
    """step description: 2ê°œ ì´ìƒ ê³µë°± -> 1ê°œ, ì–‘ë ê³µë°± ì œê±°."""
    return " ".join(desc.split())


def parse_fct_file(file_path: Path) -> list[dict]:
    """
    FCT ë¡œê·¸ í•œ ê°œ íŒŒì¼ì„ ì½ì–´ì„œ
    JSON ë ˆì½”ë“œ(ë”•ì…”ë„ˆë¦¬) ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    key:
      - Station
      - Barcode information
      - step_description
      - value
      - min
      - max
      - result
      + DB ì¤‘ë³µ ì²´í¬ìš© file_path í¬í•¨.
    """
    try:
        with file_path.open("r", encoding="cp949", errors="ignore") as f:
            lines = [line.rstrip("\n") for line in f]
    except UnicodeDecodeError:
        with file_path.open("r", encoding="utf-8", errors="ignore") as f:
            lines = [line.rstrip("\n") for line in f]

    if not lines:
        return []

    station = None
    barcode = None

    # Station (3ë²ˆì§¸ ì¤„ ìš°ì„ )
    if len(lines) >= 3:
        m = STATION_PATTERN.search(lines[2])
        if m:
            station = m.group(1).strip()
    if station is None:
        for line in lines:
            m = STATION_PATTERN.search(line)
            if m:
                station = m.group(1).strip()
                break

    # Barcode information (5ë²ˆì§¸ ì¤„ ìš°ì„ )
    if len(lines) >= 5:
        m = BARCODE_PATTERN.search(lines[4])
        if m:
            barcode = m.group(1).strip()
    if barcode is None:
        for line in lines:
            m = BARCODE_PATTERN.search(line)
            if m:
                barcode = m.group(1).strip()
                break

    records = []
    for line in lines:
        m = STEP_PATTERN.match(line)
        if not m:
            continue

        desc_raw = m.group("desc")
        value_raw = m.group("value")
        min_raw = m.group("min")
        max_raw = m.group("max")
        result_raw = m.group("result")

        step_desc = normalize_step_desc(desc_raw)

        rec = {
            "file_path": str(file_path),
            "Station": station if station is not None else "",
            "Barcode information": barcode if barcode is not None else "",
            "step_description": step_desc,
            "value": str(value_raw).strip(),
            "min": str(min_raw).strip(),
            "max": str(max_raw).strip(),
            "result": str(result_raw).strip(),
        }
        records.append(rec)

    return records


# ============================================
# 3) íŒŒì¼ ìˆ˜ì§‘ (ë‚ ì§œ ìœˆë„ìš° ì ìš©)
# ============================================

def collect_fct_files(base_dir: Path, window_start_str: str, window_end_str: str) -> list[Path]:
    """
    TC6~9 / yyyymmdd / GoodFile/BadFile ì•„ë˜ì˜ ëª¨ë“  *.txt ìˆ˜ì§‘.
    ë‚ ì§œ í´ë”ëŠ” window_start_str ~ window_end_str ë²”ìœ„ë§Œ ì²˜ë¦¬.
    """
    file_list: list[Path] = []

    for tc in TC_FOLDERS:
        tc_path = base_dir / tc
        if not tc_path.exists():
            continue

        for date_dir in tc_path.iterdir():
            if not date_dir.is_dir():
                continue

            folder_name = date_dir.name.strip()
            # í´ë”ëª… yyyymmdd ê²€ì‚¬
            if not (folder_name.isdigit() and len(folder_name) == 8):
                continue

            # ë‚ ì§œ ìœˆë„ìš° ë²”ìœ„ ì²´í¬
            if not (window_start_str <= folder_name <= window_end_str):
                continue

            # GoodFile / BadFile
            for gb in TARGET_FOLDERS:
                target_dir = date_dir / gb
                if not target_dir.exists():
                    continue

                # .txt ìˆ˜ì§‘
                for f in target_dir.glob("*.txt"):
                    file_list.append(f)

    return file_list


# ============================================
# 4) í•œ ë²ˆì˜ ì‚¬ì´í´ì—ì„œ í•  ì¼
# ============================================

def process_once():
    """í•œ ë²ˆ ì‚¬ì´í´: ë‚ ì§œ ìœˆë„ìš° ì ìš© â†’ DB ì •ë¦¬ â†’ ì¤‘ë³µ file_path í™•ì¸ â†’ ìƒˆ íŒŒì¼ íŒŒì‹± â†’ ë°°ì¹˜ DB ì ì¬."""
    window_start_date, window_end_date = get_window_dates()
    window_start_str = window_start_date.strftime("%Y%m%d")
    window_end_str = window_end_date.strftime("%Y%m%d")

    print("\n==============================================")
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] process_once ì‹œì‘")
    print(f"[ìœˆë„ìš°] í´ë”/ë°ì´í„° ìœ íš¨ ê¸°ê°„: {window_start_date} ~ {window_end_date}")

    conn = get_connection()
    try:
        init_db(conn)

        # 6ê°œì›” ì´ì „ DB ë°ì´í„° ì •ë¦¬
        cleanup_old_data(conn, window_start_date)

        # ì •ë¦¬ í›„, ì´ë¯¸ ì²˜ë¦¬ëœ file_path ëª©ë¡
        processed_files = get_processed_file_paths(conn)

        # ì „ì²´ íŒŒì¼ ìŠ¤ìº” (ë‚ ì§œ ìœˆë„ìš° ì ìš©)
        all_files = collect_fct_files(BASE_LOG_DIR, window_start_str, window_end_str)
        all_files_str = [str(p) for p in all_files]

        new_files = [Path(p) for p in all_files_str if p not in processed_files]

        print(f"  ì´ íŒŒì¼ ìˆ˜(ìœˆë„ìš° ë‚´): {len(all_files)}ê°œ")
        print(f"  ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜(DB): {len(processed_files)}ê°œ")
        print(f"  ì´ë²ˆì— ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(new_files)}ê°œ")

        if not new_files:
            print("  ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼ ì—†ìŒ. ì‚¬ì´í´ ì¢…ë£Œ.")
            return  # ìƒˆ íŒŒì¼ ì—†ìœ¼ë©´ ë

        total_inserted_rows = 0
        batch_records: list[dict] = []

        if USE_MULTIPROCESSING:
            # ğŸ”¥ ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ë¥¼ í•­ìƒ 2ê°œë¡œ ê³ ì •
            n_proc = 2
            print(f"  ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©: í”„ë¡œì„¸ìŠ¤ {n_proc}ê°œ")

            with Pool(processes=n_proc) as pool:
                for idx, recs in enumerate(
                    pool.imap_unordered(parse_fct_file, new_files, chunksize=10), start=1
                ):
                    if recs:
                        batch_records.extend(recs)

                    # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ DB INSERT
                    if len(batch_records) >= BATCH_SIZE_ROWS:
                        inserted = insert_records(conn, batch_records)
                        total_inserted_rows += inserted
                        print(
                            f"    â†’ ë°°ì¹˜ INSERT (rows={inserted}, ëˆ„ì  rows={total_inserted_rows}) "
                            f" at file {idx}/{len(new_files)}"
                        )
                        batch_records.clear()

                    if idx % 1000 == 0 or idx == len(new_files):
                        print(f"    â†’ í˜„ì¬ {idx}/{len(new_files)} íŒŒì¼ íŒŒì‹± ì™„ë£Œ")

        else:
            print("  ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬ (USE_MULTIPROCESSING = False)")
            for idx, f in enumerate(new_files, start=1):
                recs = parse_fct_file(f)
                if recs:
                    batch_records.extend(recs)

                if len(batch_records) >= BATCH_SIZE_ROWS:
                    inserted = insert_records(conn, batch_records)
                    total_inserted_rows += inserted
                    print(
                        f"    â†’ ë°°ì¹˜ INSERT (rows={inserted}, ëˆ„ì  rows={total_inserted_rows}) "
                        f" at file {idx}/{len(new_files)}"
                    )
                    batch_records.clear()

                if idx % 1000 == 0 or idx == len(new_files):
                    print(f"    â†’ í˜„ì¬ {idx}/{len(new_files)} íŒŒì¼ íŒŒì‹± ì™„ë£Œ")

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch_records:
            inserted = insert_records(conn, batch_records)
            total_inserted_rows += inserted
            print(
                f"  ë§ˆì§€ë§‰ ë°°ì¹˜ INSERT (rows={inserted}, ëˆ„ì  rows={total_inserted_rows})"
            )

        print(f"  ì´ INSERTëœ ë ˆì½”ë“œ ìˆ˜: {total_inserted_rows}ê°œ")
        print("  DB ì ì¬ ì™„ë£Œ.")

    finally:
        conn.close()
        print("process_once ì¢…ë£Œ")
        print("==============================================\n")


# ============================================
# 5) ë©”ì¸ ë£¨í”„ (1ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰)
# ============================================

def main_loop():
    print("=== a4_fct_json_table_processing ì‹œì‘ (1ì´ˆë§ˆë‹¤ í´ë§) ===")
    print(f"ê¸°ë³¸ ë¡œê·¸ ê²½ë¡œ: {BASE_LOG_DIR}")
    print(f"DB: {DB_CONFIG['host']}:{DB_CONFIG['port']} / {DB_CONFIG['dbname']} (user={DB_CONFIG['user']})")

    while True:
        try:
            process_once()
        except Exception as e:
            print(f"[ì—ëŸ¬ ë°œìƒ] {e}")
        # 1ì´ˆ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹¤í–‰
        time.sleep(1)


if __name__ == "__main__":
    freeze_support()  # ìœˆë„ìš° / exe ë³€í™˜ ì‹œ ì•ˆì „
    main_loop()
