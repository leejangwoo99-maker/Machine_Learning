import re
import time
import calendar
from pathlib import Path
from datetime import datetime, date
from concurrent.futures import ProcessPoolExecutor, as_completed

import psycopg2
import psycopg2.extras

# ==============================
# ì„¤ì • ì˜ì—­
# ==============================

# ê¸°ë³¸ ë¡œê·¸ ê²½ë¡œ ì„¤ì • (HistoryLog NAS)
BASE_LOG_DIR = Path(r"\\192.168.108.101\HistoryLog")  # Z: ë“œë¼ì´ë¸Œ ì›ë³¸ ê²½ë¡œ
# BASE_LOG_DIR = Path(r"C:\Users\user\Desktop\RAW_LOG")  # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©

# ì¤‘ê°„ í´ë”
MIDDLE_FOLDERS = ["TC6", "TC7", "TC8", "TC9", "Vision03"]

# ë‚ ì§œ í´ë” ì•„ë˜ì—ì„œ ìš°ì„  íƒìƒ‰í•  ì„œë¸Œí´ë” (ì—†ìœ¼ë©´ ë‚ ì§œ í´ë” ë°”ë¡œ ë°‘ íŒŒì¼ë„ ìŠ¤ìº”)
TARGET_FOLDERS = ["GoodFile", "BadFile"]

# TC6~9 â†’ FCT1~4 ë§¤í•‘
TC_TO_FCT = {
    "TC6": "FCT1",
    "TC7": "FCT2",
    "TC8": "FCT3",
    "TC9": "FCT4",
}

# PostgreSQL ì ‘ì† ì •ë³´ (NAS DB)
DB_CONFIG = {
    "host": "192.168.108.162",
    "port": 5432,
    "dbname": "postgres",
    "user": "postgres",
    "password": "leejangwoo1!",
}

SCHEMA_NAME = "a2_fct_vision_testlog_json_processing"
TABLE_NAME = "fct_vision_testlog_json_processing"

# ê³ ì • ìµœì†Œ ì‹œì‘ì¼ (ì´ ë‚ ì§œ ì´ì „ í´ë”ëŠ” ë¬´ì¡°ê±´ ì œì™¸)
FIXED_START_DATE = date(2025, 10, 1)

# í•œ ë²ˆì— íŒŒì‹± + INSERT í•  ìµœëŒ€ íŒŒì¼ ìˆ˜ (ë©”ëª¨ë¦¬ ìµœì í™”ìš©)
BATCH_SIZE = 10000

# ==============================
# ë‚ ì§œ ìœˆë„ìš° ê³„ì‚°
# ==============================

def six_months_ago(d: date) -> date:
    """
    ì˜¤ëŠ˜ ê¸°ì¤€ 6ê°œì›” ì „ ë‚ ì§œ ê³„ì‚° (relativedelta ì—†ì´ ì§ì ‘ êµ¬í˜„).
    """
    year = d.year
    month = d.month - 6
    if month <= 0:
        year -= 1
        month += 12

    last_day = calendar.monthrange(year, month)[1]
    day = min(d.day, last_day)
    return date(year, month, day)


def get_window_dates():
    """
    - today: ì˜¤ëŠ˜
    - window_start_date: max(FIXED_START_DATE, today-6ê°œì›”)
    - window_end_date: today

    => ì´ ë²”ìœ„ì— ë“¤ì–´ì˜¤ëŠ” yyyymmdd í´ë”ë§Œ íŒŒì‹± + DB ìœ ì§€
    """
    today = date.today()
    six_before = six_months_ago(today)
    window_start_date = max(FIXED_START_DATE, six_before)
    window_end_date = today
    return window_start_date, window_end_date

# ==============================
# ê³µí†µ ìœ í‹¸ í•¨ìˆ˜
# ==============================

def read_text_file(path: Path):
    """í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ ì¤„ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (ì¸ì½”ë”© ìë™ ì²˜ë¦¬ ì‹œë„)."""
    for enc in ("cp949", "utf-8-sig", "utf-8"):
        try:
            with path.open("r", encoding=enc, errors="replace") as f:
                return [line.rstrip("\n\r") for line in f]
        except UnicodeDecodeError:
            continue
    with path.open("rb") as f:
        return f.read().decode("latin1", errors="replace").splitlines()


def parse_colon_line(line: str):
    """'Key       :Value' í˜•íƒœë¥¼ 'Key', 'Value'ë¡œ ë¶„ë¦¬."""
    if ":" not in line:
        return line.strip(), ""
    left, right = line.split(":", 1)
    return left.strip(), right.strip()


def parse_end_time_fct(line: str):
    """
    FCTìš© End Time íŒŒì‹±
    ì˜ˆ) 'End Time                :2025/10/01  01:46:41'
    -> End day: '20251001', End time: '01:46:41'
    """
    _, value = parse_colon_line(line)
    m = re.search(r"(?P<date>\d{4}/\d{2}/\d{2})\s+(?P<time>\d{2}:\d{2}:\d{2})", value)
    if not m:
        return "", ""
    day_raw = m.group("date")
    time_raw = m.group("time")
    day = day_raw.replace("/", "")
    return day, time_raw


def parse_end_time_vision(line: str):
    """
    Visionìš© End Time íŒŒì‹±
    ì˜ˆ) 'End Time                : 2025-10-01 04:30:55'
    -> End day: '20251001', End time: '04:30:55'
    """
    _, value = parse_colon_line(line)
    m = re.search(r"(?P<date>\d{4}-\d{2}-\d{2})\s+(?P<time>\d{2}:\d{2}:\d{2})", value)
    if not m:
        return "", ""
    day_raw = m.group("date")
    time_raw = m.group("time")
    day = day_raw.replace("-", "")
    return day, time_raw


# step ë¼ì¸ íŒŒì‹±ìš©
step_pattern = re.compile(
    r"""^
    (?P<step_no>\d+\.\d+)\s+
    (?P<desc>.+?)\s*,\s*
    (?P<value>[^,]+)\s*,\s*
    (?P<min>[^,]+)\s*,\s*
    (?P<max>[^,]+)\s*,\s*
    \[(?P<result>[^\]]+)\]
    """,
    re.VERBOSE,
)

def parse_steps(lines, start_idx: int = 18):
    """19ë²ˆì§¸ ì¤„(ì¸ë±ìŠ¤ 18)ë¶€í„° ëê¹Œì§€ step íŒŒì‹±."""
    steps = []
    for line in lines[start_idx:]:
        line = line.strip()
        if not line:
            continue
        m = step_pattern.match(line)
        if not m:
            continue
        step_no = m.group("step_no").strip()
        desc = m.group("desc").strip()
        value = m.group("value").strip()
        min_v = m.group("min").strip()
        max_v = m.group("max").strip()
        result = m.group("result").strip()

        step_dict = {
            step_no: desc,
            "value": value,
            "min": min_v,
            "max": max_v,
            "step result": result,
        }
        steps.append(step_dict)
    return steps


def classify_equipment(middle_folder: str, lines):
    """
    TC6~9 -> FCT1~4 ë§¤í•‘
    Vision03 -> 6ë²ˆì§¸ ì¤„ Test Program ê¸°ì¤€ Vision1/2
    """
    if middle_folder in TC_TO_FCT:
        return TC_TO_FCT[middle_folder]

    if middle_folder == "Vision03":
        if len(lines) >= 6:
            _, value = parse_colon_line(lines[5])
            if "LED1" in value:
                return "Vision1"
            if "LED2" in value:
                return "Vision2"
        return "Vision_Unknown"

    return "Unknown"


def parse_one_log_file(path: Path, middle_folder: str):
    """í•œ ê°œ ë¡œê·¸ íŒŒì¼ íŒŒì‹± â†’ (json_data, record) ë¦¬í„´."""
    lines = read_text_file(path)

    if len(lines) < 19:
        return None

    equip_group = classify_equipment(middle_folder, lines)

    if equip_group.startswith("FCT"):
        _, station_val = parse_colon_line(lines[2])
        end_day, end_time = parse_end_time_fct(lines[8])
    elif equip_group.startswith("Vision"):
        station_val = equip_group  # Vision1/2 í‘œê¸°
        end_day, end_time = parse_end_time_vision(lines[8])
    else:
        _, station_val = parse_colon_line(lines[2])
        end_day, end_time = parse_end_time_fct(lines[8])

    _, barcode_val = parse_colon_line(lines[4])
    _, result_val = parse_colon_line(lines[12])
    _, runtime_val = parse_colon_line(lines[13])

    steps = parse_steps(lines, start_idx=18)

    json_data = {
        "End day": end_day,
        "End time": end_time,
        "Station": station_val,
        "Barcode information": barcode_val,
        "Result": result_val,
        "Run Time": runtime_val,
        "equipment_group": equip_group,
        "equipment_raw": middle_folder,
        "file_path": str(path),
        "steps": steps,
    }

    record = {
        "equipment_group": equip_group,
        "equipment_raw": middle_folder,
        "file_path": str(path),
        "End day": end_day,
        "End time": end_time,
        "Station": station_val,
        "Barcode information": barcode_val,
        "Result": result_val,
        "Run Time": runtime_val,
    }

    return json_data, record

# ==============================
# DB ê´€ë ¨ í•¨ìˆ˜
# ==============================

def get_connection():
    conn = psycopg2.connect(**DB_CONFIG)
    conn.autocommit = False
    return conn


def ensure_schema_and_table(conn):
    with conn.cursor() as cur:
        cur.execute(f"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME};")
        cur.execute(f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{TABLE_NAME} (
            id BIGSERIAL PRIMARY KEY,
            file_path TEXT UNIQUE,
            equipment_group TEXT,
            equipment_raw TEXT,
            end_day TEXT,
            end_time TEXT,
            station TEXT,
            barcode_information TEXT,
            result TEXT,
            run_time TEXT,
            payload JSONB,
            created_at TIMESTAMPTZ DEFAULT NOW()
        );
        """)
    conn.commit()


def cleanup_old_data(conn, window_start_date: date):
    """
    í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ 6ê°œì›” ì´ìƒëœ ë°ì´í„° ì™„ì „ ì‚­ì œ (DELETE).
    â†’ end_day < window_start_date ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ.
    """
    cutoff_str = window_start_date.strftime("%Y%m%d")
    with conn.cursor() as cur:
        cur.execute(
            f"""
            DELETE FROM {SCHEMA_NAME}.{TABLE_NAME}
            WHERE end_day < %s
            """,
            (cutoff_str,),
        )
        deleted = cur.rowcount
    conn.commit()
    print(f"[ì •ë¦¬] 6ê°œì›” ì´ì „ ë°ì´í„° ì‚­ì œ ì™„ë£Œ (rows={deleted})")


def load_existing_file_paths(conn):
    """
    ì´ë¯¸ DBì— ì˜¬ë¼ê°„ file_path ëª©ë¡ ì½ì–´ì˜¤ê¸°.
    cleanup_old_data ì´í›„ í˜¸ì¶œë˜ë¯€ë¡œ,
    ì‹¤ì œë¡œëŠ” ìµœê·¼ 6ê°œì›”(+ê³ ì • ì‹œì‘ì¼) ë°ì´í„°ë§Œ ë“¤ì–´ìˆê²Œ ë¨.
    """
    with conn.cursor() as cur:
        cur.execute(f"SELECT file_path FROM {SCHEMA_NAME}.{TABLE_NAME};")
        rows = cur.fetchall()
    return set(r[0] for r in rows)


def insert_records(conn, json_objects, records):
    if not json_objects or not records:
        return 0

    rows = []
    for json_obj, rec in zip(json_objects, records):
        rows.append((
            rec["file_path"],
            rec["equipment_group"],
            rec["equipment_raw"],
            rec["End day"],
            rec["End time"],
            rec["Station"],
            rec["Barcode information"],
            rec["Result"],
            rec["Run Time"],
            psycopg2.extras.Json(json_obj),
        ))

    sql = f"""
    INSERT INTO {SCHEMA_NAME}.{TABLE_NAME}
    (
        file_path,
        equipment_group,
        equipment_raw,
        end_day,
        end_time,
        station,
        barcode_information,
        result,
        run_time,
        payload
    )
    VALUES %s
    ON CONFLICT (file_path) DO NOTHING;
    """

    with conn.cursor() as cur:
        psycopg2.extras.execute_values(cur, sql, rows, template=None, page_size=500)
    conn.commit()
    return len(rows)

# ==============================
# ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ë˜í¼
# ==============================

def parse_one_wrapper(args):
    path_str, middle_folder = args
    path = Path(path_str)
    try:
        return parse_one_log_file(path, middle_folder)
    except Exception:
        return None

# ==============================
# ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ìµœì í™”)
# ==============================

def process_batch(executor, batch_targets, conn, existing_paths, batch_index):
    """
    batch_targets: [(path_str, mid), ...]
    - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ íŒŒì‹± í›„
      í•´ë‹¹ ë°°ì¹˜ë§Œ DBì— INSERTí•˜ê³  ë©”ëª¨ë¦¬ì—ì„œ ë²„ë¦¼.
    """
    if not batch_targets:
        return 0

    print(f"[ë°°ì¹˜] #{batch_index} - {len(batch_targets)}ê°œ íŒŒì¼ íŒŒì‹± ì‹œì‘")
    futures = [executor.submit(parse_one_wrapper, t) for t in batch_targets]

    json_list = []
    record_list = []

    for i, f in enumerate(as_completed(futures), start=1):
        result = f.result()
        if result is None:
            continue
        json_obj, rec = result
        json_list.append(json_obj)
        record_list.append(rec)

        if i % 1000 == 0:
            print(f"  â†’ ë°°ì¹˜ #{batch_index} í˜„ì¬ {i}/{len(batch_targets)} íŒŒì‹± ì™„ë£Œ")

    inserted = insert_records(conn, json_list, record_list)

    # ìƒˆë¡œ ë“¤ì–´ê°„ íŒŒì¼ì€ existing_pathsì—ë„ ì¶”ê°€í•´ì„œ ê°™ì€ ëŸ°ì—ì„œ ì¤‘ë³µ ë°©ì§€
    for rec in record_list:
        existing_paths.add(rec["file_path"])

    print(f"[ë°°ì¹˜] #{batch_index} - DB INSERT ì™„ë£Œ (inserted={inserted})")

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œë§Œ json_list / record_listë¥¼ ë³´ìœ í–ˆë‹¤ê°€ ë²„ë¦¬ê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ ì¤„ì–´ë“¦.
    return inserted

# ==============================
# í•œ ë²ˆì˜ ìŠ¤ìº”/íŒŒì‹±/DB ì—…ë¡œë“œ ì‚¬ì´í´
# ==============================

def run_one_cycle():
    # ë‚ ì§œ ìœˆë„ìš° ê³„ì‚°
    window_start_date, window_end_date = get_window_dates()
    window_start_str = window_start_date.strftime("%Y%m%d")
    window_end_str = window_end_date.strftime("%Y%m%d")

    print("\n==================== CYCLE START ====================")
    print("[DEBUG] BASE_LOG_DIR         :", BASE_LOG_DIR)
    print("[DEBUG] BASE_LOG_DIR exists? :", BASE_LOG_DIR.exists())
    print(f"[ìœˆë„ìš°] íŒŒì‹± ê¸°ê°„: {window_start_date} ~ {window_end_date}")

    conn = get_connection()
    try:
        ensure_schema_and_table(conn)

        # 6ê°œì›” ì´ì „ DB ë°ì´í„° ì‚­ì œ
        cleanup_old_data(conn, window_start_date)

        # ìµœê·¼ 6ê°œì›”(ìœˆë„ìš°) ë‚´ì˜ file_pathë§Œ ë©”ëª¨ë¦¬ì— ìœ ì§€
        existing_paths = load_existing_file_paths(conn)
        print(f"[INFO] DBì— ì´ë¯¸ ë“±ë¡ëœ file_path ìˆ˜(ìœˆë„ìš° ë‚´): {len(existing_paths)}")

        total_scanned = 0
        total_new_target = 0
        total_inserted = 0
        skipped = 0
        batch_index = 1

        # ğŸ”¥ ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ë¥¼ 2ê°œë¡œ ê³ ì •
        max_workers = 2
        print(f"[ë©€í‹°í”„ë¡œì„¸ì‹±] ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ ìˆ˜: {max_workers}")

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            batch_targets = []

            for mid in MIDDLE_FOLDERS:
                mid_dir = BASE_LOG_DIR / mid
                print("[DEBUG] mid_dir:", mid_dir, "exists?:", mid_dir.exists())

                if not mid_dir.exists():
                    continue

                for date_dir in sorted(mid_dir.iterdir()):
                    if not date_dir.is_dir():
                        continue

                    date_folder_name = date_dir.name  # ì˜ˆ: "20251122"
                    if not (date_folder_name.isdigit() and len(date_folder_name) == 8):
                        # yyyymmdd í˜•íƒœê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
                        continue

                    # ìœˆë„ìš° ê¸°ì¤€ìœ¼ë¡œ yyyymmdd ë¬¸ìì—´ ë¹„êµ (ê°™ì€ í˜•ì‹ì´ë©´ ë¬¸ìì—´ ë¹„êµ == ë‚ ì§œ ë¹„êµ)
                    if not (window_start_str <= date_folder_name <= window_end_str):
                        continue

                    # ì´ ì‹œì ì—ì„œ date_dirëŠ” ìœˆë„ìš° ì•ˆì— ìˆëŠ” í´ë”
                    any_txt_here = False

                    # 1) GoodFile / BadFile ë°‘ íŒŒì¼ë“¤
                    for sub in TARGET_FOLDERS:
                        target_dir = date_dir / sub
                        if not target_dir.exists():
                            continue

                        for f in target_dir.iterdir():
                            if not f.is_file():
                                continue
                            total_scanned += 1

                            path_str = str(f)
                            if path_str in existing_paths:
                                skipped += 1
                                continue

                            if len(batch_targets) < 5:
                                print("[DEBUG] FOUND FILE (sub):", path_str)

                            batch_targets.append((path_str, mid))
                            total_new_target += 1
                            any_txt_here = True

                            if len(batch_targets) >= BATCH_SIZE:
                                total_inserted += process_batch(
                                    executor,
                                    batch_targets,
                                    conn,
                                    existing_paths,
                                    batch_index,
                                )
                                batch_targets = []
                                batch_index += 1

                    # 2) GoodFile/BadFile ì—†ê³  ë‚ ì§œ í´ë” ë°”ë¡œ ë°‘ì— íŒŒì¼ ìˆëŠ” ê²½ìš°
                    if not any_txt_here:
                        for f in date_dir.iterdir():
                            if not f.is_file():
                                continue
                            total_scanned += 1

                            path_str = str(f)
                            if path_str in existing_paths:
                                skipped += 1
                                continue

                            if len(batch_targets) < 5:
                                print("[DEBUG] FOUND FILE (date_dir):", path_str)

                            batch_targets.append((path_str, mid))
                            total_new_target += 1

                            if len(batch_targets) >= BATCH_SIZE:
                                total_inserted += process_batch(
                                    executor,
                                    batch_targets,
                                    conn,
                                    existing_paths,
                                    batch_index,
                                )
                                batch_targets = []
                                batch_index += 1

            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if batch_targets:
                total_inserted += process_batch(
                    executor,
                    batch_targets,
                    conn,
                    existing_paths,
                    batch_index,
                )

        print(f"[CYCLE] ì „ì²´ ìŠ¤ìº” íŒŒì¼ ìˆ˜       : {total_scanned}")
        print(f"[CYCLE] ìƒˆë¡œ ëŒ€ìƒì´ ëœ íŒŒì¼ ìˆ˜  : {total_new_target}")
        print(f"[CYCLE] DBê¸°ë°˜ ìŠ¤í‚µ(ì´ë¯¸ ì¡´ì¬) ìˆ˜: {skipped}")
        print(f"[CYCLE] ì´ë²ˆ ì‚¬ì´í´ INSERT ìˆ˜   : {total_inserted}")
        print("==================== CYCLE END ====================")

    finally:
        try:
            conn.close()
        except Exception:
            pass

# ==============================
# ë©”ì¸: 1ì´ˆë§ˆë‹¤ ë¬´í•œ ë°˜ë³µ
# ==============================

def main():
    print("[START] a2_fct_vision_testlog_json_processing - ë¬´í•œ ë£¨í”„ ì‹œì‘")
    while True:
        try:
            run_one_cycle()
        except Exception as e:
            print(f"[ERROR] run_one_cycle ì˜ˆì™¸ ë°œìƒ: {e}")
        time.sleep(1)

if __name__ == "__main__":
    main()
