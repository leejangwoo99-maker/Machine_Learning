from pathlib import Path
import re
from datetime import datetime, date
import time
import multiprocessing as mp
import os
import calendar

import psycopg2
from psycopg2.extras import execute_values

# ==========================
# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
# ==========================
BASE_LOG_DIR = Path(r"\\192.168.108.101\HistoryLog")  # NAS ê²½ë¡œ
VISION_FOLDER_NAME = "Vision03"                       # Vision ë¡œê·¸ ì¤‘ê°„ í´ë”
TARGET_FOLDERS = ["GoodFile", "BadFile"]

# ë‚ ì§œ í´ë” ìµœì†Œ ê¸°ì¤€ (ì´ì „ ë°ì´í„° ì œì™¸) - ê³ ì • ì‹œì‘ì¼
FIXED_START_DATE = date(2025, 10, 1)   # yyyymmddì— í•´ë‹¹

# ë°°ì¹˜ ì²˜ë¦¬ ì‹œ í•œ ë²ˆì— DBì— ë„£ì„ ìµœëŒ€ row ìˆ˜ (ë©”ëª¨ë¦¬ ìµœì í™”ìš©)
BATCH_SIZE_ROWS = 50000

# ì‹¤ì‹œê°„ ì „ìš©: ìµœê·¼ Nì´ˆ ì´ë‚´ì— ìˆ˜ì •ëœ íŒŒì¼ë§Œ ëŒ€ìƒ
REALTIME_LOOKBACK_SECONDS = 120  # ì˜ˆ: ìµœê·¼ 2ë¶„

# ==========================
# PostgreSQL ì ‘ì† ì •ë³´
# ==========================
DB_CONFIG = {
    "host": "192.168.108.162",
    "port": 5432,
    "dbname": "postgres",
    "user": "postgres",
    "password": "leejangwoo1!",
}

SCHEMA_MAIN = "a3_vision_json_table"
SCHEMA_HIST = "a3_vision_json_table_processing_history"
TABLE_MAIN = "vision_json_table"
TABLE_HIST = "vision_json_table_processing_history"


# ==========================
# ë‚ ì§œ ìœˆë„ìš° ìœ í‹¸
# ==========================
def six_months_ago(d: date) -> date:
    """
    ì˜¤ëŠ˜ ê¸°ì¤€ 6ê°œì›” ì „ ë‚ ì§œ ê³„ì‚° (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ì°¸ê³ ìš©ìœ¼ë¡œ ë‚¨ê²¨ë‘ ).
    """
    year = d.year
    month = d.month - 6
    if month <= 0:
        year -= 1
        month += 12

    last_day = calendar.monthrange(year, month)[1]
    day = min(d.day, last_day)
    return date(year, month, day)


def get_window_dates():
    """
    ì˜¤ëŠ˜ ê¸°ì¤€ìœ¼ë¡œ 'ì´ë²ˆ ë‹¬ 1ì¼ ~ ì˜¤ëŠ˜' ë²”ìœ„ë¥¼ ë°˜í™˜.
    ì˜ˆ)
      - today = 2025-12-04 â†’ 2025-12-01 ~ 2025-12-04
      - today = 2025-12-31 â†’ 2025-12-01 ~ 2025-12-31
      - today = 2026-01-01 â†’ 2026-01-01 ~ 2026-01-01

    FIXED_START_DATE ì´ì „ì€ ë¬´ì¡°ê±´ ì œì™¸.
    """
    today = date.today()

    # ì´ë²ˆ ë‹¬ 1ì¼
    month_start = today.replace(day=1)

    # ê³ ì • ì‹œì‘ì¼ ì´í›„ë§Œ
    window_start_date = max(month_start, FIXED_START_DATE)
    window_end_date = today

    return window_start_date, window_end_date


# ==========================
# DB ìœ í‹¸
# ==========================
def get_connection():
    return psycopg2.connect(**DB_CONFIG)


def ensure_schema_and_tables(conn):
    with conn.cursor() as cur:
        # ë©”ì¸ ìŠ¤í‚¤ë§ˆ/í…Œì´ë¸”
        cur.execute(f"CREATE SCHEMA IF NOT EXISTS {SCHEMA_MAIN};")
        cur.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {SCHEMA_MAIN}.{TABLE_MAIN} (
                id           BIGSERIAL PRIMARY KEY,
                file_path    TEXT NOT NULL,
                station      TEXT,
                barcode_information TEXT,
                step_description     TEXT,
                value        TEXT,
                min          TEXT,
                max          TEXT,
                result       TEXT,
                processed_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
            );
            """
        )

        # íˆìŠ¤í† ë¦¬ ìŠ¤í‚¤ë§ˆ/í…Œì´ë¸”
        cur.execute(f"CREATE SCHEMA IF NOT EXISTS {SCHEMA_HIST};")
        cur.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {SCHEMA_HIST}.{TABLE_HIST} (
                file_path    TEXT PRIMARY KEY,
                processed_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
            );
            """
        )

    conn.commit()


def cleanup_old_data(conn, window_start_date: date):
    """
    (ì˜µì…˜) í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ 6ê°œì›” ì´ìƒëœ ë°ì´í„° ì‚­ì œ (DELETE).
    ì§€ê¸ˆì€ Python ì½”ë“œì—ì„œ í˜¸ì¶œí•˜ì§€ ì•Šê³ ,
    í•„ìš” ì‹œ ì§ì ‘ SQLë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥.

    - ë©”ì¸ í…Œì´ë¸” : processed_at < window_start_date 00:00:00
    - íˆìŠ¤í† ë¦¬    : processed_at < window_start_date 00:00:00
    """
    cutoff_dt = datetime.combine(window_start_date, datetime.min.time())

    cur = conn.cursor()

    # main
    cur.execute(
        f"""
        DELETE FROM {SCHEMA_MAIN}.{TABLE_MAIN}
        WHERE processed_at < %s
        """,
        (cutoff_dt,),
    )
    deleted_main = cur.rowcount

    # history
    cur.execute(
        f"""
        DELETE FROM {SCHEMA_HIST}.{TABLE_HIST}
        WHERE processed_at < %s
        """,
        (cutoff_dt,),
    )
    deleted_hist = cur.rowcount

    conn.commit()
    cur.close()

    print(
        f"[ì •ë¦¬] 6ê°œì›” ì´ì „ ë°ì´í„° ì‚­ì œ ì™„ë£Œ "
        f"(main={deleted_main}, hist={deleted_hist})",
        flush=True,
    )


def load_processed_file_paths(conn):
    """
    ì´ë¯¸ ì²˜ë¦¬ëœ file_path ëª©ë¡ ë¡œë”©.
    (í˜„ì¬ëŠ” ì „ì²´ íˆìŠ¤í† ë¦¬ì—ì„œ ê°€ì ¸ì˜¤ë©°,
     ì˜¤ë˜ëœ ë°ì´í„°ëŠ” DBì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ì§ì ‘ ì •ë¦¬í•˜ëŠ” ê²ƒì„ ì¶”ì²œ.)
    """
    with conn.cursor() as cur:
        cur.execute(
            f"""
            SELECT file_path
            FROM {SCHEMA_HIST}.{TABLE_HIST}
            """
        )
        rows = cur.fetchall()
    return {r[0] for r in rows}


def insert_history(conn, file_paths):
    if not file_paths:
        return
    data = [(fp,) for fp in file_paths]
    with conn.cursor() as cur:
        execute_values(
            cur,
            f"""
            INSERT INTO {SCHEMA_HIST}.{TABLE_HIST}
                (file_path)
            VALUES %s
            ON CONFLICT (file_path) DO NOTHING
            """,
            data,
        )
    conn.commit()


def insert_main_rows(conn, rows):
    if not rows:
        return 0

    records = [
        (
            r["file_path"],
            r["Station"],
            r["Barcode information"],
            r["step_description"],
            r["value"],
            r["min"],
            r["max"],
            r["result"],
        )
        for r in rows
    ]

    with conn.cursor() as cur:
        execute_values(
            cur,
            f"""
            INSERT INTO {SCHEMA_MAIN}.{TABLE_MAIN}
                (file_path, station, barcode_information, step_description,
                 value, min, max, result)
            VALUES %s
            """,
            records,
        )
    conn.commit()
    return len(records)


# ==========================
# íŒŒì‹± ìœ í‹¸
# ==========================
def parse_barcode_line(line: str) -> str:
    m = re.search(r"Barcode information\s*:\s*(.*)", line)
    return m.group(1).strip() if m else ""


def parse_program_line(line: str) -> str:
    m = re.search(r"Test Program\s*:\s*(.*)", line)
    if not m:
        return ""
    prog = m.group(1).strip()
    if prog == "LED1":
        return "Vision1"
    elif prog == "LED2":
        return "Vision2"
    else:
        return prog


def parse_data_lines(lines):
    """
    ê° step ë¼ì¸ì„ íŒŒì‹±í•´ì„œ list[dict] ë°˜í™˜.
    pandas ì—†ì´ ë°”ë¡œ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ë„ë¡ êµ¬ì„± (ë©”ëª¨ë¦¬ ì ˆì•½).
    """
    rows = []

    for raw_line in lines:
        line = raw_line.strip("\r\n")
        if not line.strip() or "," not in line:
            continue

        parts = [p.strip() for p in line.split(",")]
        if len(parts) < 2:
            continue

        desc = re.sub(r"\s{2,}", " ", parts[0]).strip()
        value = parts[1] if len(parts) > 1 else ""
        min_val = parts[2] if len(parts) > 2 else ""
        max_val = parts[3] if len(parts) > 3 else ""
        result = parts[4] if len(parts) > 4 else ""

        rows.append(
            {
                "step_description": desc,
                "value": value,
                "min": min_val,
                "max": max_val,
                "result": result,
            }
        )

    return rows


# ==========================
# ì›Œì»¤: í•œ íŒŒì¼ ì²˜ë¦¬ (mp.Poolì—ì„œ ì‚¬ìš©)
# ==========================
def process_one_file(file_path_str: str):
    p = Path(file_path_str)
    try:
        with p.open("r", encoding="cp949", errors="ignore") as f:
            lines = f.readlines()
    except Exception as e:
        print(f"[ERROR] íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {p} ({e})", flush=True)
        return []

    if len(lines) < 19:
        return []

    barcode = parse_barcode_line(lines[4]) if len(lines) > 4 else ""
    station = parse_program_line(lines[5]) if len(lines) > 5 else ""
    step_rows = parse_data_lines(lines[18:])

    if not step_rows:
        return []

    rows = []
    for sr in step_rows:
        rows.append(
            {
                "file_path": str(p),
                "Station": station,
                "Barcode information": barcode,
                "step_description": sr["step_description"],
                "value": sr["value"],
                "min": sr["min"],
                "max": sr["max"],
                "result": sr["result"],
            }
        )

    return rows


# ==========================
# í•œ ë²ˆ ì‹¤í–‰(run_once)
# ==========================
def run_once():
    started_at = datetime.now()
    print(f"\n================ run_once ì‹œì‘: {started_at} ================", flush=True)

    # ë‚ ì§œ ìœˆë„ìš° ê³„ì‚° (ì´ë²ˆ ë‹¬ 1ì¼ ~ ì˜¤ëŠ˜, FIXED_START_DATE ì ìš©)
    window_start_date, window_end_date = get_window_dates()
    window_start_str = window_start_date.strftime("%Y%m%d")
    window_end_str = window_end_date.strftime("%Y%m%d")

    print(f"[ìœˆë„ìš°] íŒŒì‹± ê¸°ê°„: {window_start_date} ~ {window_end_date}", flush=True)

    # ì‹¤ì‹œê°„ ê¸°ì¤€ ì‹œê° (ìµœê·¼ Nì´ˆ ì´ë‚´ ìˆ˜ì •ëœ íŒŒì¼ë§Œ ëŒ€ìƒ)
    now_ts = time.time()
    cutoff_ts = now_ts - REALTIME_LOOKBACK_SECONDS
    print(
        f"[ì‹¤ì‹œê°„] ìµœê·¼ {REALTIME_LOOKBACK_SECONDS}ì´ˆ ì´ë‚´ ìˆ˜ì •ëœ íŒŒì¼ë§Œ ì²˜ë¦¬ (cutoff_ts={cutoff_ts})",
        flush=True,
    )

    vision_root = BASE_LOG_DIR / VISION_FOLDER_NAME
    print(f"[DEBUG] vision_root: {vision_root}", flush=True)

    if not vision_root.exists():
        print(f"[WARN] Vision03 í´ë”ê°€ ì—†ìŒ: {vision_root}", flush=True)
        return

    conn = get_connection()
    try:
        ensure_schema_and_tables(conn)

        # âœ… ë°ì´í„° ì‚­ì œëŠ” DB ìª½ì—ì„œ ì§ì ‘ SQLë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì„ ì¶”ì²œ
        # cleanup_old_data(conn, window_start_date)  # í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œí•´ì„œ ì‚¬ìš©

        # ì •ë¦¬ í›„, í˜„ì¬ í…Œì´ë¸” ê¸°ì¤€ìœ¼ë¡œ file_path ë¡œë“œ
        processed_set = load_processed_file_paths(conn)
        print(f"[INFO] íˆìŠ¤í† ë¦¬ file_path ìˆ˜: {len(processed_set)}ê°œ", flush=True)

        # -------- íŒŒì¼ ìŠ¤ìº” (í´ë” ìœˆë„ìš° + mtime í•„í„°) --------
        file_list = []
        total_scanned = 0

        date_dirs = []
        for d in sorted(vision_root.iterdir()):
            if not d.is_dir():
                continue
            name = d.name
            # yyyymmdd í˜•ì‹ + ìœˆë„ìš° ë²”ìœ„ ì•ˆì¸ì§€ ì²´í¬
            if not re.fullmatch(r"\d{8}", name):
                continue
            if not (window_start_str <= name <= window_end_str):
                continue
            date_dirs.append(d)

        print(f"[DEBUG] ë‚ ì§œ í´ë” ìˆ˜(ìœˆë„ìš° ì ìš© í›„): {len(date_dirs)}ê°œ", flush=True)

        for date_dir in date_dirs:
            folder_date = date_dir.name

            for gb in TARGET_FOLDERS:
                sub_dir = date_dir / gb
                if not sub_dir.exists():
                    continue

                for f in sub_dir.iterdir():
                    if not f.is_file():
                        continue

                    # ğŸ”¥ ì‹¤ì‹œê°„ mtime í•„í„°: ìµœê·¼ REALTIME_LOOKBACK_SECONDS ì´ë‚´ ìˆ˜ì •ëœ íŒŒì¼ë§Œ ëŒ€ìƒ
                    try:
                        if f.stat().st_mtime < cutoff_ts:
                            continue
                    except FileNotFoundError:
                        # ì‚¬ì´ì— ì‚­ì œëœ ê²½ìš° ë“±ì€ ë¬´ì‹œ
                        continue

                    total_scanned += 1
                    fp_str = str(f)

                    # ì´ë¯¸ ì²˜ë¦¬í•œ íŒŒì¼ì´ë©´ ìŠ¤í‚µ
                    if fp_str in processed_set:
                        continue

                    file_list.append(fp_str)

        print(f"[INFO] ì „ì²´ ìŠ¤ìº” íŒŒì¼ ìˆ˜(ìœˆë„ìš°+mtime í†µê³¼): {total_scanned}ê°œ", flush=True)
        print(f"[INFO] ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(file_list)}ê°œ", flush=True)

        if not file_list:
            print("[INFO] ì²˜ë¦¬í•  ì‹ ê·œ íŒŒì¼ ì—†ìŒ.", flush=True)
            return

        # -------- ë©€í‹°í”„ë¡œì„¸ì‹± + ë°°ì¹˜ ì²˜ë¦¬ --------
        # CPU ì½”ì–´ ìˆ˜ì™€ ìƒê´€ì—†ì´ í•­ìƒ 4ê°œ í”„ë¡œì„¸ìŠ¤ë§Œ ì‚¬ìš© (ì›í•˜ë©´ 4ë¡œ ì¡°ì ˆ ê°€ëŠ¥)
        cpu_cnt = 4
        print(f"[INFO] ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ ìˆ˜: {cpu_cnt}", flush=True)

        batch_rows = []
        batch_file_paths = set()
        total_inserted_rows = 0
        total_new_files = 0

        with mp.Pool(processes=cpu_cnt) as pool:
            for idx, rows in enumerate(
                pool.imap_unordered(process_one_file, file_list), start=1
            ):
                if rows:
                    batch_rows.extend(rows)
                    # í•œ íŒŒì¼ì˜ ëª¨ë“  rowëŠ” ê°™ì€ file_pathë¥¼ ê°€ì§€ë¯€ë¡œ ì²« ë²ˆì§¸ ê²ƒë§Œ ì‚¬ìš©
                    batch_file_paths.add(rows[0]["file_path"])
                    total_new_files += 1

                # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ DBì— INSERT í›„ ë©”ëª¨ë¦¬ í•´ì œ
                if len(batch_rows) >= BATCH_SIZE_ROWS:
                    inserted = insert_main_rows(conn, batch_rows)
                    insert_history(conn, batch_file_paths)
                    total_inserted_rows += inserted

                    print(
                        f"[ë°°ì¹˜] {idx}/{len(file_list)} íŒŒì¼ ì²˜ë¦¬ê¹Œì§€ "
                        f"(ì´ë²ˆ ë°°ì¹˜ rows={inserted}, ëˆ„ì  rows={total_inserted_rows})",
                        flush=True,
                    )

                    batch_rows.clear()
                    batch_file_paths.clear()

                # ì§„í–‰ ìƒí™© ë¡œê·¸
                if idx % 1000 == 0 or idx == len(file_list):
                    print(
                        f"[ì§„í–‰] {idx}/{len(file_list)} íŒŒì¼ íŒŒì‹± ì™„ë£Œ "
                        f"(í˜„ì¬ ë°°ì¹˜ rows={len(batch_rows)})",
                        flush=True,
                    )

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch_rows:
            inserted = insert_main_rows(conn, batch_rows)
            insert_history(conn, batch_file_paths)
            total_inserted_rows += inserted
            print(
                f"[ë°°ì¹˜] ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (rows={inserted}, ëˆ„ì  rows={total_inserted_rows})",
                flush=True,
            )

        finished_at = datetime.now()
        print(
            f"[ì™„ë£Œ] ì‹ ê·œ íŒŒì¼ {total_new_files}ê°œ, "
            f"ì‹ ê·œ row {total_inserted_rows}ê°œ PostgreSQL íŒŒì‹± ì™„ë£Œ. "
            f"(ì†Œìš”ì‹œê°„: {finished_at - started_at})",
            flush=True,
        )

    finally:
        conn.close()
        print("================ run_once ì¢…ë£Œ ================\n", flush=True)


# ==========================
# ë©”ì¸ ë£¨í”„: 1ì´ˆë§ˆë‹¤ ë°˜ë³µ
# ==========================
if __name__ == "__main__":
    try:
        print("[START] a3_vision_json_table - ë¬´í•œ ë£¨í”„ ì‹œì‘", flush=True)
        while True:
            try:
                run_once()
            except Exception as e:
                print("[ERROR] run_once ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e, flush=True)
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
