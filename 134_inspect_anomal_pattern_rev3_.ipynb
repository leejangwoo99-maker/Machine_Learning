{
 "cells": [
  {
   "cell_type": "code",
   "id": "513de70e-bf4c-4c47-9dbd-737cb3178734",
   "metadata": {},
   "source": [
    "# cell1\n",
    "import numpy as np\n",
    "\n",
    "def window_vector_from_values(values):\n",
    "    \"\"\"\n",
    "    일자 평균 시계열 → 5차원 특성 벡터\n",
    "    \"\"\"\n",
    "    values = np.asarray(values, dtype=float)\n",
    "\n",
    "    if len(values) < 2:\n",
    "        return {\n",
    "            \"mean\": np.nan,\n",
    "            \"std\": np.nan,\n",
    "            \"amplitude\": np.nan,\n",
    "            \"diff_mean\": np.nan,\n",
    "            \"diff_std\": np.nan,\n",
    "        }\n",
    "\n",
    "    diffs = np.diff(values)\n",
    "\n",
    "    return {\n",
    "        \"mean\": float(np.mean(values)),\n",
    "        \"std\": float(np.std(values, ddof=0)),\n",
    "        \"amplitude\": float(np.max(values) - np.min(values)),\n",
    "        \"diff_mean\": float(np.mean(diffs)),\n",
    "        \"diff_std\": float(np.std(diffs, ddof=0)),\n",
    "    }\n",
    "\n",
    "print(\"[OK] window_vector_from_values loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6940985b-a9a9-4265-b648-c0e8a329284f",
   "metadata": {},
   "source": [
    "# cell2\n",
    "import numpy as np\n",
    "\n",
    "def window_vector_from_values(values):\n",
    "    \"\"\"\n",
    "    일자 평균 시계열 → 5차원 특성 벡터\n",
    "    \"\"\"\n",
    "    values = np.asarray(values, dtype=float)\n",
    "\n",
    "    if len(values) < 2:\n",
    "        return {\n",
    "            \"mean\": np.nan,\n",
    "            \"std\": np.nan,\n",
    "            \"amplitude\": np.nan,\n",
    "            \"diff_mean\": np.nan,\n",
    "            \"diff_std\": np.nan,\n",
    "        }\n",
    "\n",
    "    diffs = np.diff(values)\n",
    "\n",
    "    return {\n",
    "        \"mean\": float(np.mean(values)),\n",
    "        \"std\": float(np.std(values, ddof=0)),\n",
    "        \"amplitude\": float(np.max(values) - np.min(values)),\n",
    "        \"diff_mean\": float(np.mean(diffs)),\n",
    "        \"diff_std\": float(np.std(diffs, ddof=0)),\n",
    "    }\n",
    "\n",
    "print(\"[OK] window_vector_from_values loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8c9b6b5-7b71-49f7-a54e-cb34baeb991a",
   "metadata": {},
   "source": [
    "# cell3\n",
    "# Predictive Maintenance - Normal Reference Builder (Integrated Cell)\n",
    "# - Query a2_fct_table.fct_table for (station, step_desc, date range)\n",
    "# - Aggregate daily mean (MMDD) + sample_amount\n",
    "# - Build 5D feature vector (mean/std/amplitude/diff_mean/diff_std)\n",
    "# - Save JSONB reference pattern into e4_predictive_maintenance.predictive_maintenance\n",
    "#   with UPSERT on (station, step_description, pattern_name)\n",
    "# ============================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from IPython.display import display\n",
    "\n",
    "# =========================\n",
    "# [0] DB 엔진 생성 (필수)\n",
    "# =========================\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"100.105.75.47\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"\",#보안\n",
    "}\n",
    "\n",
    "def get_engine(cfg=DB_CONFIG):\n",
    "    user = cfg[\"user\"]\n",
    "    password = urllib.parse.quote_plus(cfg[\"password\"])\n",
    "    host = cfg[\"host\"]\n",
    "    port = cfg[\"port\"]\n",
    "    dbname = cfg[\"dbname\"]\n",
    "    conn_str = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "    return create_engine(conn_str, pool_pre_ping=True)\n",
    "\n",
    "engine = get_engine()\n",
    "print(\"[OK] SQLAlchemy engine created\")\n",
    "\n",
    "# =========================\n",
    "# [1] Features + Vector 함수 (필수)\n",
    "# =========================\n",
    "FEATURES = [\"mean\", \"std\", \"amplitude\", \"diff_mean\", \"diff_std\"]\n",
    "\n",
    "def window_vector_from_values(values):\n",
    "    \"\"\"\n",
    "    일자 평균 시계열 → 5차원 특성 벡터\n",
    "    FEATURES = [mean, std, amplitude, diff_mean, diff_std]\n",
    "    \"\"\"\n",
    "    values = np.asarray(values, dtype=float)\n",
    "\n",
    "    if len(values) < 2:\n",
    "        return {\n",
    "            \"mean\": np.nan,\n",
    "            \"std\": np.nan,\n",
    "            \"amplitude\": np.nan,\n",
    "            \"diff_mean\": np.nan,\n",
    "            \"diff_std\": np.nan,\n",
    "        }\n",
    "\n",
    "    diffs = np.diff(values)\n",
    "\n",
    "    return {\n",
    "        \"mean\": float(np.mean(values)),\n",
    "        \"std\": float(np.std(values, ddof=0)),\n",
    "        \"amplitude\": float(np.max(values) - np.min(values)),\n",
    "        \"diff_mean\": float(np.mean(diffs)),\n",
    "        \"diff_std\": float(np.std(diffs, ddof=0)),\n",
    "    }\n",
    "\n",
    "print(\"[OK] window_vector_from_values loaded\")\n",
    "\n",
    "# =========================\n",
    "# [2] 조건\n",
    "# =========================\n",
    "SCHEMA = \"a2_fct_table\"\n",
    "TABLE  = \"fct_table\"\n",
    "\n",
    "station   = \"FCT2\"\n",
    "step_desc = \"1.34_Test_VUSB_Type-C_A(ELoad2=1.35A)vol\"\n",
    "\n",
    "# end_day 비교는 YYYYMMDD로 통일\n",
    "start_day = \"20251229\"\n",
    "end_day   = \"20260115\"\n",
    "\n",
    "TARGET_SCHEMA = \"e4_predictive_maintenance\"\n",
    "TARGET_TABLE  = \"predictive_maintenance\"\n",
    "pattern_name  = \"pd_board_normal_ref\"\n",
    "\n",
    "# =========================\n",
    "# NaN -> None 변환(JSONB용)\n",
    "# =========================\n",
    "def sanitize_for_json(obj):\n",
    "    if isinstance(obj, float) and (np.isnan(obj) or np.isinf(obj)):\n",
    "        return None\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: sanitize_for_json(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [sanitize_for_json(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "# =========================\n",
    "# 1) DB 조회 (end_day를 YYYYMMDD로 변환해서 비교)\n",
    "# =========================\n",
    "sql = text(f\"\"\"\n",
    "SELECT\n",
    "  replace(CAST(end_day AS text), '-', '') AS end_day_yyyymmdd,\n",
    "  station,\n",
    "  value\n",
    "FROM {SCHEMA}.{TABLE}\n",
    "WHERE station = :station\n",
    "  AND replace(CAST(end_day AS text), '-', '') BETWEEN :start_day AND :end_day\n",
    "  AND step_description = :step_desc\n",
    "ORDER BY end_day_yyyymmdd\n",
    "\"\"\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    raw = pd.read_sql(sql, conn, params={\n",
    "        \"station\": station,\n",
    "        \"start_day\": start_day,\n",
    "        \"end_day\": end_day,\n",
    "        \"step_desc\": step_desc\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터 0건 방지\n",
    "# =========================\n",
    "if raw.empty:\n",
    "    raise ValueError(\n",
    "        f\"[ERROR] No rows found. Check end_day format/type in DB. \"\n",
    "        f\"station={station}, range={start_day}~{end_day}, step_desc={step_desc}\"\n",
    "    )\n",
    "\n",
    "raw[\"value_num\"] = pd.to_numeric(raw[\"value\"], errors=\"coerce\")\n",
    "raw = raw.dropna(subset=[\"value_num\"]).copy()\n",
    "\n",
    "if raw.empty:\n",
    "    raise ValueError(\"[ERROR] value column has no numeric rows after conversion.\")\n",
    "\n",
    "raw[\"mmdd\"] = raw[\"end_day_yyyymmdd\"].str.slice(4, 8)\n",
    "\n",
    "# =========================\n",
    "# 3) MMDD별 평균(2dp) + 표본수\n",
    "# =========================\n",
    "avg_df_post = (\n",
    "    raw.groupby([\"station\", \"mmdd\"], as_index=False)\n",
    "       .agg(value_avg=(\"value_num\", \"mean\"),\n",
    "            sample_amount=(\"value_num\", \"count\"))\n",
    "       .sort_values([\"station\", \"mmdd\"])\n",
    ")\n",
    "avg_df_post[\"value_avg\"] = avg_df_post[\"value_avg\"].round(2)\n",
    "\n",
    "baseline_days = int(avg_df_post[\"mmdd\"].nunique())\n",
    "if baseline_days < 2:\n",
    "    raise ValueError(f\"[ERROR] Not enough days to build vector. baseline_days={baseline_days}\")\n",
    "\n",
    "# =========================\n",
    "# 4) 정상 기준 벡터(구간 전체로 1개 벡터)\n",
    "# =========================\n",
    "values = avg_df_post[\"value_avg\"].values\n",
    "v_normal_post = window_vector_from_values(values)\n",
    "\n",
    "ref_pattern = {k: float(v_normal_post[k]) for k in FEATURES}\n",
    "ref_pattern_2dp = {k: round(ref_pattern[k], 2) for k in FEATURES}\n",
    "\n",
    "# =========================\n",
    "# 5) DB 저장(JSONB)\n",
    "# =========================\n",
    "euclid_graph = {\n",
    "    \"type\": \"normal_reference\",\n",
    "    \"features\": FEATURES,\n",
    "    \"reference_pattern\": ref_pattern,\n",
    "    \"reference_pattern_2dp\": ref_pattern_2dp,\n",
    "    \"data_range\": {\"start_day\": start_day, \"end_day\": end_day},\n",
    "    \"baseline_days\": baseline_days,\n",
    "    \"source\": {\n",
    "        \"schema\": SCHEMA,\n",
    "        \"table\": TABLE,\n",
    "        \"station\": station,\n",
    "        \"step_description\": step_desc\n",
    "    },\n",
    "}\n",
    "\n",
    "# JSONB는 NaN 금지 → sanitize + allow_nan=False\n",
    "euclid_graph_clean = sanitize_for_json(euclid_graph)\n",
    "euclid_graph_json = json.dumps(euclid_graph_clean, ensure_ascii=False, allow_nan=False)\n",
    "\n",
    "create_schema_sql = text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_SCHEMA};\")\n",
    "\n",
    "create_table_sql = text(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TARGET_SCHEMA}.{TARGET_TABLE} (\n",
    "    station               TEXT NOT NULL,\n",
    "    step_description      TEXT NOT NULL,\n",
    "    pattern_name          TEXT NOT NULL,\n",
    "    window_size           INTEGER NOT NULL,\n",
    "    normal_start_mmdd     TEXT,\n",
    "    normal_end_mmdd       TEXT,\n",
    "    abnormal_start_mmdd   TEXT,\n",
    "    abnormal_end_mmdd     TEXT,\n",
    "    euclid_graph          JSONB NOT NULL,\n",
    "    updated_at            TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "    PRIMARY KEY (station, step_description, pattern_name)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "upsert_sql = text(f\"\"\"\n",
    "INSERT INTO {TARGET_SCHEMA}.{TARGET_TABLE} (\n",
    "    station, step_description, pattern_name,\n",
    "    window_size,\n",
    "    normal_start_mmdd, normal_end_mmdd,\n",
    "    abnormal_start_mmdd, abnormal_end_mmdd,\n",
    "    euclid_graph, updated_at\n",
    ") VALUES (\n",
    "    :station, :step_description, :pattern_name,\n",
    "    :window_size,\n",
    "    :normal_start_mmdd, :normal_end_mmdd,\n",
    "    NULL, NULL,\n",
    "    CAST(:euclid_graph AS JSONB), NOW()\n",
    ")\n",
    "ON CONFLICT (station, step_description, pattern_name)\n",
    "DO UPDATE SET\n",
    "    window_size = EXCLUDED.window_size,\n",
    "    normal_start_mmdd = EXCLUDED.normal_start_mmdd,\n",
    "    normal_end_mmdd = EXCLUDED.normal_end_mmdd,\n",
    "    euclid_graph = EXCLUDED.euclid_graph,\n",
    "    updated_at = NOW();\n",
    "\"\"\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(create_schema_sql)\n",
    "    conn.execute(create_table_sql)\n",
    "    conn.execute(upsert_sql, {\n",
    "        \"station\": station,\n",
    "        \"step_description\": step_desc,\n",
    "        \"pattern_name\": pattern_name,\n",
    "        \"window_size\": baseline_days,\n",
    "        \"normal_start_mmdd\": start_day[4:8],  # \"1117\"\n",
    "        \"normal_end_mmdd\": end_day[4:8],      # \"1124\"\n",
    "        \"euclid_graph\": euclid_graph_json,\n",
    "    })\n",
    "\n",
    "print(\"[OK] Saved:\", pattern_name, \"| baseline_days =\", baseline_days)\n",
    "display(avg_df_post)\n",
    "print(\"Normal ref vector (2dp):\", ref_pattern_2dp)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f0b9f36-0b28-4ce5-abec-05ff1360edc1",
   "metadata": {},
   "source": [
    "# cell4 (FCT2 포함 버전)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import text\n",
    "\n",
    "SCHEMA = \"a2_fct_table\"\n",
    "TABLE  = \"fct_table\"\n",
    "\n",
    "# ✅ FCT2 추가\n",
    "stations  = [\"FCT1\", \"FCT2\", \"FCT3\", \"FCT4\"]\n",
    "\n",
    "start_day = \"20251229\"\n",
    "end_day   = \"20260115\"\n",
    "step_desc = \"1.34_Test_VUSB_Type-C_A(ELoad2=1.35A)vol\"\n",
    "\n",
    "sql = text(f\"\"\"\n",
    "SELECT\n",
    "  trim(end_day) AS end_day_yyyymmdd,\n",
    "  trim(station) AS station,\n",
    "  step_description,\n",
    "  value\n",
    "FROM {SCHEMA}.{TABLE}\n",
    "WHERE trim(station) = ANY(:stations)\n",
    "  AND trim(end_day) BETWEEN :start_day AND :end_day\n",
    "  AND step_description = :step_desc\n",
    "ORDER BY station, end_day_yyyymmdd\n",
    "\"\"\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    raw = pd.read_sql(sql, conn, params={\n",
    "        \"stations\": stations,\n",
    "        \"start_day\": start_day,\n",
    "        \"end_day\": end_day,\n",
    "        \"step_desc\": step_desc\n",
    "    })\n",
    "\n",
    "print(f\"[DEBUG] raw rows = {len(raw):,}\")\n",
    "if raw.empty:\n",
    "    raise ValueError(\"[ERROR] raw is empty. (station/date/step_description 조건 불일치)\")\n",
    "\n",
    "print(\"[DEBUG] value sample (top10):\", raw[\"value\"].astype(str).head(10).tolist())\n",
    "\n",
    "raw[\"value_num\"] = pd.to_numeric(raw[\"value\"], errors=\"coerce\")\n",
    "print(f\"[DEBUG] numeric rows after to_numeric = {raw['value_num'].notna().sum():,} / {len(raw):,}\")\n",
    "\n",
    "if raw[\"value_num\"].notna().sum() == 0:\n",
    "    raw[\"value_num\"] = (\n",
    "        raw[\"value\"].astype(str)\n",
    "        .str.extract(r\"(-?\\d+(?:\\.\\d+)?)\", expand=False)\n",
    "    )\n",
    "    raw[\"value_num\"] = pd.to_numeric(raw[\"value_num\"], errors=\"coerce\")\n",
    "    print(f\"[DEBUG] numeric rows after regex extract = {raw['value_num'].notna().sum():,} / {len(raw):,}\")\n",
    "\n",
    "raw = raw.dropna(subset=[\"value_num\"]).copy()\n",
    "if raw.empty:\n",
    "    raise ValueError(\"[ERROR] value_num is empty after numeric parsing. value 포맷 확인 필요.\")\n",
    "\n",
    "raw[\"mmdd\"] = raw[\"end_day_yyyymmdd\"].str.slice(4, 8)\n",
    "raw[\"date\"] = pd.to_datetime(raw[\"end_day_yyyymmdd\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "raw = raw.dropna(subset=[\"date\"]).copy()\n",
    "\n",
    "avg_df_all = (\n",
    "    raw.groupby([\"station\", \"mmdd\"], as_index=False)\n",
    "       .agg(\n",
    "           value_avg=(\"value_num\", \"mean\"),\n",
    "           sample_amount=(\"value_num\", \"count\"),\n",
    "           date=(\"date\", \"min\")\n",
    "       )\n",
    "       .sort_values([\"station\", \"mmdd\"])\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "avg_df_all[\"value_avg\"] = avg_df_all[\"value_avg\"].round(2)\n",
    "\n",
    "print(f\"[OK] avg_df_all rows = {len(avg_df_all):,} | stations={sorted(avg_df_all['station'].unique().tolist())}\")\n",
    "display(avg_df_all.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20e98ef4-0f85-4406-85b9-a0b8adcc8317",
   "metadata": {},
   "source": [
    "# cell5\n",
    "# ============================================\n",
    "# [CELL 0] Analysis Date Range Definition (FIXED)\n",
    "# - 2025 고정 변환 제거\n",
    "# - START_DAY/END_DAY 기반으로 연도 자동 적용\n",
    "# - 가능하면 end_day_yyyymmdd / date 컬럼을 우선 사용\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 분석 기간 (여기만 바꿔서 사용)\n",
    "# -----------------------------\n",
    "START_DAY = \"20251229\"   # YYYYMMDD\n",
    "END_DAY   = \"20260115\"   # YYYYMMDD\n",
    "\n",
    "START_DT = pd.to_datetime(START_DAY, format=\"%Y%m%d\")\n",
    "END_DT   = pd.to_datetime(END_DAY, format=\"%Y%m%d\")\n",
    "\n",
    "print(f\"[INFO] Analysis Period = {START_DT.date()} ~ {END_DT.date()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 안전 변환 함수들\n",
    "# -----------------------------\n",
    "def yyyymmdd_to_date(yyyymmdd: str) -> pd.Timestamp:\n",
    "    \"\"\"YYYYMMDD(8자리) 문자열을 datetime으로 변환\"\"\"\n",
    "    yyyymmdd = str(yyyymmdd).strip()\n",
    "    return pd.to_datetime(yyyymmdd, format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "def mmdd_to_date_by_range(mmdd: str, start_dt: pd.Timestamp, end_dt: pd.Timestamp) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    MMDD(4자리) → date 변환\n",
    "    - 기본 연도는 start_dt.year 사용\n",
    "    - 만약 (start~end)가 연말~연초를 가로지르는 케이스면 연도 자동 보정\n",
    "      예) start=20251228, end=20260105 인 경우\n",
    "          mmdd=0102 는 2026-01-02로 판단\n",
    "    \"\"\"\n",
    "    mmdd = str(mmdd).strip()\n",
    "    if len(mmdd) != 4 or not mmdd.isdigit():\n",
    "        return pd.NaT\n",
    "\n",
    "    y_start = start_dt.year\n",
    "    y_end   = end_dt.year\n",
    "\n",
    "    # 우선 start 연도로 붙여본다\n",
    "    dt1 = pd.to_datetime(f\"{y_start}{mmdd}\", format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "    # start~end가 연도를 가로지르면(예: 2025-12 ~ 2026-01) 보정\n",
    "    crosses_year = (y_start != y_end)\n",
    "\n",
    "    if crosses_year and pd.notna(dt1):\n",
    "        # dt1이 end_dt보다 너무 뒤(예: 2025-01-02가 되어버린 경우)면 end 연도로 재시도\n",
    "        # 또는 dt1이 start_dt보다 너무 앞이면(end 연도로 가야 하는) end 연도로 변환\n",
    "        dt2 = pd.to_datetime(f\"{y_end}{mmdd}\", format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "        # dt1이 범위 밖이고 dt2가 범위에 더 잘 맞으면 dt2 선택\n",
    "        if pd.notna(dt2):\n",
    "            in1 = (dt1 >= start_dt) and (dt1 <= end_dt)\n",
    "            in2 = (dt2 >= start_dt) and (dt2 <= end_dt)\n",
    "            if (not in1) and in2:\n",
    "                return dt2\n",
    "\n",
    "    return dt1\n",
    "\n",
    "# -----------------------------\n",
    "# 3) avg_df_all 기간 필터\n",
    "# -----------------------------\n",
    "assert \"avg_df_all\" in globals(), \"[ERROR] avg_df_all must be created before CELL 0\"\n",
    "\n",
    "avg_df_all = avg_df_all.copy()\n",
    "\n",
    "# (우선순위) date 컬럼이 이미 있으면 그대로 사용\n",
    "if \"date\" in avg_df_all.columns:\n",
    "    avg_df_all[\"date\"] = pd.to_datetime(avg_df_all[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# (차선) end_day_yyyymmdd가 있으면 그걸로 date 생성\n",
    "elif \"end_day_yyyymmdd\" in avg_df_all.columns:\n",
    "    avg_df_all[\"date\"] = avg_df_all[\"end_day_yyyymmdd\"].apply(yyyymmdd_to_date)\n",
    "\n",
    "# (최후) mmdd만 있으면 START/END 기반 연도 자동 적용\n",
    "elif \"mmdd\" in avg_df_all.columns:\n",
    "    avg_df_all[\"date\"] = avg_df_all[\"mmdd\"].apply(lambda x: mmdd_to_date_by_range(x, START_DT, END_DT))\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"[ERROR] avg_df_all must have one of ['date', 'end_day_yyyymmdd', 'mmdd']\")\n",
    "\n",
    "# date 생성 실패 제거\n",
    "avg_df_all = avg_df_all.dropna(subset=[\"date\"]).copy()\n",
    "\n",
    "# 기간 필터 적용\n",
    "avg_df_all = avg_df_all[\n",
    "    (avg_df_all[\"date\"] >= START_DT) &\n",
    "    (avg_df_all[\"date\"] <= END_DT)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"[OK] avg_df_all filtered rows = {len(avg_df_all):,}\")\n",
    "print(f\"[OK] stations = {sorted(avg_df_all['station'].unique().tolist()) if len(avg_df_all) else []}\")\n",
    "display(avg_df_all.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4379d0a8-4e0c-4e81-9ab2-f0f0c35f1104",
   "metadata": {},
   "source": [
    "# cell6\n",
    "import json\n",
    "from sqlalchemy import text\n",
    "\n",
    "TARGET_SCHEMA = \"e4_predictive_maintenance\"\n",
    "TARGET_TABLE  = \"predictive_maintenance\"\n",
    "\n",
    "def load_pattern(station, step_desc, pattern_name):\n",
    "    q = text(f\"\"\"\n",
    "SELECT euclid_graph\n",
    "    FROM {TARGET_SCHEMA}.{TARGET_TABLE}\n",
    "    WHERE station=:station AND step_description=:step_desc AND pattern_name=:pattern_name\n",
    "    \"\"\")\n",
    "    with engine.begin() as conn:\n",
    "        row = conn.execute(q, {\n",
    "            \"station\": station,\n",
    "            \"step_desc\": step_desc,\n",
    "            \"pattern_name\": pattern_name\n",
    "        }).fetchone()\n",
    "    if row is None:\n",
    "        raise ValueError(f\"[ERROR] Pattern not found: {station} / {pattern_name}\")\n",
    "    g = row[0]\n",
    "    if isinstance(g, str):\n",
    "        g = json.loads(g)\n",
    "    return g\n",
    "\n",
    "# 저장해 둔 패턴 로드\n",
    "g_normal = load_pattern(\"FCT2\", step_desc, \"pd_board_normal_ref\")\n",
    "g_abn    = load_pattern(\"FCT2\", step_desc, \"pd_board_degradation_ref\")\n",
    "\n",
    "FEATURES = g_normal[\"features\"]\n",
    "V_normal = np.array([g_normal[\"reference_pattern\"][k] for k in FEATURES], dtype=float)\n",
    "A_ref    = np.array([g_abn[\"reference_pattern\"][k] for k in FEATURES], dtype=float)\n",
    "\n",
    "print(\"[OK] FEATURES =\", FEATURES)\n",
    "print(\"[OK] V_normal(2dp) =\", {k: round(float(g_normal[\"reference_pattern\"][k]), 2) for k in FEATURES})\n",
    "print(\"[OK] A_ref(2dp)    =\", {k: round(float(g_abn[\"reference_pattern\"][k]), 2) for k in FEATURES})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9a1c2c9-85bc-4c9d-b975-35c647c797f1",
   "metadata": {},
   "source": [
    "# cell8 (FCT2 포함 버전)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "WINDOW = 5\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-12):\n",
    "    na = np.linalg.norm(a)\n",
    "    nb = np.linalg.norm(b)\n",
    "    if na < eps or nb < eps:\n",
    "        return np.nan\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "assert \"avg_df_all\" in globals(), \"[ERROR] avg_df_all 이(가) 없습니다. 이전 셀 실행 순서를 확인하세요.\"\n",
    "need_cols = {\"station\", \"mmdd\", \"value_avg\"}\n",
    "missing = need_cols - set(avg_df_all.columns)\n",
    "assert not missing, f\"[ERROR] avg_df_all 컬럼 누락: {missing}\"\n",
    "\n",
    "print(f\"[INFO] avg_df_all rows={len(avg_df_all):,} | stations={sorted(avg_df_all['station'].unique().tolist())}\")\n",
    "\n",
    "# ✅ FCT2 추가\n",
    "st_targets = [\"FCT1\", \"FCT2\", \"FCT3\", \"FCT4\"]\n",
    "lens = {st: int((avg_df_all[\"station\"] == st).sum()) for st in st_targets}\n",
    "print(\"[INFO] rows per station:\", lens, \"| WINDOW =\", WINDOW)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for st in st_targets:\n",
    "    df_st = avg_df_all[avg_df_all[\"station\"] == st].copy()\n",
    "    df_st = df_st.dropna(subset=[\"mmdd\", \"value_avg\"]).copy()\n",
    "    df_st[\"mmdd\"] = df_st[\"mmdd\"].astype(str)\n",
    "    df_st = df_st.sort_values(\"mmdd\").reset_index(drop=True)\n",
    "\n",
    "    if len(df_st) < WINDOW:\n",
    "        print(f\"[WARN] skip {st}: len(df_st)={len(df_st)} < WINDOW({WINDOW})\")\n",
    "        continue\n",
    "\n",
    "    for i in range(len(df_st) - WINDOW + 1):\n",
    "        chunk = df_st.iloc[i:i+WINDOW]\n",
    "        mmdd_end = chunk[\"mmdd\"].iloc[-1]\n",
    "\n",
    "        v = window_vector_from_values(chunk[\"value_avg\"].values)\n",
    "        V_t = np.array([v[k] for k in FEATURES], dtype=float)\n",
    "\n",
    "        A_t = V_t - V_normal\n",
    "\n",
    "        rows.append({\n",
    "            \"station\": st,\n",
    "            \"mmdd\": mmdd_end,\n",
    "            \"score_from_normal\": float(np.linalg.norm(A_t)),\n",
    "            \"cos_sim_to_ref\": cosine_sim(A_t, A_ref),\n",
    "            \"dist_to_ref\": float(np.linalg.norm(A_t - A_ref)),\n",
    "        })\n",
    "\n",
    "if not rows:\n",
    "    compare_df = pd.DataFrame(columns=[\"station\", \"mmdd\", \"score_from_normal\", \"cos_sim_to_ref\", \"dist_to_ref\"])\n",
    "    print(\"[ERROR] compare_df 생성 실패: rows가 비어 있습니다.\")\n",
    "    print(\"        원인 후보:\")\n",
    "    print(\"        1) avg_df_all이 기간 필터(START_DAY/END_DAY)로 비어짐\")\n",
    "    print(\"        2) 대상 스테이션 데이터가 WINDOW(5)일 미만\")\n",
    "    print(\"        3) value_avg가 NaN으로 전부 drop됨\")\n",
    "else:\n",
    "    compare_df = pd.DataFrame(rows).sort_values([\"station\", \"mmdd\"]).reset_index(drop=True)\n",
    "    compare_df[\"score_from_normal\"] = compare_df[\"score_from_normal\"].round(2)\n",
    "    compare_df[\"cos_sim_to_ref\"] = compare_df[\"cos_sim_to_ref\"].round(3)\n",
    "    compare_df[\"dist_to_ref\"] = compare_df[\"dist_to_ref\"].round(2)\n",
    "\n",
    "display(compare_df.head(10))\n",
    "print(\"[OK] compare_df rows =\", len(compare_df))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbabdbb2-9f82-43ff-aa07-37bd16b20273",
   "metadata": {},
   "source": [
    "# cell9 =========================\n",
    "# 코사인 유사도 임계값\n",
    "# =========================\n",
    "COS_TH = 0.70   # 0.7 이상이면 \"이상 패턴과 방향이 유사\"\n",
    "\n",
    "print(\"[OK] COS_TH defined:\", COS_TH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aeee16f0-db7c-4dee-bd99-c057918af81e",
   "metadata": {},
   "source": [
    "# cell10\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "dfp = compare_df.copy()\n",
    "\n",
    "# mmdd → 실제 연도 자동 적용 (2025 고정 금지)\n",
    "def mmdd_to_date(mmdd):\n",
    "    mmdd = str(mmdd).zfill(4)\n",
    "    y_start = START_DT.year\n",
    "    y_end   = END_DT.year\n",
    "\n",
    "    d1 = pd.to_datetime(f\"{y_start}{mmdd}\", format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    if (y_start != y_end) and (d1 < START_DT or d1 > END_DT):\n",
    "        d2 = pd.to_datetime(f\"{y_end}{mmdd}\", format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        if START_DT <= d2 <= END_DT:\n",
    "            return d2\n",
    "    return d1\n",
    "\n",
    "dfp[\"date\"] = dfp[\"mmdd\"].apply(mmdd_to_date)\n",
    "\n",
    "# ✅ 시간순으로 선이 제대로 이어지도록 정렬 (Plotly는 행 순서대로 선을 연결함)\n",
    "dfp = (dfp.sort_values(['station','date'], kind='mergesort')\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "# 1) 코사인 유사도 추이\n",
    "fig1 = px.line(\n",
    "    dfp,\n",
    "    x=\"date\",\n",
    "    y=\"cos_sim_to_ref\",\n",
    "    color=\"station\",\n",
    "    markers=True,\n",
    "    title=f\"Cosine Similarity to Anomaly Pattern (A_ref) | TH={COS_TH}\",\n",
    "    labels={\"date\":\"Date\", \"cos_sim_to_ref\":\"cosine(A_t, A_ref)\", \"station\":\"Station\"},\n",
    ")\n",
    "fig1.add_hline(y=COS_TH, line_dash=\"dash\")\n",
    "fig1.update_xaxes(tickformat=\"%m-%d\")\n",
    "fig1.update_layout(width=1100, height=420)\n",
    "fig1.show()\n",
    "\n",
    "# 2) 정상 기준 대비 score 추이\n",
    "fig2 = px.line(\n",
    "    dfp,\n",
    "    x=\"date\",\n",
    "    y=\"score_from_normal\",\n",
    "    color=\"station\",\n",
    "    markers=True,\n",
    "    title=\"Score from Normal Baseline (||A_t||)\",\n",
    "    labels={\"date\":\"Date\", \"score_from_normal\":\"||A_t||\", \"station\":\"Station\"},\n",
    ")\n",
    "fig2.update_xaxes(tickformat=\"%m-%d\")\n",
    "fig2.update_layout(width=1100, height=420)\n",
    "fig2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b8c3b36-7748-4d91-a3aa-0f1d92e0456c",
   "metadata": {},
   "source": [
    "# cell11 (연도 꼬임 제거 버전: START_DT/END_DT 기반)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "K_MAD = 4.0\n",
    "MIN_SAMPLES_FOR_ROBUST = 8\n",
    "\n",
    "def mad(x: pd.Series) -> float:\n",
    "    x = x.dropna().astype(float).values\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    med = np.median(x)\n",
    "    return float(np.median(np.abs(x - med)))\n",
    "\n",
    "def robust_threshold(x: pd.Series, k=K_MAD):\n",
    "    x = x.dropna().astype(float)\n",
    "    if len(x) < MIN_SAMPLES_FOR_ROBUST:\n",
    "        if len(x) == 0:\n",
    "            return np.nan\n",
    "        p90 = float(np.percentile(x, 90))\n",
    "        return p90 * 1.10 if np.isfinite(p90) else np.nan\n",
    "\n",
    "    med = float(np.median(x))\n",
    "    m = mad(x)\n",
    "    if not np.isfinite(m) or m == 0:\n",
    "        return float(np.percentile(x, 95))\n",
    "    return med + k * m\n",
    "\n",
    "# mmdd → date (연도 자동)\n",
    "def mmdd_to_date(mmdd: str) -> pd.Timestamp:\n",
    "    mmdd = str(mmdd).zfill(4)\n",
    "    y_start = START_DT.year\n",
    "    y_end   = END_DT.year\n",
    "\n",
    "    d1 = pd.to_datetime(f\"{y_start}{mmdd}\", format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    if (y_start != y_end) and (pd.notna(d1)) and ((d1 < START_DT) or (d1 > END_DT)):\n",
    "        d2 = pd.to_datetime(f\"{y_end}{mmdd}\", format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        if pd.notna(d2) and (START_DT <= d2 <= END_DT):\n",
    "            return d2\n",
    "    return d1\n",
    "\n",
    "# 1) station별 threshold\n",
    "th_df = (\n",
    "    compare_df.groupby(\"station\")[\"score_from_normal\"]\n",
    "    .apply(lambda s: robust_threshold(s, k=K_MAD))\n",
    "    .reset_index(name=\"th_score\")\n",
    ")\n",
    "display(th_df)\n",
    "\n",
    "dfi = compare_df.merge(th_df, on=\"station\", how=\"left\").copy()\n",
    "dfi[\"date\"] = dfi[\"mmdd\"].apply(mmdd_to_date)\n",
    "\n",
    "# 2) 상태 분류\n",
    "dfi[\"is_cos_like\"] = dfi[\"cos_sim_to_ref\"] >= COS_TH\n",
    "dfi[\"is_score_high\"] = dfi[\"score_from_normal\"] >= dfi[\"th_score\"]\n",
    "\n",
    "def classify(row):\n",
    "    if row[\"is_cos_like\"] and row[\"is_score_high\"]:\n",
    "        return \"CRITICAL\"\n",
    "    if row[\"is_cos_like\"] and (not row[\"is_score_high\"]):\n",
    "        return \"WARNING\"\n",
    "    if (not row[\"is_cos_like\"]) and row[\"is_score_high\"]:\n",
    "        return \"WATCH\"\n",
    "    return \"OK\"\n",
    "\n",
    "dfi[\"status\"] = dfi.apply(classify, axis=1)\n",
    "\n",
    "# 3) 지속성(연속 경보일)\n",
    "ALERT_LEVELS = {\"WARNING\", \"CRITICAL\"}\n",
    "\n",
    "def add_consecutive_alerts(df_station: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_station = df_station.sort_values(\"date\").copy()\n",
    "    consec = 0\n",
    "    out = []\n",
    "    for _, r in df_station.iterrows():\n",
    "        if r[\"status\"] in ALERT_LEVELS:\n",
    "            consec += 1\n",
    "        else:\n",
    "            consec = 0\n",
    "        out.append(consec)\n",
    "    df_station[\"alert_streak\"] = out\n",
    "    return df_station\n",
    "\n",
    "dfi = (\n",
    "    dfi.groupby(\"station\", as_index=True, group_keys=True)\n",
    "       .apply(add_consecutive_alerts, include_groups=False)\n",
    "       .reset_index(level=0)\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 4) 핵심 요약표\n",
    "N_LAST = 5\n",
    "latest_df = (\n",
    "    dfi.sort_values([\"station\", \"date\"])\n",
    "       .groupby(\"station\")\n",
    "       .tail(N_LAST)\n",
    "       .sort_values([\"station\", \"date\"])\n",
    ")\n",
    "display(latest_df[[\n",
    "    \"station\",\"mmdd\",\"score_from_normal\",\"th_score\",\"cos_sim_to_ref\",\"dist_to_ref\",\n",
    "    \"status\",\"alert_streak\"\n",
    "]])\n",
    "\n",
    "summary = (\n",
    "    dfi.sort_values([\"station\",\"date\"])\n",
    "       .groupby(\"station\")\n",
    "       .agg(\n",
    "           last_date=(\"date\",\"max\"),\n",
    "           last_status=(\"status\",\"last\"),\n",
    "           last_score=(\"score_from_normal\",\"last\"),\n",
    "           th_score=(\"th_score\",\"last\"),\n",
    "           last_cos=(\"cos_sim_to_ref\",\"last\"),\n",
    "           max_score=(\"score_from_normal\",\"max\"),\n",
    "           max_cos=(\"cos_sim_to_ref\",\"max\"),\n",
    "           max_streak=(\"alert_streak\",\"max\"),\n",
    "           crit_days=(\"status\", lambda s: int((s==\"CRITICAL\").sum())),\n",
    "           warn_days=(\"status\", lambda s: int((s==\"WARNING\").sum())),\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "# 5) 최종 판단 텍스트\n",
    "def interpret_station(row):\n",
    "    st = row[\"station\"]\n",
    "    ls = row[\"last_status\"]\n",
    "    score = row[\"last_score\"]\n",
    "    th = row[\"th_score\"]\n",
    "    cosv = row[\"last_cos\"]\n",
    "    streak = row[\"max_streak\"]\n",
    "\n",
    "    if ls == \"CRITICAL\":\n",
    "        return (f\"- {st}: 현재 CRITICAL. cos={cosv:.3f}(>=TH {COS_TH}), \"\n",
    "                f\"||A_t||={score:.2f}(>=th {th:.2f}). 최대 연속 경보 {streak}일. \"\n",
    "                f\"즉시 점검/교체 타이밍 검토 권장.\")\n",
    "    if ls == \"WARNING\":\n",
    "        return (f\"- {st}: 현재 WARNING(전조). cos={cosv:.3f}(>=TH {COS_TH})로 방향성 정렬. \"\n",
    "                f\"||A_t||={score:.2f}, th={th:.2f}는 미초과/경계. \"\n",
    "                f\"추세 지속 시 CRITICAL 전이 가능.\")\n",
    "    if ls == \"WATCH\":\n",
    "        return (f\"- {st}: 현재 WATCH. ||A_t||={score:.2f}(>=th {th:.2f})로 이탈은 있으나 \"\n",
    "                f\"cos={cosv:.3f}(<TH {COS_TH})로 ref 패턴 불일치. \"\n",
    "                f\"다른 원인 가능 → FAIL%, 다른 step 교차검증 권장.\")\n",
    "    return (f\"- {st}: 현재 OK. cos={cosv:.3f}, ||A_t||={score:.2f}(th {th:.2f}) 범위 내.\")\n",
    "\n",
    "print(\"\\n[INTERPRETATION]\")\n",
    "for _, r in summary.sort_values(\"station\").iterrows():\n",
    "    print(interpret_station(r))\n",
    "\n",
    "# 6) status 타임라인\n",
    "status_map = {\"OK\":0, \"WATCH\":1, \"WARNING\":2, \"CRITICAL\":3}\n",
    "dfv = dfi.copy()\n",
    "dfv[\"status_level\"] = dfv[\"status\"].map(status_map)\n",
    "\n",
    "fig_status = px.line(\n",
    "    dfv,\n",
    "    x=\"date\",\n",
    "    y=\"status_level\",\n",
    "    color=\"station\",\n",
    "    markers=True,\n",
    "    title=f\"Station Status Timeline (0=OK,1=WATCH,2=WARNING,3=CRITICAL) | COS_TH={COS_TH}, K_MAD={K_MAD}\",\n",
    "    labels={\"date\":\"Date\",\"status_level\":\"Status Level\",\"station\":\"Station\"},\n",
    ")\n",
    "fig_status.update_xaxes(tickformat=\"%m-%d\")\n",
    "fig_status.update_yaxes(dtick=1, range=[-0.2, 3.2])\n",
    "fig_status.update_layout(width=1100, height=420)\n",
    "fig_status.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d62b18f2-9b6b-48d8-9e6f-16830669f9a6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b99acb0a-6eb9-4fa1-935a-e482168abd79",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-ds-py313)",
   "language": "python",
   "name": "ds_py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
