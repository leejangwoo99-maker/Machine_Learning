# ============================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# ============================================
import pandas as pd
from sqlalchemy import create_engine, text
import urllib.parse
from datetime import datetime
import sys

import plotly.express as px

# Jupyter / .py ë‘˜ ë‹¤ì—ì„œ display() ì“°ê¸° ìœ„í•œ ì²˜ë¦¬
try:
    from IPython.display import display  # ë…¸íŠ¸ë¶ì´ë©´ ì´ê±° ì‚¬ìš©
except ImportError:  # .py ë‹¨ë… ì‹¤í–‰ì´ë©´ printë¡œ ëŒ€ì²´
    def display(obj):
        print(obj)


# ============================================
# 1. DB ì ‘ì† (SQLAlchemy ì—”ì§„ ë°©ì‹)
# ============================================
user = "postgres"
password_raw = "leejangwoo1!"
host = "100.105.75.47"
port = 5432
dbname = "postgres"

password = urllib.parse.quote_plus(password_raw)
conn_str = f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}"
engine = create_engine(conn_str)

VIEW_NAME = "a2_fct_vision_testlog_json_processing.vw_pass_fail_pd_nonpd_runtime"

# ê²°ê³¼ ì €ì¥ìš© ìŠ¤í‚¤ë§ˆ / í…Œì´ë¸” ì´ë¦„
SCHEMA_RESULT   = "e2_fct_vision_test_ct"
TABLE_UPPER     = "fct_upper_outlier"
TABLE_CT        = "fct_vision_test_ct"
TABLE_FAIL      = "fct_fail"
TABLE_PROCESSED = "fct_processed_file"   # âœ… ì²˜ë¦¬ ì™„ë£Œ file_path ê´€ë¦¬ìš©

# ì´ë²ˆ ì‹¤í–‰ì˜ ì²˜ë¦¬ ì‹œê°„ (ëª¨ë“  í…Œì´ë¸” ê³µí†µ)
processed_time = datetime.now()

print("=== [1] DB ì—”ì§„ ìƒì„± ì™„ë£Œ ===")
print("Connection String:", conn_str)
print("processed_time:", processed_time)
print()


# ============================================
# 2. VIEWì—ì„œ ì•„ì§ ì²˜ë¦¬ ì•ˆ í•œ file_pathë§Œ ì½ê¸°
#    (ì²˜ë¦¬ ì™„ë£Œ file_pathëŠ” íŒ¨ìŠ¤)
# ============================================
print("=== [2] VIEWì—ì„œ 'ë¯¸ì²˜ë¦¬ file_path' ë°ì´í„° ì½ê¸° ===")

# 2-0. ìŠ¤í‚¤ë§ˆ ë° ì²˜ë¦¬ì™„ë£Œ í…Œì´ë¸” ìƒì„±
with engine.begin() as conn:
    conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {SCHEMA_RESULT}"))
    conn.execute(text(f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA_RESULT}.{TABLE_PROCESSED} (
            file_path      TEXT PRIMARY KEY,
            processed_time TIMESTAMP
        )
    """))

# 2-1. ì²˜ë¦¬ ì™„ë£Œëœ file_pathëŠ” ì œì™¸í•˜ê³  VIEW ì¡°íšŒ
query_all = text(f"""
    SELECT
        v.end_time,
        v.end_day,
        v.remark,
        v.barcode_information,
        v.station,
        v.run_time,
        v.result,
        v.file_path
    FROM {VIEW_NAME} AS v
    LEFT JOIN {SCHEMA_RESULT}.{TABLE_PROCESSED} AS p
        ON v.file_path = p.file_path
    WHERE p.file_path IS NULL      -- âœ… ì•„ì§ ì²˜ë¦¬í•œ ì  ì—†ëŠ” file_pathë§Œ
""")

with engine.connect() as conn:
    df_all = pd.read_sql(query_all, conn)

if df_all.empty:
    print("ğŸ‘‰ ìƒˆë¡œ ì²˜ë¦¬í•  ë°ì´í„°(ë¯¸ì²˜ë¦¬ file_path)ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    sys.exit(0)

# ì»¬ëŸ¼ ìˆœì„œ: end_day ë‹¤ìŒì— end_time ì˜¤ë„ë¡ ì •ë¦¬
desired_cols = [
    "end_day",
    "end_time",
    "remark",
    "barcode_information",
    "station",
    "run_time",
    "result",
    "file_path",
]
df_all = df_all[desired_cols]

print("ì´ë²ˆì— ìƒˆë¡œ ì²˜ë¦¬í•  ë¡œìš° ìˆ˜:", len(df_all))
print("ì»¬ëŸ¼ ëª©ë¡:", df_all.columns.tolist())
display(df_all.head())

# run_time ìˆ«ìí˜• ì •ë¦¬
df_all["run_time"] = pd.to_numeric(df_all["run_time"], errors="coerce")
df_all = df_all.dropna(subset=["run_time"]).reset_index(drop=True)

print("\nrun_time ìˆ«ìí˜• ë³€í™˜ + NaN ì œê±° í›„ ë¡œìš° ìˆ˜:", len(df_all))
display(df_all.head())
print()

# PASS / FAIL ë¶„ë¦¬
df_pass_raw = df_all[df_all["result"] != "FAIL"].copy().reset_index(drop=True)
df_fail_raw = df_all[df_all["result"] == "FAIL"].copy().reset_index(drop=True)

print("PASS ë¡œìš° ìˆ˜:", len(df_pass_raw))
print("FAIL ë¡œìš° ìˆ˜:", len(df_fail_raw))
print()


# ============================================
# 3. station + remark ë³„ run_time í‰ê· (run_average) ê³„ì‚° (ì°¸ê³ ìš©)
# ============================================
print("=== [3] station + remark ë³„ run_average ê³„ì‚° (ì°¸ê³ ìš©) ===")

df_group = (
    df_pass_raw
    .groupby(["station", "remark"], as_index=False)["run_time"]
    .mean()
    .rename(columns={"run_time": "run_average"})
    .sort_values(["station", "remark"])
    .reset_index(drop=True)
)

display(df_group)
print()


# ============================================
# 4. station + remark ë³„ IQR ê¸°ë°˜ lower_extreme / upper ê³„ì‚°
#    - Extreme Lower = Q1 - 3*IQR
#    - Upper         = Q3 + 1.5*IQR
# ============================================
print("=== [4] IQR ê¸°ë°˜ ì´ìƒì¹˜ ê²½ê³„ ê³„ì‚° (PASS ë°ì´í„° ê¸°ì¤€) ===")

df_pass = df_pass_raw.copy()

grouped = df_pass.groupby(["station", "remark"])["run_time"]
Q1 = grouped.transform(lambda x: x.quantile(0.25))
Q3 = grouped.transform(lambda x: x.quantile(0.75))
IQR = Q3 - Q1

df_pass["lower_extreme"] = Q1 - 3.0 * IQR
df_pass["upper"]         = Q3 + 1.5 * IQR

display(df_pass.head())
print()


# ============================================
# 5. ì´ìƒì¹˜ í”Œë˜ê·¸ ì¶”ê°€
# ============================================
print("=== [5] ì´ìƒì¹˜ í”Œë˜ê·¸ ì¶”ê°€ ===")

df_pass["is_outlier"] = (df_pass["run_time"] < df_pass["lower_extreme"]) | (
    df_pass["run_time"] > df_pass["upper"]
)

outlier_count = df_pass["is_outlier"].sum()
print(f"PASS ë°ì´í„° ì¤‘ ì´ìƒì¹˜ ê°œìˆ˜: {outlier_count} / ì „ì²´ {len(df_pass)}")
display(df_pass.head())
print()


# ============================================
# 6. Plotly boxplot ì‹œê°í™” (FCT1~FCT4 í•œ ë²ˆë§Œ)
# ============================================
print("=== [6] Plotly boxplot ì‹œê°í™” (FCT1~FCT4ë§Œ) ===")

# FCT1~FCT4 ë°ì´í„°ë§Œ ì‚¬ìš©
df_fct = df_pass[df_pass["station"].isin(["FCT1", "FCT2", "FCT3", "FCT4"])].copy()

if df_fct.empty:
    print("FCT1~FCT4 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Boxplot ìƒì„± ìƒëµ.")
else:
    df_fct["station_remark"] = df_fct["station"] + " / " + df_fct["remark"]

    fig = px.box(
        df_fct,
        x="station_remark",
        y="run_time",
        points="outliers",
        title="[FCT1~FCT4] PD / Non-PD CT ë¶„í¬ (Boxplot, ì§€í‘œ í‘œì‹œ)",
    )

    # ê·¸ë£¹ë³„ í†µê³„ëŸ‰ ê³„ì‚°
    stats_box = (
        df_fct
        .groupby("station_remark")["run_time"]
        .describe(percentiles=[0.25, 0.5, 0.75])
        .reset_index()
        .rename(columns={
            "min": "min_val",
            "25%": "q1_val",
            "50%": "median_val",
            "75%": "q3_val",
            "max": "max_val",
        })
    )

    for _, row in stats_box.iterrows():
        x = row["station_remark"]

        fig.add_annotation(
            x=x, y=row["min_val"],
            text=f"min: {row['min_val']:.2f}",
            showarrow=False, yshift=-20, font=dict(size=10),
        )
        fig.add_annotation(
            x=x, y=row["q1_val"],
            text=f"q1: {row['q1_val']:.2f}",
            showarrow=False, yshift=-5, font=dict(size=10),
        )
        fig.add_annotation(
            x=x, y=row["median_val"],
            text=f"median: {row['median_val']:.2f}",
            showarrow=False, yshift=10, font=dict(size=10),
        )
        fig.add_annotation(
            x=x, y=row["q3_val"],
            text=f"q3: {row['q3_val']:.2f}",
            showarrow=False, yshift=25, font=dict(size=10),
        )
        fig.add_annotation(
            x=x, y=row["max_val"],
            text=f"max: {row['max_val']:.2f}",
            showarrow=False, yshift=40, font=dict(size=10),
        )

    fig.update_layout(
        xaxis_title="Station / Remark",
        yaxis_title="CT",
    )
    fig.show()


# ============================================
# 7. ì´ìƒì¹˜ ì œê±° í›„ yyyymm + station + remark ë³„ final_runtime ê³„ì‚°
#    - end_day + end_time â†’ end_ts
#    - 00:00:00 ~ 08:29:59 ëŠ” ì „ë‚ (prod_date - 1ì¼)ë¡œ ê°„ì£¼
# ============================================
print("=== [7] ì´ìƒì¹˜ ì œê±° í›„ ì›”ë³„(ìƒì‚°ì¼ ê¸°ì¤€) CT ì§‘ê³„ ===")

df_no_outlier = df_pass[~df_pass["is_outlier"]].copy().reset_index(drop=True)
print("ì´ìƒì¹˜ ì œê±° í›„ PASS ë°ì´í„° í–‰ ìˆ˜:", len(df_no_outlier))
display(df_no_outlier.head())

# ë¬¸ìì—´ ê²°í•©ìš©
df_no_outlier["end_day"]  = df_no_outlier["end_day"].astype(str)
df_no_outlier["end_time"] = df_no_outlier["end_time"].astype(str)

# ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ (YYYYMMDD HH:MM:SS)
df_no_outlier["end_ts"] = pd.to_datetime(
    df_no_outlier["end_day"] + " " + df_no_outlier["end_time"],
    errors="coerce"
)

# NaT ì œê±°
df_no_outlier = df_no_outlier.dropna(subset=["end_ts"]).reset_index(drop=True)

# ê¸°ë³¸ ìƒì‚°ì¼ = end_tsì˜ ë‚ ì§œ
df_no_outlier["prod_date"] = df_no_outlier["end_ts"].dt.date

# 00:00:00 ~ 08:29:59 êµ¬ê°„ì´ë©´ ì „ë‚ ë¡œ ì´ë™
hour = df_no_outlier["end_ts"].dt.hour
minute = df_no_outlier["end_ts"].dt.minute

mask_night_morning = (hour < 8) | ((hour == 8) & (minute < 30))

df_no_outlier.loc[mask_night_morning, "prod_date"] = (
    df_no_outlier.loc[mask_night_morning, "prod_date"]
    - pd.to_timedelta(1, unit="D")
)

# ìµœì¢… ì§‘ê³„ìš© yyyymm (ìƒì‚°ì¼ ê¸°ì¤€)
df_no_outlier["yyyymm"] = pd.to_datetime(df_no_outlier["prod_date"]).dt.strftime("%Y%m")

# yyyymm + station + remark ê¸°ì¤€ í‰ê·  CT
df_final_ct = (
    df_no_outlier
    .groupby(["yyyymm", "station", "remark"], as_index=False)["run_time"]
    .mean()
    .rename(columns={"run_time": "final_runtime"})
    .sort_values(["yyyymm", "station", "remark"])
    .reset_index(drop=True)
)

display(df_final_ct)
print()


# ============================================
# 8. DB ì €ì¥ìš© ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„
#    - fct_upper_outlier : FCT1~4 upper ì´ìƒì¹˜ ë°”ì½”ë“œ + file_path
#    - fct_fail          : FAIL ë°”ì½”ë“œ + file_path
#    - fct_vision_test_ct: yyyymm + station + remark ë³„ final_runtime
#    â€» PASS(ì´ìƒì¹˜ ì œê±°) ë°”ì½”ë“œ ë¦¬ìŠ¤íŠ¸(df_pass_unique)ëŠ” íŒŒì‹±/ì €ì¥ ì•ˆ í•¨
# ============================================
print("=== [8] DB ì €ì¥ìš© ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„ ===")

target_stations = ["FCT1", "FCT2", "FCT3", "FCT4"]

# 8-1. FCT1~4 upper ì´ìƒì¹˜ (PASS ë°ì´í„° ê¸°ì¤€)
df_upper_fct = df_pass[
    (df_pass["run_time"] > df_pass["upper"]) &
    (df_pass["station"].isin(target_stations))
].copy()

df_upper_unique = (
    df_upper_fct[["barcode_information", "file_path"]]
    .drop_duplicates()
    .reset_index(drop=True)
)

print("FCT1~4 upper ì´ìƒì¹˜ ë°”ì½”ë“œ ê°œìˆ˜(ì¤‘ë³µ ì œê±°):", len(df_upper_unique))
display(df_upper_unique.head())
print()

# 8-2. FAIL ë°”ì½”ë“œ
df_fail_unique = (
    df_fail_raw[["barcode_information", "file_path"]]
    .drop_duplicates()
    .reset_index(drop=True)
)
print("FAIL ë°”ì½”ë“œ ê°œìˆ˜(ì¤‘ë³µ ì œê±°):", len(df_fail_unique))
display(df_fail_unique.head())
print()

# 8-3. CT ê²°ê³¼ (ì—¬ê¸´ file_path í•„ìš” ì—†ìŒ, yyyymm+station+remark ë‹¨ìœ„)
df_final_ct_db = df_final_ct.copy()


def chunk_records(records, chunk_size=5000):
    """ë¦¬ìŠ¤íŠ¸(ë˜ëŠ” list(dict))ë¥¼ chunk_size ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ yield."""
    for i in range(0, len(records), chunk_size):
        yield records[i:i + chunk_size]


# ============================================
# 9. DB ìŠ¤í‚¤ë§ˆ ë° í…Œì´ë¸” ìƒì„± + ë°ì´í„° ì €ì¥
#    - fct_pass ê´€ë ¨ í…Œì´ë¸”/INSERT ì—†ìŒ (ìš”ì²­ëŒ€ë¡œ ì œì™¸)
#    - file_path + barcode_information ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
#    - ì²˜ë¦¬ ì™„ë£Œ file_path ê¸°ë¡
# ============================================
print("=== [9] DB ìŠ¤í‚¤ë§ˆ ë° í…Œì´ë¸” ìƒì„± + ë°ì´í„° ì €ì¥ ===")

with engine.begin() as conn:
    # 9-1. ìŠ¤í‚¤ë§ˆ ìƒì„± (ì´ë¯¸ [2]ì—ì„œ í•œ ë²ˆ í–ˆì§€ë§Œ idempotent)
    conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {SCHEMA_RESULT}"))

    # 9-2. í…Œì´ë¸” ìƒì„±
    conn.execute(text(f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA_RESULT}.{TABLE_UPPER} (
            file_path           TEXT NOT NULL,
            barcode_information TEXT NOT NULL,
            PRIMARY KEY (file_path, barcode_information)
        )
    """))

    conn.execute(text(f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA_RESULT}.{TABLE_FAIL} (
            file_path           TEXT NOT NULL,
            barcode_information TEXT NOT NULL,
            PRIMARY KEY (file_path, barcode_information)
        )
    """))

    conn.execute(text(f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA_RESULT}.{TABLE_CT} (
            yyyymm        VARCHAR(6)  NOT NULL,
            station       VARCHAR(50) NOT NULL,
            remark        VARCHAR(20) NOT NULL,
            final_runtime NUMERIC,
            PRIMARY KEY (yyyymm, station, remark)
        )
    """))

    conn.execute(text(f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA_RESULT}.{TABLE_PROCESSED} (
            file_path      TEXT PRIMARY KEY,
            processed_time TIMESTAMP
        )
    """))

    # ---------- ë°ì´í„° INSERT (ì¤‘ë³µ ë°©ì§€, chunk ë‹¨ìœ„) ----------

    # fct_upper_outlier : FCT1~4 upper ì´ìƒì¹˜
    if not df_upper_unique.empty:
        recs = df_upper_unique.to_dict(orient="records")
        for idx, chunk in enumerate(chunk_records(recs, chunk_size=5000), start=1):
            conn.execute(
                text(f"""
                    INSERT INTO {SCHEMA_RESULT}.{TABLE_UPPER}
                        (file_path, barcode_information)
                    VALUES
                        (:file_path, :barcode_information)
                    ON CONFLICT (file_path, barcode_information) DO NOTHING
                """),
                chunk
            )
            print(f"  - fct_upper_outlier chunk {idx} ì™„ë£Œ ({len(chunk)} rows)")
        print("â†’ fct_upper_outlier ì €ì¥ ì™„ë£Œ")

    # fct_fail : FAIL ë°”ì½”ë“œ
    if not df_fail_unique.empty:
        recs = df_fail_unique.to_dict(orient="records")
        for idx, chunk in enumerate(chunk_records(recs, chunk_size=5000), start=1):
            conn.execute(
                text(f"""
                    INSERT INTO {SCHEMA_RESULT}.{TABLE_FAIL}
                        (file_path, barcode_information)
                    VALUES
                        (:file_path, :barcode_information)
                    ON CONFLICT (file_path, barcode_information) DO NOTHING
                """),
                chunk
            )
            print(f"  - fct_fail chunk {idx} ì™„ë£Œ ({len(chunk)} rows)")
        print("â†’ fct_fail ì €ì¥ ì™„ë£Œ")

    # fct_vision_test_ct : yyyymm + station + remark ë³„ CT (UPSERT)
    if not df_final_ct_db.empty:
        recs = df_final_ct_db.to_dict(orient="records")
        for idx, chunk in enumerate(chunk_records(recs, chunk_size=5000), start=1):
            conn.execute(
                text(f"""
                    INSERT INTO {SCHEMA_RESULT}.{TABLE_CT}
                        (yyyymm, station, remark, final_runtime)
                    VALUES
                        (:yyyymm, :station, :remark, :final_runtime)
                    ON CONFLICT (yyyymm, station, remark)
                    DO UPDATE SET
                        final_runtime = EXCLUDED.final_runtime
                """),
                chunk
            )
            print(f"  - fct_vision_test_ct chunk {idx} ì™„ë£Œ ({len(chunk)} rows)")
        print("â†’ fct_vision_test_ct ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    # ì²˜ë¦¬ ì™„ë£Œ file_path ê¸°ë¡
    df_new_files = (
        df_all[["file_path"]]
        .drop_duplicates()
        .reset_index(drop=True)
    )

    if not df_new_files.empty:
        recs = [
            {"file_path": fp, "processed_time": processed_time}
            for fp in df_new_files["file_path"]
        ]
        for idx, chunk in enumerate(chunk_records(recs, chunk_size=5000), start=1):
            conn.execute(
                text(f"""
                    INSERT INTO {SCHEMA_RESULT}.{TABLE_PROCESSED}
                        (file_path, processed_time)
                    VALUES (:file_path, :processed_time)
                    ON CONFLICT (file_path) DO UPDATE
                        SET processed_time = EXCLUDED.processed_time
                """),
                chunk
            )
            print(f"  - fct_processed_file chunk {idx} ì™„ë£Œ ({len(chunk)} rows)")
        print("â†’ fct_processed_file ì²˜ë¦¬ì™„ë£Œ ê¸°ë¡ ì €ì¥ ì™„ë£Œ")

print("\n=== ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
