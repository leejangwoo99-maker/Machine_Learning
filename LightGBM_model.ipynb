{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DATE_FROM = \"2025-10-05\"\n",
    "DATE_TO   = \"2025-12-05\"\n",
    "CHUNK_DAYS = 7\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"100.105.75.47\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"\",# 보안 사항\n",
    "}\n",
    "\n",
    "SRC_SCHEMA = \"f_database\"\n",
    "SRC_TABLE  = \"fct_database\"\n",
    "\n",
    "MAP_SCHEMA = \"f_database\"\n",
    "MAP_TABLE_CANDIDATES = [\n",
    "    \"step_description_problme\", \"step_description_problem\",\n",
    "    \"step_description_problem1_4\", \"step_description_problem_map\"\n",
    "]\n",
    "\n",
    "ML_SCHEMA = \"h_machine_learning\"\n",
    "TBL_FEATURE_STORE = \"1_database_house\"\n",
    "TBL_FEATURE_LIST  = \"2_features\"\n",
    "TBL_MODEL         = \"3_machine_learning_model\"\n",
    "TBL_MODEL_SUMMARY = \"4_model_summary\"\n",
    "TBL_ZSTATS_TAG2   = \"5_zstats_tag_level2\"\n",
    "TBL_HEALTH_INDEX  = \"6_health_index\"\n",
    "\n",
    "VERBOSE = True\n",
    "print(f\"DATE_FROM={DATE_FROM} DATE_TO={DATE_TO} CHUNK_DAYS={CHUNK_DAYS}\")"
   ],
   "id": "ec2043ef72e5e57f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def make_engine(cfg: dict) -> Engine:\n",
    "    url = f\"postgresql+psycopg2://{cfg['user']}:{cfg['password']}@{cfg['host']}:{cfg['port']}/{cfg['dbname']}\"\n",
    "    return create_engine(url, pool_pre_ping=True)\n",
    "\n",
    "ENGINE = make_engine(DB_CONFIG)\n",
    "\n",
    "def ensure_schema(engine: Engine, schema: str) -> None:\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {schema};\"))\n",
    "\n",
    "def table_exists(engine: Engine, schema: str, table: str) -> bool:\n",
    "    q = text(\"\"\"\n",
    "        SELECT 1 FROM information_schema.tables\n",
    "        WHERE table_schema=:s AND table_name=:t LIMIT 1\n",
    "    \"\"\")\n",
    "    with engine.begin() as conn:\n",
    "        return conn.execute(q, {\"s\": schema, \"t\": table}).fetchone() is not None\n",
    "\n",
    "def column_exists(engine: Engine, schema: str, table: str, col: str) -> bool:\n",
    "    q = text(\"\"\"\n",
    "        SELECT 1 FROM information_schema.columns\n",
    "        WHERE table_schema=:s AND table_name=:t AND column_name=:c LIMIT 1\n",
    "    \"\"\")\n",
    "    with engine.begin() as conn:\n",
    "        return conn.execute(q, {\"s\": schema, \"t\": table, \"c\": col}).fetchone() is not None\n",
    "\n",
    "def add_column_if_missing(engine: Engine, schema: str, table: str, col: str, col_type_sql: str) -> None:\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'ALTER TABLE \"{schema}\".\"{table}\" ADD COLUMN IF NOT EXISTS \"{col}\" {col_type_sql};'))\n",
    "\n",
    "def ensure_state_table(engine: Engine, schema: str) -> None:\n",
    "    ensure_schema(engine, schema)\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS \"{schema}\".\"etl_state\" (\n",
    "                name TEXT PRIMARY KEY,\n",
    "                value TEXT,\n",
    "                updated_at TIMESTAMP DEFAULT now()\n",
    "            );\n",
    "        \"\"\"))\n",
    "\n",
    "def get_state(engine: Engine, schema: str, name: str) -> Optional[str]:\n",
    "    ensure_state_table(engine, schema)\n",
    "    with engine.begin() as conn:\n",
    "        r = conn.execute(text(f'SELECT value FROM \"{schema}\".\"etl_state\" WHERE name=:n'), {\"n\": name}).fetchone()\n",
    "    return None if r is None else r[0]\n",
    "\n",
    "def set_state(engine: Engine, schema: str, name: str, value: str) -> None:\n",
    "    ensure_state_table(engine, schema)\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"\"\"\n",
    "            INSERT INTO \"{schema}\".\"etl_state\"(name, value, updated_at)\n",
    "            VALUES (:n, :v, now())\n",
    "            ON CONFLICT (name) DO UPDATE SET value=EXCLUDED.value, updated_at=now();\n",
    "        \"\"\"), {\"n\": name, \"v\": value})\n",
    "\n",
    "ensure_schema(ENGINE, ML_SCHEMA)\n",
    "print(\"[OK] engine ready, schema ensured:\", ML_SCHEMA)"
   ],
   "id": "ab3ca154e49975d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_date(s: str) -> pd.Timestamp:\n",
    "    return pd.to_datetime(s).normalize()\n",
    "\n",
    "def daterange_chunks(date_from: str, date_to: str, chunk_days: int) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:\n",
    "    start = parse_date(date_from)\n",
    "    end = parse_date(date_to)\n",
    "    chunks = []\n",
    "    cur = start\n",
    "    while cur <= end:\n",
    "        cur_end = min(cur + pd.Timedelta(days=chunk_days-1), end)\n",
    "        chunks.append((cur, cur_end))\n",
    "        cur = cur_end + pd.Timedelta(days=1)\n",
    "    return chunks\n",
    "\n",
    "CHUNKS = daterange_chunks(DATE_FROM, DATE_TO, CHUNK_DAYS)\n",
    "CHUNKS"
   ],
   "id": "b77ec908313c2830"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_mapping_table(engine: Engine) -> Optional[str]:\n",
    "    for t in MAP_TABLE_CANDIDATES:\n",
    "        if table_exists(engine, MAP_SCHEMA, t):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "MAP_TABLE = find_mapping_table(ENGINE)\n",
    "print(\"Mapping table:\", MAP_TABLE)"
   ],
   "id": "9161eba6555d25b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ✅ 변경사항\n",
    "# 1) smoke test: LIMIT 적용(기본 5000)\n",
    "# 2) contents 제외\n",
    "# 3) step_description IS NOT NULL 필터 추가(대부분 NULL이라면 큰 폭 감소)\n",
    "# 4) test_time 유지(정렬/순서 파악)\n",
    "# 5) 실행 중/단계 로그 출력 + 쿼리 시간 측정\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "SMOKE_TEST_LIMIT = 5000  # smoke test용 LIMIT\n",
    "\n",
    "BASE_COLS = [\n",
    "    \"barcode_information\", '\"group\"', \"station\", \"remark\",\n",
    "    \"end_day\", \"end_time\",\n",
    "    # \"contents\",  # ❌ 제외\n",
    "    \"step_description\", \"test_time\",\n",
    "    \"set_up_or_test_ct\",\n",
    "    \"value\", '\"min\"', '\"max\"', \"result\",\n",
    "    # \"file_path\"  # 필요 없으면 제외 가능(원하면 다시 넣어도 됨)\n",
    "]\n",
    "\n",
    "def fetch_raw_chunk(\n",
    "    engine: Engine,\n",
    "    d0: pd.Timestamp,\n",
    "    d1: pd.Timestamp,\n",
    "    *,\n",
    "    limit: int | None = None,\n",
    "    log_prefix: str = \"[FETCH]\"\n",
    ") -> pd.DataFrame:\n",
    "    t0 = time.time()\n",
    "    print(f\"{log_prefix} start | range={d0.date()}~{d1.date()} | limit={limit}\")\n",
    "\n",
    "    cols = \", \".join(BASE_COLS)\n",
    "\n",
    "    # step_description NULL 제외(대부분 NULL이면 성능 크게 개선)\n",
    "    q = f\"\"\"\n",
    "    SELECT {cols}\n",
    "    FROM \"{SRC_SCHEMA}\".\"{SRC_TABLE}\"\n",
    "    WHERE end_day >= :d0\n",
    "      AND end_day <= :d1\n",
    "      AND step_description IS NOT NULL\n",
    "    ORDER BY end_day ASC, end_time ASC, test_time ASC\n",
    "    \"\"\"\n",
    "\n",
    "    if limit is not None:\n",
    "        q += \"\\nLIMIT :limit\"\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(\n",
    "            text(q),\n",
    "            conn,\n",
    "            params={\"d0\": d0.date(), \"d1\": d1.date(), **({\"limit\": int(limit)} if limit is not None else {})}\n",
    "        )\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"{log_prefix} done  | rows={len(df):,} cols={df.shape[1]} | elapsed={dt:.2f}s\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# smoke test (LIMIT 적용)\n",
    "if CHUNKS:\n",
    "    print(\"[SMOKE] begin\")\n",
    "    d0, d1 = CHUNKS[0]\n",
    "    raw0 = fetch_raw_chunk(ENGINE, d0, d1, limit=SMOKE_TEST_LIMIT, log_prefix=\"[SMOKE/FETCH]\")\n",
    "    print(\"[SMOKE] sample head(3)\")\n",
    "    display(raw0.head(3))\n",
    "    print(\"[SMOKE] end\")"
   ],
   "id": "afeacc124e1377a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_end_ts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"end_day\"] = pd.to_datetime(df[\"end_day\"]).dt.date\n",
    "    df[\"end_time_str\"] = df[\"end_time\"].astype(str).str.slice(0, 8)\n",
    "    df[\"end_ts\"] = pd.to_datetime(df[\"end_day\"].astype(str) + \" \" + df[\"end_time_str\"], errors=\"coerce\")\n",
    "    df[\"test_time_str\"] = df[\"test_time\"].astype(str).str.slice(0, 8)\n",
    "    df[\"test_ts\"] = pd.to_datetime(df[\"end_day\"].astype(str) + \" \" + df[\"test_time_str\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def label_shift(end_ts: pd.Series) -> pd.DataFrame:\n",
    "    ts = pd.to_datetime(end_ts)\n",
    "    day_start = pd.to_datetime(ts.dt.date.astype(str) + \" 08:30:00\")\n",
    "    night_start = pd.to_datetime(ts.dt.date.astype(str) + \" 20:30:00\")\n",
    "    morning_end = pd.to_datetime(ts.dt.date.astype(str) + \" 08:29:59\")\n",
    "\n",
    "    is_day = (ts >= day_start) & (ts < night_start)\n",
    "    shift_name = np.where(is_day, \"DAY\", \"NIGHT\")\n",
    "\n",
    "    morning = (shift_name == \"NIGHT\") & (ts <= morning_end)\n",
    "    shift_date = ts.dt.floor(\"D\") - pd.to_timedelta(morning.astype(int), unit=\"D\")\n",
    "\n",
    "    return pd.DataFrame({\"shift_name\": shift_name, \"shift_date\": shift_date.dt.date})\n",
    "\n",
    "def add_shift_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = make_end_ts(df)\n",
    "    s = label_shift(df[\"end_ts\"])\n",
    "    df[\"shift_name\"] = s[\"shift_name\"].values\n",
    "    df[\"shift_date\"] = s[\"shift_date\"].values\n",
    "    return df\n",
    "\n",
    "raw0_ = add_shift_cols(raw0)\n",
    "raw0_[[\"end_day\",\"end_time_str\",\"end_ts\",\"shift_name\",\"shift_date\"]].head(5)"
   ],
   "id": "2961b91b36abe874"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SPAREPARTS = {\"usb_a\", \"usb_c\", \"mini_b\", \"probe_pin\", \"passmark\", \"pd_board\", \"relay_board\"}\n",
    "\n",
    "def load_step_problem_map(engine: Engine, map_table: Optional[str]) -> pd.DataFrame:\n",
    "    if map_table is None:\n",
    "        return pd.DataFrame(columns=[\"step_description\",\"problem1\",\"problem2\",\"problem3\",\"problem4\"])\n",
    "    q = f'SELECT step_description, problem1, problem2, problem3, problem4 FROM \"{MAP_SCHEMA}\".\"{map_table}\"'\n",
    "    with engine.connect() as conn:\n",
    "        return pd.read_sql(text(q), conn)\n",
    "\n",
    "map_df = load_step_problem_map(ENGINE, MAP_TABLE)\n",
    "print(\"map rows:\", len(map_df))\n",
    "map_df.head(3)"
   ],
   "id": "26d0420d88d5cfb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_tag_columns(df: pd.DataFrame, map_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if '\"group\"' in df.columns and \"group\" not in df.columns:\n",
    "        df = df.rename(columns={'\"group\"': \"group\"})\n",
    "    if \"group\" not in df.columns:\n",
    "        df[\"group\"] = \"\"\n",
    "\n",
    "    df[\"step_description\"] = df[\"step_description\"].astype(str)\n",
    "    if len(map_df) > 0:\n",
    "        m = map_df.copy()\n",
    "        m[\"step_description\"] = m[\"step_description\"].astype(str)\n",
    "        df = df.merge(m, on=\"step_description\", how=\"left\")\n",
    "    else:\n",
    "        for p in [\"problem1\",\"problem2\",\"problem3\",\"problem4\"]:\n",
    "            df[p] = np.nan\n",
    "\n",
    "    def _to_num(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    for p in [\"problem1\",\"problem2\",\"problem3\",\"problem4\"]:\n",
    "        df[p] = df[p].apply(_to_num)\n",
    "\n",
    "    step_lower = df[\"step_description\"].str.lower()\n",
    "    tag2 = np.where(step_lower.str.contains(\"usb_a\"), \"usb_a\",\n",
    "           np.where(step_lower.str.contains(\"usb_c\"), \"usb_c\",\n",
    "           np.where(step_lower.str.contains(\"mini_b\"), \"mini_b\",\n",
    "           np.where(step_lower.str.contains(\"probe\"), \"probe_pin\",\n",
    "           np.where(step_lower.str.contains(\"passmark\"), \"passmark\",\n",
    "           np.where(step_lower.str.contains(\"pd_board\"), \"pd_board\",\n",
    "           np.where(step_lower.str.contains(\"relay\"), \"relay_board\", \"equipment\")))))))\n",
    "    df[\"tag_level2\"] = tag2\n",
    "    df[\"tag_level1\"] = np.where(df[\"tag_level2\"].isin(list(SPAREPARTS)), \"sparepart\", \"equipment\")\n",
    "\n",
    "    df[\"problem_weight\"] = (\n",
    "        df[\"problem1\"].fillna(0)*1.0 +\n",
    "        df[\"problem2\"].fillna(0)*0.7 +\n",
    "        df[\"problem3\"].fillna(0)*0.4 +\n",
    "        df[\"problem4\"].fillna(0)*0.2\n",
    "    )\n",
    "    return df\n",
    "\n",
    "raw0_tag = build_tag_columns(raw0_, map_df)\n",
    "raw0_tag[[\"step_description\",\"problem1\",\"problem2\",\"problem3\",\"problem4\",\"problem_weight\",\"tag_level1\",\"tag_level2\"]].head(10)"
   ],
   "id": "bb0fc15b3e2487a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_fail_deviation(value, minv, maxv) -> float:\n",
    "    try:\n",
    "        v = float(value); mn = float(minv); mx = float(maxv)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    denom = abs(mx - mn) if mx != mn else 1.0\n",
    "    if v < mn: return (mn - v) / denom\n",
    "    if v > mx: return (v - mx) / denom\n",
    "    return 0.0\n",
    "\n",
    "def build_labels(df: pd.DataFrame, group_mode: str, run_fail_n: int = 3, cum_fail_pct: float = 0.03) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"is_fail\"] = (df[\"result\"].astype(str).str.upper() == \"FAIL\").astype(int)\n",
    "    df[\"fail_dev\"] = np.where(\n",
    "        df[\"is_fail\"].eq(1),\n",
    "        df.apply(lambda r: compute_fail_deviation(r.get(\"value\"), r.get(\"min\"), r.get(\"max\")), axis=1),\n",
    "        0.0\n",
    "    )\n",
    "    df = df.sort_values([\"shift_date\",\"station\",\"end_ts\",\"test_ts\"], kind=\"mergesort\")\n",
    "\n",
    "    if group_mode == \"station_step\":\n",
    "        gcols = [\"shift_date\",\"station\",\"step_description\"]\n",
    "    elif group_mode == \"station_tag2\":\n",
    "        gcols = [\"shift_date\",\"station\",\"tag_level2\"]\n",
    "    else:\n",
    "        raise ValueError(\"group_mode must be station_step or station_tag2\")\n",
    "\n",
    "    def _runlen(s):\n",
    "        out = np.zeros(len(s), dtype=int)\n",
    "        cur = 0\n",
    "        for i, v in enumerate(s):\n",
    "            cur = cur + 1 if v == 1 else 0\n",
    "            out[i] = cur\n",
    "        return out\n",
    "\n",
    "    df[\"_run_fail_len\"] = df.groupby(gcols, sort=False)[\"is_fail\"].transform(_runlen)\n",
    "    df[\"_cum_cnt\"] = df.groupby(gcols, sort=False).cumcount() + 1\n",
    "    df[\"_cum_fail\"] = df.groupby(gcols, sort=False)[\"is_fail\"].cumsum()\n",
    "    df[\"_cum_fail_pct\"] = df[\"_cum_fail\"] / df[\"_cum_cnt\"]\n",
    "\n",
    "    df[\"y_run3\"] = (df[\"_run_fail_len\"] >= run_fail_n).astype(int)\n",
    "    df[\"y_pct3\"] = (df[\"_cum_fail_pct\"] >= cum_fail_pct).astype(int)\n",
    "    df[\"y\"] = ((df[\"y_run3\"] == 1) | (df[\"y_pct3\"] == 1)).astype(int)\n",
    "    return df\n",
    "\n",
    "lab_step = build_labels(raw0_tag, \"station_step\")\n",
    "lab_tag2 = build_labels(raw0_tag, \"station_tag2\")\n",
    "\n",
    "raw0_tag[\"is_fail\"] = lab_step[\"is_fail\"].values\n",
    "raw0_tag[\"fail_dev\"] = lab_step[\"fail_dev\"].values\n",
    "raw0_tag[\"y\"] = np.maximum(lab_step[\"y\"].values, lab_tag2[\"y\"].values)\n",
    "\n",
    "raw0_tag[[\"shift_date\",\"station\",\"step_description\",\"tag_level2\",\"end_ts\",\"result\",\"y\"]].tail(10)"
   ],
   "id": "7f98f540b9cf041a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def agg_features_per_barcode(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"set_up_or_test_ct\"] = pd.to_numeric(df[\"set_up_or_test_ct\"], errors=\"coerce\")\n",
    "\n",
    "    def _agg(g: pd.DataFrame) -> pd.Series:\n",
    "        r = g.sort_values(\"end_ts\").iloc[-1]\n",
    "        out = {\n",
    "            \"barcode_information\": r[\"barcode_information\"],\n",
    "            \"group\": r.get(\"group\",\"\"),\n",
    "            \"station\": r.get(\"station\",\"\"),\n",
    "            \"remark\": r.get(\"remark\",\"\"),\n",
    "            \"shift_date\": r.get(\"shift_date\"),\n",
    "            \"shift_name\": r.get(\"shift_name\",\"\"),\n",
    "            \"end_day\": r.get(\"end_day\"),\n",
    "            \"end_time\": r.get(\"end_time_str\",\"\"),\n",
    "            \"end_ts\": r.get(\"end_ts\"),\n",
    "            \"ct_mean\": float(np.nanmean(g[\"set_up_or_test_ct\"].values)) if g[\"set_up_or_test_ct\"].notna().any() else np.nan,\n",
    "            \"ct_max\":  float(np.nanmax(g[\"set_up_or_test_ct\"].values)) if g[\"set_up_or_test_ct\"].notna().any() else np.nan,\n",
    "            \"ct_sum\":  float(np.nansum(g[\"set_up_or_test_ct\"].values)) if g[\"set_up_or_test_ct\"].notna().any() else np.nan,\n",
    "            \"fail_cnt\": int(g[\"is_fail\"].sum()),\n",
    "            \"step_cnt\": int(len(g)),\n",
    "            \"fail_rate\": float(g[\"is_fail\"].mean()),\n",
    "            \"fail_dev_sum\": float(np.nansum(g[\"fail_dev\"].values)),\n",
    "            \"fail_dev_max\": float(np.nanmax(g[\"fail_dev\"].values)) if np.isfinite(np.nanmax(g[\"fail_dev\"].values)) else 0.0,\n",
    "            \"y\": int(g[\"y\"].max()),\n",
    "        }\n",
    "        for tag1, sub in g.groupby(\"tag_level1\"):\n",
    "            out[f\"fail_cnt__tag1__{tag1}\"] = int(sub[\"is_fail\"].sum())\n",
    "            out[f\"fail_rate__tag1__{tag1}\"] = float(sub[\"is_fail\"].mean())\n",
    "        for tag2, sub in g.groupby(\"tag_level2\"):\n",
    "            out[f\"fail_cnt__tag2__{tag2}\"] = int(sub[\"is_fail\"].sum())\n",
    "            out[f\"fail_rate__tag2__{tag2}\"] = float(sub[\"is_fail\"].mean())\n",
    "        return pd.Series(out)\n",
    "\n",
    "    return df.groupby(\"barcode_information\", sort=False).apply(_agg).reset_index(drop=True)\n",
    "\n",
    "feat0 = agg_features_per_barcode(raw0_tag)\n",
    "feat0.head(3), feat0.shape"
   ],
   "id": "fc24ddf5d40aaefe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# [CELL 8] FEATURE STORE BUILD (STREAMING + UPSERT) - FIXED v2\n",
    "# - named cursor 제거: cursor.description None 이슈 원천 차단\n",
    "# - end_day(date/text) 혼재 대응 SELECT 구성\n",
    "# - end_ts 생성 -> shift_date/shift_name 규칙 반영 저장\n",
    "# ============================================================\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "import time, io, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from psycopg2.extras import execute_values\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "# =========================\n",
    "# (A) USER SETTINGS\n",
    "# =========================\n",
    "ML_SCHEMA = \"h_machine_learning\"\n",
    "TBL_FEATURE_STORE = \"1_database_house\"\n",
    "\n",
    "SRC_SCHEMA = \"f_database\"\n",
    "SRC_TABLE  = \"fct_database\"\n",
    "\n",
    "DATE_FROM = \"2025-09-27\"\n",
    "DATE_TO   = \"2025-12-05\"\n",
    "CHUNK_DAYS = 7\n",
    "\n",
    "SMOKE_TEST  = False\n",
    "SMOKE_LIMIT = 50_000\n",
    "\n",
    "RAW_FETCH_ITERSIZE = 50_000\n",
    "RAW_FETCH_MAX_ROWS_PER_CHUNK = 2_000_000\n",
    "\n",
    "PK_COLS = [\"barcode_information\", \"end_day\", \"end_time\"]\n",
    "FORCE_RECREATE_TABLE = False\n",
    "\n",
    "DIRECT_UPSERT_MAX_ROWS = 50_000\n",
    "\n",
    "COPY_CHUNK_START_ROWS = 200_000\n",
    "COPY_CHUNK_FALLBACKS  = [200_000, 100_000, 50_000, 20_000]\n",
    "COPY_RETRY_PER_CHUNK  = 3\n",
    "COPY_RETRY_SLEEP_SEC  = 5\n",
    "\n",
    "STAGING_BY_END_DAY = True\n",
    "\n",
    "SPAREPART_CODES = {\n",
    "    \"usb_a\", \"usb_c\", \"mini_b\", \"probe_pin\", \"passmark\", \"pd_board\", \"relay_board\"\n",
    "}\n",
    "\n",
    "# Shift 규칙(요청)\n",
    "DAY_START_SEC   = 8*3600 + 30*60   # 08:30:00\n",
    "NIGHT_START_SEC = 20*3600 + 30*60  # 20:30:00\n",
    "\n",
    "# =========================\n",
    "# (B) STATE TABLE\n",
    "# =========================\n",
    "def ensure_state_table(engine: Engine, schema: str):\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \"{schema}\";'))\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS \"{schema}\".\"etl_state\"(\n",
    "                state_key TEXT PRIMARY KEY,\n",
    "                state_val TEXT,\n",
    "                updated_date TIMESTAMP DEFAULT NOW()\n",
    "            );\n",
    "        \"\"\"))\n",
    "\n",
    "def get_state(engine: Engine, schema: str, key: str) -> Optional[str]:\n",
    "    ensure_state_table(engine, schema)\n",
    "    with engine.connect() as conn:\n",
    "        r = conn.execute(\n",
    "            text(f'SELECT state_val FROM \"{schema}\".\"etl_state\" WHERE state_key=:k'),\n",
    "            {\"k\": key}\n",
    "        ).fetchone()\n",
    "    return r[0] if r else None\n",
    "\n",
    "def set_state(engine: Engine, schema: str, key: str, val: str):\n",
    "    ensure_state_table(engine, schema)\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"\"\"\n",
    "            INSERT INTO \"{schema}\".\"etl_state\"(state_key, state_val)\n",
    "            VALUES (:k, :v)\n",
    "            ON CONFLICT (state_key) DO UPDATE SET\n",
    "                state_val=EXCLUDED.state_val,\n",
    "                updated_date=NOW();\n",
    "        \"\"\"), {\"k\": key, \"v\": val})\n",
    "\n",
    "# =========================\n",
    "# (C) UTIL: TABLE META\n",
    "# =========================\n",
    "def get_table_columns(engine: Engine, schema: str, table: str) -> List[str]:\n",
    "    q = \"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema=:s AND table_name=:t\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(text(q), conn, params={\"s\": schema, \"t\": table})\n",
    "    return df[\"column_name\"].tolist()\n",
    "\n",
    "def get_column_data_type(engine: Engine, schema: str, table: str, col: str) -> Optional[str]:\n",
    "    q = \"\"\"\n",
    "    SELECT data_type\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema=:s AND table_name=:t AND column_name=:c\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        r = conn.execute(text(q), {\"s\": schema, \"t\": table, \"c\": col}).fetchone()\n",
    "    return r[0] if r else None\n",
    "\n",
    "def q_ident(c: str) -> str:\n",
    "    return f'\"{c}\"'\n",
    "\n",
    "# =========================\n",
    "# (D) TABLE BOOTSTRAP + PK\n",
    "# =========================\n",
    "def _conflict_cols_sql(pk_cols: List[str]) -> str:\n",
    "    return \", \".join([q_ident(c) for c in pk_cols])\n",
    "\n",
    "def ensure_feature_store_table(\n",
    "    engine: Engine,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    pk_cols: Optional[List[str]] = None,\n",
    "    force_recreate: bool = False\n",
    "):\n",
    "    if pk_cols is None:\n",
    "        pk_cols = PK_COLS\n",
    "    pk_sql = _conflict_cols_sql(pk_cols)\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \"{schema}\";'))\n",
    "\n",
    "        if force_recreate:\n",
    "            conn.execute(text(f'DROP TABLE IF EXISTS \"{schema}\".\"{table}\" CASCADE;'))\n",
    "\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS \"{schema}\".\"{table}\"(\n",
    "                \"barcode_information\" TEXT NOT NULL,\n",
    "                \"end_day\" DATE NOT NULL,\n",
    "                \"end_time\" TEXT NOT NULL,\n",
    "\n",
    "                \"shift_date\" DATE,\n",
    "                \"shift_name\" TEXT,\n",
    "\n",
    "                \"y_sparepart\" SMALLINT NOT NULL DEFAULT 0,\n",
    "                \"y_equipment\" SMALLINT NOT NULL DEFAULT 0,\n",
    "                \"y_any\"       SMALLINT NOT NULL DEFAULT 0,\n",
    "\n",
    "                \"event_type\"  TEXT,\n",
    "\n",
    "                \"problem1\" TEXT,\n",
    "                \"problem2\" TEXT,\n",
    "                \"problem3\" TEXT,\n",
    "                \"problem4\" TEXT,\n",
    "\n",
    "                \"end_ts\" TIMESTAMP,\n",
    "\n",
    "                updated_date TIMESTAMP DEFAULT NOW(),\n",
    "                PRIMARY KEY ({pk_sql})\n",
    "            );\n",
    "        \"\"\"))\n",
    "\n",
    "        conn.execute(text(f\"\"\"\n",
    "            ALTER TABLE \"{schema}\".\"{table}\"\n",
    "            ADD COLUMN IF NOT EXISTS updated_date TIMESTAMP DEFAULT NOW();\n",
    "        \"\"\"))\n",
    "\n",
    "def assert_composite_pk(engine: Engine, schema: str, table: str, pk_cols: List[str]):\n",
    "    q = \"\"\"\n",
    "    SELECT kcu.column_name\n",
    "    FROM information_schema.table_constraints tc\n",
    "    JOIN information_schema.key_column_usage kcu\n",
    "      ON tc.constraint_name = kcu.constraint_name\n",
    "     AND tc.table_schema = kcu.table_schema\n",
    "    WHERE tc.table_schema = :s\n",
    "      AND tc.table_name   = :t\n",
    "      AND tc.constraint_type = 'PRIMARY KEY'\n",
    "    ORDER BY kcu.ordinal_position;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(text(q), {\"s\": schema, \"t\": table}).fetchall()\n",
    "    got = [r[0] for r in rows]\n",
    "    if got != pk_cols:\n",
    "        raise RuntimeError(\n",
    "            f\"[PK MISMATCH] expected={pk_cols} but got={got}. \"\n",
    "            f\"기존 테이블 PK가 다르면 DROP 후 재생성하거나 FORCE_RECREATE_TABLE=True로 1회 실행하세요.\"\n",
    "        )\n",
    "\n",
    "# =========================\n",
    "# (E) ALTER ADD COLUMN IF NOT EXISTS\n",
    "# =========================\n",
    "def add_missing_columns(engine: Engine, schema: str, table: str, df: pd.DataFrame):\n",
    "    q = \"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema=:s AND table_name=:t\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        cur = pd.read_sql(text(q), conn, params={\"s\": schema, \"t\": table})\n",
    "    existing = set(cur[\"column_name\"].tolist())\n",
    "\n",
    "    def pg_type(series: pd.Series) -> str:\n",
    "        if pd.api.types.is_integer_dtype(series):\n",
    "            return \"BIGINT\"\n",
    "        if pd.api.types.is_float_dtype(series):\n",
    "            return \"DOUBLE PRECISION\"\n",
    "        if pd.api.types.is_bool_dtype(series):\n",
    "            return \"BOOLEAN\"\n",
    "        if pd.api.types.is_datetime64_any_dtype(series):\n",
    "            return \"TIMESTAMP\"\n",
    "        return \"TEXT\"\n",
    "\n",
    "    alters = []\n",
    "    for c in df.columns:\n",
    "        if c not in existing:\n",
    "            alters.append(f'ADD COLUMN IF NOT EXISTS \"{c}\" {pg_type(df[c])}')\n",
    "\n",
    "    if alters:\n",
    "        sql = f'ALTER TABLE \"{schema}\".\"{table}\" ' + \", \".join(alters) + \";\"\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"\"\"\n",
    "            ALTER TABLE \"{schema}\".\"{table}\"\n",
    "            ADD COLUMN IF NOT EXISTS updated_date TIMESTAMP DEFAULT NOW();\n",
    "        \"\"\"))\n",
    "\n",
    "# =========================\n",
    "# (F) UPSERT (DIRECT)\n",
    "# =========================\n",
    "def _build_conflict_action(cols: List[str], pk_cols: List[str]) -> str:\n",
    "    upd_cols = [c for c in cols if c not in pk_cols and c != \"updated_date\"]\n",
    "    set_parts = [f'{q_ident(c)}=EXCLUDED.{q_ident(c)}' for c in upd_cols]\n",
    "    set_parts.append(\"updated_date=NOW()\")\n",
    "    return \"DO UPDATE SET \" + \", \".join(set_parts)\n",
    "\n",
    "def _direct_upsert_execute_values(\n",
    "    engine: Engine,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    df: pd.DataFrame,\n",
    "    pk_cols: Optional[List[str]] = None\n",
    "):\n",
    "    if pk_cols is None:\n",
    "        pk_cols = PK_COLS\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    missing_pk = [c for c in pk_cols if c not in cols]\n",
    "    if missing_pk:\n",
    "        raise ValueError(f\"[UPSERT] missing PK cols: {missing_pk}\")\n",
    "\n",
    "    conflict_action = _build_conflict_action(cols, pk_cols)\n",
    "\n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO \"{schema}\".\"{table}\"({\",\".join([q_ident(c) for c in cols])})\n",
    "    VALUES %s\n",
    "    ON CONFLICT ({_conflict_cols_sql(pk_cols)}) {conflict_action};\n",
    "    \"\"\"\n",
    "\n",
    "    raw_conn = engine.raw_connection()\n",
    "    try:\n",
    "        with raw_conn.cursor() as cur:\n",
    "            tuples = [\n",
    "                tuple(None if pd.isna(v) else v for v in row)\n",
    "                for row in df.itertuples(index=False, name=None)\n",
    "            ]\n",
    "            execute_values(cur, insert_sql, tuples, page_size=10_000)\n",
    "        raw_conn.commit()\n",
    "    finally:\n",
    "        raw_conn.close()\n",
    "\n",
    "# =========================\n",
    "# (G) COPY + STAGING + UPSERT\n",
    "# =========================\n",
    "def _ensure_unlogged_staging(engine: Engine, schema: str, staging: str, target_table: str):\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE UNLOGGED TABLE IF NOT EXISTS \"{schema}\".\"{staging}\"\n",
    "            (LIKE \"{schema}\".\"{target_table}\" INCLUDING DEFAULTS);\n",
    "        \"\"\"))\n",
    "\n",
    "def _truncate_staging(engine: Engine, schema: str, staging: str):\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f'TRUNCATE TABLE \"{schema}\".\"{staging}\";'))\n",
    "\n",
    "def _copy_dataframe_to_staging(engine: Engine, schema: str, staging: str, df: pd.DataFrame):\n",
    "    cols = list(df.columns)\n",
    "    copy_sql = f\"\"\"\n",
    "    COPY \"{schema}\".\"{staging}\"({\",\".join([q_ident(c) for c in cols])})\n",
    "    FROM STDIN WITH (FORMAT CSV, DELIMITER ',', NULL '', QUOTE '\\\"', ESCAPE '\\\"');\n",
    "    \"\"\"\n",
    "    buf = io.StringIO()\n",
    "    df.to_csv(buf, index=False, header=False, na_rep=\"\", quoting=csv.QUOTE_MINIMAL)\n",
    "    buf.seek(0)\n",
    "\n",
    "    raw_conn = engine.raw_connection()\n",
    "    try:\n",
    "        with raw_conn.cursor() as cur:\n",
    "            cur.copy_expert(copy_sql, buf)\n",
    "        raw_conn.commit()\n",
    "    finally:\n",
    "        raw_conn.close()\n",
    "\n",
    "def _merge_staging_to_target(\n",
    "    engine: Engine,\n",
    "    schema: str,\n",
    "    target: str,\n",
    "    staging: str,\n",
    "    cols: List[str],\n",
    "    pk_cols: Optional[List[str]] = None\n",
    "):\n",
    "    if pk_cols is None:\n",
    "        pk_cols = PK_COLS\n",
    "\n",
    "    conflict_action = _build_conflict_action(cols, pk_cols)\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "    INSERT INTO \"{schema}\".\"{target}\"({\",\".join([q_ident(c) for c in cols])})\n",
    "    SELECT {\",\".join([q_ident(c) for c in cols])}\n",
    "    FROM \"{schema}\".\"{staging}\"\n",
    "    ON CONFLICT ({_conflict_cols_sql(pk_cols)}) {conflict_action};\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(merge_sql))\n",
    "\n",
    "def _copy_loop(engine: Engine, schema: str, table: str, staging_table: str,\n",
    "               df: pd.DataFrame, cols: List[str], pk_cols: List[str], fallbacks: List[int]):\n",
    "    n = len(df)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        remaining = n - i\n",
    "        success = False\n",
    "        last_err = None\n",
    "\n",
    "        sizes_try = [min(s, remaining) for s in fallbacks if s <= remaining]\n",
    "        if not sizes_try:\n",
    "            sizes_try = [remaining]\n",
    "\n",
    "        for size in sizes_try:\n",
    "            for attempt in range(1, COPY_RETRY_PER_CHUNK + 1):\n",
    "                try:\n",
    "                    df_chunk = df.iloc[i:i+size].copy()\n",
    "                    _truncate_staging(engine, schema, staging_table)\n",
    "                    _copy_dataframe_to_staging(engine, schema, staging_table, df_chunk)\n",
    "                    _merge_staging_to_target(engine, schema, table, staging_table, cols, pk_cols=pk_cols)\n",
    "\n",
    "                    print(f\"[COPY-UPSERT] ok | rows={len(df_chunk):,} | chunk={size:,} | at={i:,}/{n:,}\")\n",
    "                    i += size\n",
    "                    success = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    last_err = e\n",
    "                    print(f\"[COPY-UPSERT][WARN] fail | chunk={size:,} | attempt={attempt}/{COPY_RETRY_PER_CHUNK} | \"\n",
    "                          f\"err={type(e).__name__}: {e}\")\n",
    "                    if attempt < COPY_RETRY_PER_CHUNK:\n",
    "                        time.sleep(COPY_RETRY_SLEEP_SEC)\n",
    "\n",
    "            if success:\n",
    "                break\n",
    "            else:\n",
    "                print(\"[COPY-UPSERT] downshift -> next smaller chunk\")\n",
    "\n",
    "        if not success:\n",
    "            raise RuntimeError(f\"[COPY-UPSERT][FATAL] all retries failed at i={i}/{n}. last_err={last_err}\")\n",
    "\n",
    "def _copy_staging_upsert(\n",
    "    engine: Engine,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    df: pd.DataFrame,\n",
    "    pk_cols: Optional[List[str]] = None,\n",
    "    staging_table: Optional[str] = None,\n",
    "    chunk_rows: int = COPY_CHUNK_START_ROWS\n",
    "):\n",
    "    if pk_cols is None:\n",
    "        pk_cols = PK_COLS\n",
    "    if staging_table is None:\n",
    "        staging_table = f\"stg__{table}\"\n",
    "\n",
    "    _ensure_unlogged_staging(engine, schema, staging_table, table)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "\n",
    "    fallbacks = [chunk_rows] + [s for s in COPY_CHUNK_FALLBACKS if s != chunk_rows]\n",
    "    fallbacks = [s for s in fallbacks if s > 0]\n",
    "\n",
    "    if STAGING_BY_END_DAY and \"end_day\" in df.columns:\n",
    "        df2 = df.copy()\n",
    "        df2[\"__end_day__\"] = pd.to_datetime(df2[\"end_day\"], errors=\"coerce\").dt.date\n",
    "        days = [d for d in sorted(df2[\"__end_day__\"].dropna().unique())]\n",
    "        print(f\"[COPY-UPSERT] STAGING_BY_END_DAY=True | days={len(days)}\")\n",
    "\n",
    "        total = 0\n",
    "        for d in days:\n",
    "            sub = df2[df2[\"__end_day__\"] == d].drop(columns=[\"__end_day__\"])\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "            print(f\"[DAY] {d} | rows={len(sub):,}\")\n",
    "            _copy_loop(engine, schema, table, staging_table, sub, cols, pk_cols, fallbacks)\n",
    "            total += len(sub)\n",
    "\n",
    "        tail = df2[df2[\"__end_day__\"].isna()].drop(columns=[\"__end_day__\"])\n",
    "        if len(tail) > 0:\n",
    "            print(f\"[DAY] NaT | rows={len(tail):,}\")\n",
    "            _copy_loop(engine, schema, table, staging_table, tail, cols, pk_cols, fallbacks)\n",
    "            total += len(tail)\n",
    "\n",
    "        print(f\"[COPY-UPSERT] done by day | total_rows={total:,}\")\n",
    "        return\n",
    "\n",
    "    _copy_loop(engine, schema, table, staging_table, df, cols, pk_cols, fallbacks)\n",
    "\n",
    "def upsert_dataframe(\n",
    "    engine: Engine,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    df: pd.DataFrame,\n",
    "    pk_cols=None,\n",
    "    chunk_rows: int = COPY_CHUNK_START_ROWS,\n",
    "    force_recreate: bool = False\n",
    "):\n",
    "    if pk_cols is None:\n",
    "        pk_cols = PK_COLS\n",
    "    if df is None or len(df) == 0:\n",
    "        return\n",
    "\n",
    "    ensure_feature_store_table(engine, schema, table, pk_cols=pk_cols, force_recreate=force_recreate)\n",
    "    assert_composite_pk(engine, schema, table, pk_cols)\n",
    "    add_missing_columns(engine, schema, table, df)\n",
    "\n",
    "    n = len(df)\n",
    "    if n <= DIRECT_UPSERT_MAX_ROWS:\n",
    "        print(f\"[UPSERT] DIRECT execute_values | rows={n:,}\")\n",
    "        _direct_upsert_execute_values(engine, schema, table, df, pk_cols=pk_cols)\n",
    "    else:\n",
    "        print(f\"[UPSERT] COPY+STAGING | rows={n:,} | start_chunk={chunk_rows:,} | fallbacks={COPY_CHUNK_FALLBACKS}\")\n",
    "        _copy_staging_upsert(engine, schema, table, df, pk_cols=pk_cols, staging_table=f\"stg__{table}\", chunk_rows=chunk_rows)\n",
    "\n",
    "# =========================\n",
    "# (H) FEATURE BUILD\n",
    "# =========================\n",
    "def _stringify_problem(v) -> str:\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    s = str(v).strip()\n",
    "    if s.lower() in (\"nan\", \"none\", \"null\"):\n",
    "        return \"\"\n",
    "    return s\n",
    "\n",
    "def _infer_event_type_from_problems(pvals: List[str]) -> str:\n",
    "    vals = [v.lower() for v in pvals if v]\n",
    "    if any(v in SPAREPART_CODES for v in vals):\n",
    "        return \"sparepart\"\n",
    "    if any(len(v) > 0 for v in vals):\n",
    "        return \"equipment\"\n",
    "    return \"\"\n",
    "\n",
    "def apply_shift_rules(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    ts = pd.to_datetime(df[\"end_ts\"], errors=\"coerce\")\n",
    "    sec = ts.dt.hour * 3600 + ts.dt.minute * 60 + ts.dt.second\n",
    "\n",
    "    is_day = (sec >= DAY_START_SEC) & (sec < NIGHT_START_SEC)\n",
    "    is_night_same_day = (sec >= NIGHT_START_SEC)\n",
    "\n",
    "    df[\"shift_name\"] = np.where(is_day, \"DAY\", \"NIGHT\")\n",
    "    df[\"shift_date\"] = np.where(\n",
    "        is_day | is_night_same_day,\n",
    "        ts.dt.date,\n",
    "        (ts - pd.Timedelta(days=1)).dt.date\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def _normalize_end_time_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"00:00:00\"\n",
    "    x = str(s).strip()\n",
    "    if not x:\n",
    "        return \"00:00:00\"\n",
    "    return x[:8] if len(x) >= 8 else \"00:00:00\"\n",
    "\n",
    "def build_select_query(engine: Engine) -> str:\n",
    "    src_cols = set(get_table_columns(engine, SRC_SCHEMA, SRC_TABLE))\n",
    "\n",
    "    end_day_type = get_column_data_type(engine, SRC_SCHEMA, SRC_TABLE, \"end_day\")\n",
    "    end_time_type = get_column_data_type(engine, SRC_SCHEMA, SRC_TABLE, \"end_time\")\n",
    "\n",
    "    # end_day expr\n",
    "    if end_day_type == \"date\":\n",
    "        end_day_expr = '\"end_day\"'\n",
    "        where_day_expr = '\"end_day\"'\n",
    "        order_day_expr = '\"end_day\"'\n",
    "    else:\n",
    "        # TEXT 'YYYYMMDD' 가정\n",
    "        end_day_expr = 'to_date(\"end_day\", \\'YYYYMMDD\\')'\n",
    "        where_day_expr = end_day_expr\n",
    "        order_day_expr = end_day_expr\n",
    "\n",
    "    # end_time expr\n",
    "    if end_time_type and \"time\" in end_time_type:\n",
    "        end_time_expr = 'to_char(\"end_time\", \\'HH24:MI:SS\\')'\n",
    "        order_time_expr = end_time_expr\n",
    "    else:\n",
    "        end_time_expr = 'substring(trim(\"end_time\") from 1 for 8)'\n",
    "        order_time_expr = end_time_expr\n",
    "\n",
    "    base_candidates = [\n",
    "        \"barcode_information\",\n",
    "        \"group\",\n",
    "        \"station\", \"remark\",\n",
    "        \"step_description\",\n",
    "        \"set_up_or_test_ct\",\n",
    "        \"value\",\n",
    "        \"min\", \"max\",\n",
    "        \"result\",\n",
    "        \"problem1\", \"problem2\", \"problem3\", \"problem4\",\n",
    "    ]\n",
    "    select_cols = [c for c in base_candidates if c in src_cols]\n",
    "\n",
    "    sel = []\n",
    "    sel.append(f'{end_day_expr} AS \"end_day\"')\n",
    "    sel.append(f'{end_time_expr} AS \"end_time\"')\n",
    "    for c in select_cols:\n",
    "        if c in (\"end_day\", \"end_time\"):\n",
    "            continue\n",
    "        sel.append(q_ident(c))\n",
    "\n",
    "    cols_sql = \", \".join(sel)\n",
    "\n",
    "    q = f\"\"\"\n",
    "    SELECT {cols_sql}\n",
    "    FROM \"{SRC_SCHEMA}\".\"{SRC_TABLE}\"\n",
    "    WHERE {where_day_expr} >= %(d0)s AND {where_day_expr} <= %(d1)s\n",
    "      AND step_description IS NOT NULL\n",
    "    ORDER BY {order_day_expr} ASC, {order_time_expr} ASC, barcode_information ASC\n",
    "    \"\"\"\n",
    "    return q\n",
    "\n",
    "def stream_raw_and_build_features(\n",
    "    engine: Engine,\n",
    "    d0: pd.Timestamp,\n",
    "    d1: pd.Timestamp,\n",
    "    itersize: int,\n",
    "    max_rows: Optional[int]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    q = build_select_query(engine)\n",
    "\n",
    "    feat_rows: List[dict] = []\n",
    "    rows_seen = 0\n",
    "\n",
    "    raw_conn = engine.raw_connection()\n",
    "    try:\n",
    "        # ✅ named cursor 제거 (핵심 수정)\n",
    "        cur = raw_conn.cursor()\n",
    "        cur.execute(q, {\"d0\": d0.date(), \"d1\": d1.date()})\n",
    "\n",
    "        # SELECT면 rows가 0이어도 description은 반드시 있어야 정상\n",
    "        if cur.description is None:\n",
    "            # 여기 오면: 드라이버/세션 상태 문제일 가능성이 큼 -> statusmessage 찍고 종료\n",
    "            status = getattr(cur, \"statusmessage\", None)\n",
    "            raise RuntimeError(f\"[CELL8] SELECT executed but cursor.description is None. status={status}\")\n",
    "\n",
    "        colnames = [d[0] for d in cur.description]\n",
    "\n",
    "        def flush_group(buf: List[dict]):\n",
    "            if not buf:\n",
    "                return\n",
    "            gdf = pd.DataFrame(buf)\n",
    "\n",
    "            barcode = str(gdf.get(\"barcode_information\").iloc[0]) if \"barcode_information\" in gdf.columns else \"\"\n",
    "            end_day  = gdf.get(\"end_day\").iloc[0] if \"end_day\" in gdf.columns else None\n",
    "            end_time = _normalize_end_time_text(gdf.get(\"end_time\").iloc[0] if \"end_time\" in gdf.columns else \"\")\n",
    "\n",
    "            grp = gdf.get(\"group\").iloc[0] if \"group\" in gdf.columns else None\n",
    "            station = gdf.get(\"station\").iloc[0] if \"station\" in gdf.columns else None\n",
    "            remark  = gdf.get(\"remark\").iloc[0] if \"remark\" in gdf.columns else None\n",
    "\n",
    "            ct_vals = pd.to_numeric(gdf.get(\"set_up_or_test_ct\"), errors=\"coerce\").dropna() if \"set_up_or_test_ct\" in gdf.columns else pd.Series([], dtype=float)\n",
    "\n",
    "            if \"result\" in gdf.columns:\n",
    "                fail_mask = gdf[\"result\"].astype(str).str.upper().eq(\"FAIL\")\n",
    "                fail_cnt = int(fail_mask.sum())\n",
    "            else:\n",
    "                fail_cnt = 0\n",
    "\n",
    "            step_cnt = int(gdf[\"step_description\"].notna().sum()) if \"step_description\" in gdf.columns else 0\n",
    "\n",
    "            v = pd.to_numeric(gdf.get(\"value\"), errors=\"coerce\") if \"value\" in gdf.columns else pd.Series([np.nan]*len(gdf))\n",
    "            vmin = pd.to_numeric(gdf.get(\"min\"), errors=\"coerce\") if \"min\" in gdf.columns else pd.Series([np.nan]*len(gdf))\n",
    "            vmax = pd.to_numeric(gdf.get(\"max\"), errors=\"coerce\") if \"max\" in gdf.columns else pd.Series([np.nan]*len(gdf))\n",
    "\n",
    "            valid_spec = v.notna() & vmin.notna() & vmax.notna()\n",
    "            dev = pd.Series(False, index=gdf.index)\n",
    "            if valid_spec.any():\n",
    "                dev.loc[valid_spec] = (v.loc[valid_spec] < vmin.loc[valid_spec]) | (v.loc[valid_spec] > vmax.loc[valid_spec])\n",
    "\n",
    "            specout_sum = float(dev.astype(np.int8).sum()) if len(dev) else 0.0\n",
    "            specout_max = float(dev.astype(np.int8).max()) if len(dev) else 0.0\n",
    "\n",
    "            vv = v.dropna()\n",
    "            if len(vv) >= 1:\n",
    "                value_mean = float(vv.mean())\n",
    "                value_std  = float(vv.std(ddof=0)) if len(vv) > 1 else 0.0\n",
    "                value_amp  = float(vv.max() - vv.min()) if len(vv) > 1 else 0.0\n",
    "                dv = vv.diff().dropna()\n",
    "                value_diff_mean = float(dv.mean()) if len(dv) else 0.0\n",
    "                value_diff_std  = float(dv.std(ddof=0)) if len(dv) > 1 else 0.0\n",
    "            else:\n",
    "                value_mean = value_std = value_amp = value_diff_mean = value_diff_std = 0.0\n",
    "\n",
    "            is_event = (fail_cnt > 0) or (specout_sum > 0)\n",
    "\n",
    "            p1 = _stringify_problem(gdf[\"problem1\"].iloc[0]) if \"problem1\" in gdf.columns else \"\"\n",
    "            p2 = _stringify_problem(gdf[\"problem2\"].iloc[0]) if \"problem2\" in gdf.columns else \"\"\n",
    "            p3 = _stringify_problem(gdf[\"problem3\"].iloc[0]) if \"problem3\" in gdf.columns else \"\"\n",
    "            p4 = _stringify_problem(gdf[\"problem4\"].iloc[0]) if \"problem4\" in gdf.columns else \"\"\n",
    "\n",
    "            event_type = _infer_event_type_from_problems([p1,p2,p3,p4])\n",
    "            if is_event and event_type == \"\":\n",
    "                event_type = \"equipment\"\n",
    "\n",
    "            y_sparepart = 1 if (is_event and event_type == \"sparepart\") else 0\n",
    "            y_equipment = 1 if (is_event and event_type == \"equipment\") else 0\n",
    "            y_any       = 1 if is_event else 0\n",
    "\n",
    "            feat_rows.append({\n",
    "                \"barcode_information\": barcode,\n",
    "                \"end_day\": end_day,\n",
    "                \"end_time\": end_time,\n",
    "\n",
    "                \"group\": grp,\n",
    "                \"station\": station,\n",
    "                \"remark\": remark,\n",
    "\n",
    "                \"problem1\": p1, \"problem2\": p2, \"problem3\": p3, \"problem4\": p4,\n",
    "\n",
    "                \"end_ts\": None,\n",
    "\n",
    "                \"ct_mean\": float(ct_vals.mean()) if len(ct_vals) else 0.0,\n",
    "                \"ct_max\":  float(ct_vals.max())  if len(ct_vals) else 0.0,\n",
    "                \"ct_sum\":  float(ct_vals.sum())  if len(ct_vals) else 0.0,\n",
    "                \"step_cnt\": float(step_cnt),\n",
    "\n",
    "                \"value_mean\": value_mean,\n",
    "                \"value_std\": value_std,\n",
    "                \"value_amp\": value_amp,\n",
    "                \"value_diff_mean\": value_diff_mean,\n",
    "                \"value_diff_std\": value_diff_std,\n",
    "\n",
    "                \"fail_cnt\": float(fail_cnt),\n",
    "                \"fail_rate\": float(fail_cnt / step_cnt) if step_cnt > 0 else 0.0,\n",
    "                \"fail_dev_sum\": float(specout_sum),\n",
    "                \"fail_dev_max\": float(specout_max),\n",
    "\n",
    "                \"event_type\": event_type,\n",
    "                \"y_sparepart\": int(y_sparepart),\n",
    "                \"y_equipment\": int(y_equipment),\n",
    "                \"y_any\": int(y_any),\n",
    "            })\n",
    "\n",
    "        current_key = None\n",
    "        group_buf: List[dict] = []\n",
    "\n",
    "        while True:\n",
    "            rows = cur.fetchmany(int(itersize))\n",
    "            if not rows:\n",
    "                break\n",
    "\n",
    "            for row in rows:\n",
    "                rec = dict(zip(colnames, row))\n",
    "                k = (\n",
    "                    str(rec.get(\"barcode_information\", \"\")),\n",
    "                    str(rec.get(\"end_day\", \"\")),\n",
    "                    str(rec.get(\"end_time\", \"\")),\n",
    "                )\n",
    "\n",
    "                if current_key is None:\n",
    "                    current_key = k\n",
    "\n",
    "                if k != current_key:\n",
    "                    flush_group(group_buf)\n",
    "                    group_buf = []\n",
    "                    current_key = k\n",
    "\n",
    "                group_buf.append(rec)\n",
    "\n",
    "                rows_seen += 1\n",
    "                if max_rows is not None and rows_seen >= max_rows:\n",
    "                    break\n",
    "\n",
    "            if max_rows is not None and rows_seen >= max_rows:\n",
    "                break\n",
    "\n",
    "        if group_buf:\n",
    "            flush_group(group_buf)\n",
    "\n",
    "        raw_conn.commit()\n",
    "    finally:\n",
    "        try:\n",
    "            raw_conn.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    feat_df = pd.DataFrame(feat_rows)\n",
    "    if len(feat_df) == 0:\n",
    "        return feat_df\n",
    "\n",
    "    feat_df = feat_df.copy()\n",
    "    feat_df[\"end_day\"]  = pd.to_datetime(feat_df[\"end_day\"], errors=\"coerce\").dt.date\n",
    "    feat_df[\"end_time\"] = feat_df[\"end_time\"].astype(str).map(_normalize_end_time_text)\n",
    "\n",
    "    day_s = pd.to_datetime(feat_df[\"end_day\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    t_s   = feat_df[\"end_time\"].astype(str).str.slice(0, 8).fillna(\"00:00:00\")\n",
    "    feat_df[\"end_ts\"] = pd.to_datetime(day_s + \" \" + t_s, errors=\"coerce\")\n",
    "\n",
    "    feat_df = apply_shift_rules(feat_df)\n",
    "\n",
    "    for yc in [\"y_sparepart\", \"y_equipment\", \"y_any\"]:\n",
    "        if yc not in feat_df.columns:\n",
    "            feat_df[yc] = 0\n",
    "        feat_df[yc] = (\n",
    "            pd.to_numeric(feat_df[yc], errors=\"coerce\")\n",
    "              .replace([np.inf, -np.inf], np.nan)\n",
    "              .fillna(0)\n",
    "              .astype(np.int16)\n",
    "        )\n",
    "\n",
    "    return feat_df\n",
    "\n",
    "# =========================\n",
    "# (I) CHUNKS\n",
    "# =========================\n",
    "def build_chunks(date_from: str, date_to: str, chunk_days: int = 7):\n",
    "    d0 = pd.Timestamp(date_from)\n",
    "    d1 = pd.Timestamp(date_to)\n",
    "    chunks = []\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        end = min(cur + pd.Timedelta(days=chunk_days-1), d1)\n",
    "        chunks.append((cur, end))\n",
    "        cur = end + pd.Timedelta(days=1)\n",
    "    return chunks\n",
    "\n",
    "CHUNKS = build_chunks(DATE_FROM, DATE_TO, CHUNK_DAYS)\n",
    "\n",
    "# =========================\n",
    "# (J) RUNNER\n",
    "# =========================\n",
    "STATE_KEY = f\"cell8_feature_store_done_until::{SRC_SCHEMA}.{SRC_TABLE}\"\n",
    "\n",
    "def run_feature_store_build_streaming(engine: Engine):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[CELL8] FEATURE STORE BUILD START (SHIFT FIXED v2)\")\n",
    "    print(f\"SRC: {SRC_SCHEMA}.{SRC_TABLE} | OUT: {ML_SCHEMA}.{TBL_FEATURE_STORE}\")\n",
    "    print(f\"DATE_FROM={DATE_FROM} | DATE_TO={DATE_TO} | chunks={len(CHUNKS)}\")\n",
    "    print(f\"SMOKE_TEST={SMOKE_TEST} | ITESIZE={RAW_FETCH_ITERSIZE:,} | MAX_ROWS={RAW_FETCH_MAX_ROWS_PER_CHUNK:,}\")\n",
    "    print(f\"FORCE_RECREATE_TABLE={FORCE_RECREATE_TABLE}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    last_done = get_state(engine, ML_SCHEMA, STATE_KEY)\n",
    "    last_done = pd.to_datetime(last_done).date() if last_done else None\n",
    "    print(\"[STATE] last_done:\", last_done)\n",
    "\n",
    "    for (d0, d1) in CHUNKS:\n",
    "        if last_done and d1.date() <= last_done:\n",
    "            print(f\"[SKIP] {d0.date()}~{d1.date()} already done\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"[CHUNK] {d0.date()} ~ {d1.date()}\")\n",
    "        max_rows = SMOKE_LIMIT if SMOKE_TEST else RAW_FETCH_MAX_ROWS_PER_CHUNK\n",
    "\n",
    "        feat_df = stream_raw_and_build_features(\n",
    "            engine,\n",
    "            d0, d1,\n",
    "            itersize=RAW_FETCH_ITERSIZE,\n",
    "            max_rows=max_rows\n",
    "        )\n",
    "\n",
    "        print(f\"[BUILD] rows={len(feat_df):,} cols={feat_df.shape[1] if len(feat_df)>0 else 0}\")\n",
    "        if len(feat_df) == 0:\n",
    "            set_state(engine, ML_SCHEMA, STATE_KEY, str(d1.date()))\n",
    "            print(\"[CHUNK] empty -> state updated:\", d1.date())\n",
    "            continue\n",
    "\n",
    "        print(\"[CHECK] shift_name null:\", int(pd.Series(feat_df[\"shift_name\"]).isna().sum()), \"/\", len(feat_df))\n",
    "        print(\"[CHECK] shift_date null:\", int(pd.to_datetime(feat_df[\"shift_date\"], errors=\"coerce\").isna().sum()), \"/\", len(feat_df))\n",
    "        print(\"[CHECK] end_ts NaT:\", int(pd.to_datetime(feat_df[\"end_ts\"], errors=\"coerce\").isna().sum()), \"/\", len(feat_df))\n",
    "\n",
    "        upsert_dataframe(\n",
    "            engine,\n",
    "            ML_SCHEMA,\n",
    "            TBL_FEATURE_STORE,\n",
    "            feat_df,\n",
    "            pk_cols=PK_COLS,\n",
    "            chunk_rows=COPY_CHUNK_START_ROWS,\n",
    "            force_recreate=FORCE_RECREATE_TABLE\n",
    "        )\n",
    "\n",
    "        set_state(engine, ML_SCHEMA, STATE_KEY, str(d1.date()))\n",
    "        print(\"[CHUNK] saved rows:\", len(feat_df), \"| state:\", d1.date())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[CELL8] DONE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# =========================\n",
    "# (K) EXEC\n",
    "# =========================\n",
    "run_feature_store_build_streaming(ENGINE)"
   ],
   "id": "b9bf0f4333d9b3d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# [CELL 9] B안 완결: Feature Store Load -> end_ts 보정 -> y_future 생성(censor) -> Split -> Z-score(train-only)\n",
    "# - FUTURE_HOURS = [12, 6, 3, 1] 루프\n",
    "# - 결과 요약(res_df) + 각 FUTURE_HOURS별 학습 세트/피처 저장(DATASETS)\n",
    "#\n",
    "# 산출물(다음 Cell10에서 그대로 사용)\n",
    "# - DATASETS[FH] = {\n",
    "#     \"fs\": usable_df,\n",
    "#     \"feature_cols\": [...],\n",
    "#     \"train_df\",\"val_df\",\"test_df\",\n",
    "#     \"X_train_z\",\"X_val_z\",\"X_test_z\",\n",
    "#     \"y_train\",\"y_val\",\"y_test\",\n",
    "#     \"zstats\"\n",
    "#   }\n",
    "# ============================================================\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "\n",
    "# =========================\n",
    "# (0) USER SETTINGS\n",
    "# =========================\n",
    "ML_SCHEMA = \"h_machine_learning\"\n",
    "TBL_FEATURE_STORE = \"1_database_house\"\n",
    "\n",
    "DATE_FROM = \"2025-09-29\"\n",
    "DATE_TO   = \"2025-10-04\"\n",
    "\n",
    "EVENT_COL = \"y\"\n",
    "FUTURE_HOURS_LIST = [12, 6, 3, 1]\n",
    "\n",
    "TEST_DAYS = 3\n",
    "VAL_RATIO_DAYS = 0.2\n",
    "MIN_POS_VAL = 30\n",
    "\n",
    "\n",
    "# =========================\n",
    "# (1) Utils\n",
    "# =========================\n",
    "def ensure_end_ts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"end_ts\" not in df.columns:\n",
    "        df[\"end_ts\"] = pd.NaT\n",
    "\n",
    "    # end_time 텍스트 정리\n",
    "    if \"end_time\" in df.columns:\n",
    "        df[\"end_time_str\"] = df[\"end_time\"].astype(str).str.slice(0, 8)\n",
    "    else:\n",
    "        df[\"end_time_str\"] = \"00:00:00\"\n",
    "\n",
    "    end_ts_fallback = pd.to_datetime(\n",
    "        df[\"end_day\"].astype(str) + \" \" + df[\"end_time_str\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    df[\"end_ts\"] = pd.to_datetime(df[\"end_ts\"], errors=\"coerce\").fillna(end_ts_fallback)\n",
    "    return df\n",
    "\n",
    "def build_y_future_with_censor(df: pd.DataFrame, future_hours: int, event_col: str = \"y\") -> pd.DataFrame:\n",
    "    df = df.sort_values(\"end_ts\", kind=\"mergesort\").reset_index(drop=True).copy()\n",
    "\n",
    "    ev = (\n",
    "        pd.to_numeric(df[event_col], errors=\"coerce\")\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .fillna(0)\n",
    "        .astype(np.int8)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "    ts = pd.to_datetime(df[\"end_ts\"], errors=\"coerce\").to_numpy(dtype=\"datetime64[ns]\")\n",
    "    max_ts = ts.max()\n",
    "\n",
    "    cut_ts = np.datetime64(pd.Timestamp(max_ts).to_datetime64()) - np.timedelta64(int(future_hours * 3600), \"s\")\n",
    "    can_label = ts <= cut_ts\n",
    "\n",
    "    n = len(df)\n",
    "    next_after = np.full(n, -1, dtype=np.int64)\n",
    "    nearest_pos = -1\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        next_after[i] = nearest_pos\n",
    "        if ev[i] == 1:\n",
    "            nearest_pos = i\n",
    "\n",
    "    horizon = ts + np.timedelta64(int(future_hours * 3600), \"s\")\n",
    "    y_future = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    has_next = next_after >= 0\n",
    "    if has_next.any():\n",
    "        next_ts = ts[next_after[has_next]]\n",
    "        y_future[has_next] = (next_ts <= horizon[has_next]).astype(np.float32)\n",
    "\n",
    "    y_future = y_future.astype(\"float32\")\n",
    "    y_future[~can_label] = np.nan\n",
    "\n",
    "    df[\"y_future\"] = y_future\n",
    "    df[\"__cut_ts__\"] = pd.to_datetime(cut_ts)\n",
    "    return df\n",
    "\n",
    "def day_split_with_min_pos(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str,\n",
    "    test_days: int,\n",
    "    val_ratio_days: float,\n",
    "    min_pos_val: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    df = df.copy()\n",
    "    df[\"end_day_date\"] = pd.to_datetime(df[\"end_day\"], errors=\"coerce\").dt.date\n",
    "    df = df.dropna(subset=[\"end_day_date\"]).copy()\n",
    "\n",
    "    all_days = sorted(df[\"end_day_date\"].unique())\n",
    "    info = {\"unique_days\": len(all_days)}\n",
    "\n",
    "    # ✅ 부족하면 에러 대신 \"스킵용\" 예외를 던져 caller에서 skip\n",
    "    if len(all_days) < (test_days + 2):\n",
    "        raise ValueError(\n",
    "            f\"[SPLIT ERROR] unique days={len(all_days)} < test_days+2 ({test_days+2}). \"\n",
    "            f\"DATE_FROM/TO를 늘리거나 TEST_DAYS를 줄이세요.\"\n",
    "        )\n",
    "\n",
    "    test_set_days = set(all_days[-test_days:])\n",
    "    trainval_days = all_days[:-test_days]\n",
    "\n",
    "    val_days_n = int(np.ceil(len(trainval_days) * val_ratio_days))\n",
    "    val_days_n = max(1, min(val_days_n, len(trainval_days) - 1))\n",
    "\n",
    "    def val_pos_count(k_from: int) -> int:\n",
    "        val_days = set(trainval_days[k_from:])\n",
    "        return int(df.loc[df[\"end_day_date\"].isin(val_days), label_col].sum())\n",
    "\n",
    "    split_at = len(trainval_days) - val_days_n\n",
    "    split_at = max(1, min(split_at, len(trainval_days) - 1))\n",
    "\n",
    "    if val_pos_count(split_at) < min_pos_val:\n",
    "        for k in range(split_at - 1, 0, -1):\n",
    "            if val_pos_count(k) >= min_pos_val:\n",
    "                split_at = k\n",
    "                break\n",
    "\n",
    "    val_days = set(trainval_days[split_at:])\n",
    "    train_days = set(trainval_days[:split_at])\n",
    "\n",
    "    train_df = df[df[\"end_day_date\"].isin(train_days)].copy()\n",
    "    val_df   = df[df[\"end_day_date\"].isin(val_days)].copy()\n",
    "    test_df  = df[df[\"end_day_date\"].isin(test_set_days)].copy()\n",
    "\n",
    "    info.update({\n",
    "        \"train_days\": len(train_days),\n",
    "        \"val_days\": len(val_days),\n",
    "        \"test_days\": len(test_set_days),\n",
    "        \"train_pos\": int(train_df[label_col].sum()),\n",
    "        \"val_pos\": int(val_df[label_col].sum()),\n",
    "        \"test_pos\": int(test_df[label_col].sum()),\n",
    "        \"train_n\": len(train_df),\n",
    "        \"val_n\": len(val_df),\n",
    "        \"test_n\": len(test_df),\n",
    "    })\n",
    "    return train_df, val_df, test_df, info\n",
    "\n",
    "def pick_features(df: pd.DataFrame, label_col: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    숫자형만 사용 + 누수 의심 제거\n",
    "    \"\"\"\n",
    "    drop_exact = {\n",
    "        \"barcode_information\", \"end_day\", \"end_time\", \"end_ts\",\n",
    "        \"updated_date\", \"end_time_str\", \"end_day_date\",\n",
    "        label_col, EVENT_COL, \"__cut_ts__\",\n",
    "    }\n",
    "    leaky_keywords = [\"fail\", \"dev\", \"result\", \"y_future\", \"y\"]  # ct/value는 남김\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    kept, dropped = [], []\n",
    "    for c in num_cols:\n",
    "        if c in drop_exact:\n",
    "            dropped.append(c); continue\n",
    "        cl = c.lower()\n",
    "        if any(k in cl for k in leaky_keywords):\n",
    "            dropped.append(c); continue\n",
    "        kept.append(c)\n",
    "    return kept, dropped\n",
    "\n",
    "def zscore_fit_transform_train_only(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    eps: float = 1e-9\n",
    "):\n",
    "    mu = train_df[feature_cols].mean(axis=0)\n",
    "    sd = train_df[feature_cols].std(axis=0, ddof=0).replace(0, np.nan).fillna(1.0)\n",
    "\n",
    "    X_train_z = (train_df[feature_cols] - mu) / (sd + eps)\n",
    "    X_val_z   = (val_df[feature_cols]   - mu) / (sd + eps)\n",
    "    X_test_z  = (test_df[feature_cols]  - mu) / (sd + eps)\n",
    "\n",
    "    zstats = pd.DataFrame({\"feature\": feature_cols, \"mu\": mu.values, \"sigma\": sd.values})\n",
    "    return X_train_z, X_val_z, X_test_z, zstats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# (2) Load feature store\n",
    "# =========================\n",
    "q = text(f\"\"\"\n",
    "SELECT *\n",
    "FROM \"{ML_SCHEMA}\".\"{TBL_FEATURE_STORE}\"\n",
    "WHERE end_day BETWEEN :d0 AND :d1\n",
    "ORDER BY end_day ASC, end_time ASC, barcode_information ASC\n",
    "\"\"\")\n",
    "\n",
    "fs_all = pd.read_sql(q, ENGINE, params={\"d0\": DATE_FROM, \"d1\": DATE_TO})\n",
    "print(\"[LOAD] rows:\", len(fs_all), \"| cols:\", fs_all.shape[1])\n",
    "\n",
    "fs_all = ensure_end_ts(fs_all)\n",
    "\n",
    "# event(y) 보정\n",
    "if EVENT_COL not in fs_all.columns:\n",
    "    fs_all[EVENT_COL] = 0\n",
    "fs_all[EVENT_COL] = (\n",
    "    pd.to_numeric(fs_all[EVENT_COL], errors=\"coerce\")\n",
    "    .replace([np.inf, -np.inf], np.nan)\n",
    "    .fillna(0)\n",
    "    .astype(np.int8)\n",
    ")\n",
    "\n",
    "print(\"[DBG] end_ts NaT:\", int(fs_all[\"end_ts\"].isna().sum()), \"/\", len(fs_all))\n",
    "print(\"[DBG] end_ts min/max:\", fs_all[\"end_ts\"].min(), fs_all[\"end_ts\"].max())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# (3) Main loop\n",
    "# =========================\n",
    "DATASETS = {}\n",
    "rows = []\n",
    "\n",
    "for FUTURE_HOURS in FUTURE_HOURS_LIST:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"[LOOP] FUTURE_HOURS={FUTURE_HOURS}\")\n",
    "\n",
    "    fsH = build_y_future_with_censor(fs_all, FUTURE_HOURS, event_col=EVENT_COL)\n",
    "\n",
    "    # usable만 학습에 사용\n",
    "    fs = fsH.dropna(subset=[\"y_future\"]).copy()\n",
    "    fs[\"y_future\"] = (\n",
    "        pd.to_numeric(fs[\"y_future\"], errors=\"coerce\")\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .fillna(0)\n",
    "        .astype(np.int8)\n",
    "    )\n",
    "\n",
    "    print(f\"[LABEL] rows: {len(fsH)} -> {len(fs)} | y_future mean:\", float(fs[\"y_future\"].mean()) if len(fs) else float(\"nan\"))\n",
    "    print(\"[LABEL] cut_ts:\", fsH[\"__cut_ts__\"].iloc[0] if len(fsH) else None)\n",
    "\n",
    "    if len(fs) == 0:\n",
    "        print(\"[SKIP] no usable rows after censor.\")\n",
    "        continue\n",
    "\n",
    "    # split\n",
    "    try:\n",
    "        train_df, val_df, test_df, split_info = day_split_with_min_pos(\n",
    "            fs, label_col=\"y_future\",\n",
    "            test_days=TEST_DAYS,\n",
    "            val_ratio_days=VAL_RATIO_DAYS,\n",
    "            min_pos_val=MIN_POS_VAL\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"[SKIP] split failed:\", type(e).__name__, e)\n",
    "        continue\n",
    "\n",
    "    print(\"[SPLIT]\", split_info)\n",
    "\n",
    "    # feature 선정\n",
    "    feature_cols, dropped = pick_features(fs, label_col=\"y_future\")\n",
    "    print(\"[FEATURE] cols:\", len(feature_cols), \"| dropped(leaky):\", dropped[:10], (\"...\" if len(dropped) > 10 else \"\"))\n",
    "\n",
    "    # numeric 강제 + NaN/inf 정리\n",
    "    for df_ in (train_df, val_df, test_df):\n",
    "        for c in feature_cols:\n",
    "            if df_[c].dtype == \"O\":\n",
    "                df_[c] = pd.to_numeric(df_[c], errors=\"coerce\")\n",
    "        df_[feature_cols] = df_[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    y_train = train_df[\"y_future\"].to_numpy(dtype=np.int8)\n",
    "    y_val   = val_df[\"y_future\"].to_numpy(dtype=np.int8)\n",
    "    y_test  = test_df[\"y_future\"].to_numpy(dtype=np.int8)\n",
    "\n",
    "    print(\"y_train pos/total:\", int(y_train.sum()), \"/\", len(y_train))\n",
    "    print(\"y_val   pos/total:\", int(y_val.sum()), \"/\", len(y_val))\n",
    "    print(\"y_test  pos/total:\", int(y_test.sum()), \"/\", len(y_test))\n",
    "\n",
    "    # Z-score (train-only)\n",
    "    X_train_z, X_val_z, X_test_z, zstats = zscore_fit_transform_train_only(\n",
    "        train_df, val_df, test_df, feature_cols\n",
    "    )\n",
    "\n",
    "    # 저장\n",
    "    DATASETS[FUTURE_HOURS] = {\n",
    "        \"fs\": fs,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"train_df\": train_df, \"val_df\": val_df, \"test_df\": test_df,\n",
    "        \"X_train_z\": X_train_z, \"X_val_z\": X_val_z, \"X_test_z\": X_test_z,\n",
    "        \"y_train\": y_train, \"y_val\": y_val, \"y_test\": y_test,\n",
    "        \"zstats\": zstats,\n",
    "        \"split_info\": split_info,\n",
    "    }\n",
    "\n",
    "    rows.append({\n",
    "        \"future_hours\": FUTURE_HOURS,\n",
    "        \"rows_all\": len(fsH),\n",
    "        \"rows_usable\": len(fs),\n",
    "        \"y_future_mean\": float(fs[\"y_future\"].mean()),\n",
    "        \"unique_days\": split_info[\"unique_days\"],\n",
    "        \"train_n\": split_info[\"train_n\"], \"val_n\": split_info[\"val_n\"], \"test_n\": split_info[\"test_n\"],\n",
    "        \"train_pos\": split_info[\"train_pos\"], \"val_pos\": split_info[\"val_pos\"], \"test_pos\": split_info[\"test_pos\"],\n",
    "        \"features\": len(feature_cols),\n",
    "        \"cut_ts\": str(fsH[\"__cut_ts__\"].iloc[0]),\n",
    "    })\n",
    "\n",
    "# summary\n",
    "res_df = pd.DataFrame(rows).sort_values(\"future_hours\", ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[CELL9 RESULTS]\")\n",
    "display(res_df)\n",
    "\n",
    "# 다음 Cell10에서 편하게 쓰라고 \"기본 선택\"도 하나 만들어둠:\n",
    "# - 의미 있게 나온 FUTURE_HOURS(예: 1)를 우선으로 잡되, 없으면 가장 큰 usable을 선택\n",
    "if len(DATASETS) > 0:\n",
    "    preferred = 1 if 1 in DATASETS else None\n",
    "    if preferred is None:\n",
    "        preferred = sorted(DATASETS.keys(), key=lambda h: len(DATASETS[h][\"fs\"]), reverse=True)[0]\n",
    "\n",
    "    SEL_FUTURE_HOURS = preferred\n",
    "    print(\"[SELECT] SEL_FUTURE_HOURS =\", SEL_FUTURE_HOURS)\n",
    "\n",
    "    fs_sel       = DATASETS[SEL_FUTURE_HOURS][\"fs\"]\n",
    "    feature_cols = DATASETS[SEL_FUTURE_HOURS][\"feature_cols\"]\n",
    "    train_df     = DATASETS[SEL_FUTURE_HOURS][\"train_df\"]\n",
    "    val_df       = DATASETS[SEL_FUTURE_HOURS][\"val_df\"]\n",
    "    test_df      = DATASETS[SEL_FUTURE_HOURS][\"test_df\"]\n",
    "    X_train_z    = DATASETS[SEL_FUTURE_HOURS][\"X_train_z\"]\n",
    "    X_val_z      = DATASETS[SEL_FUTURE_HOURS][\"X_val_z\"]\n",
    "    X_test_z     = DATASETS[SEL_FUTURE_HOURS][\"X_test_z\"]\n",
    "    y_train      = DATASETS[SEL_FUTURE_HOURS][\"y_train\"]\n",
    "    y_val        = DATASETS[SEL_FUTURE_HOURS][\"y_val\"]\n",
    "    y_test       = DATASETS[SEL_FUTURE_HOURS][\"y_test\"]\n",
    "    zstats       = DATASETS[SEL_FUTURE_HOURS][\"zstats\"]\n",
    "\n",
    "    print(\"[READY] train/val/test:\", X_train_z.shape, X_val_z.shape, X_test_z.shape)\n",
    "else:\n",
    "    print(\"[WARN] DATASETS empty. DATE_FROM/TO를 늘리거나 FUTURE_HOURS를 줄이세요.\")"
   ],
   "id": "abe40d8a8f1df8a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================\n",
    "# [CELL10 v3] B-rule LightGBM + Track A(수명가중 임계값)\n",
    "#  - 라벨: event_stream_v2에서 trigger=B + problem_group=sparepart만 사용\n",
    "#  - 모델: LightGBM로 \"1시간 내 fail_event(=B-rule 이벤트)\" 위험도 점수 산출\n",
    "#  - 정책: life_ratio 기반 동적 min_prob(threshold) 적용\n",
    "#\n",
    "# (중요) Track A 테이블 컬럼은 자동 탐지(fallback 포함)\n",
    "# ============================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import date, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support\n",
    "\n",
    "# --- [ADD] plotting ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# =========================\n",
    "# (0) CONFIG\n",
    "# =========================\n",
    "ML_SCHEMA   = \"h_machine_learning\"\n",
    "EVT_SCHEMA  = \"h_machine_learning\"\n",
    "EVT_TABLE   = \"event_stream_v2\"\n",
    "STATE_TABLE = \"etl_state_cell10_v2\"\n",
    "STATE_KEY   = \"cell10_event_stream_v2\"\n",
    "\n",
    "SRC_SCHEMA  = \"f_database\"\n",
    "SRC_TABLE   = \"fct_database\"\n",
    "\n",
    "MAP_SCHEMA  = \"f_database\"\n",
    "MAP_TABLE   = \"step_description_problem\"\n",
    "\n",
    "# feature_store\n",
    "FS_SCHEMA   = \"h_machine_learning\"\n",
    "FS_TABLE    = \"1_database_house\"\n",
    "\n",
    "# Track A (수명)\n",
    "LIFE_SCHEMA = \"e3_sparepart_replacement\"\n",
    "LIFE_TABLE  = \"sparepart_life_amount\"\n",
    "\n",
    "# 기간\n",
    "DATE_FROM = pd.Timestamp(\"2025-09-27\")\n",
    "DATE_TO   = pd.Timestamp(\"2025-12-05\")\n",
    "\n",
    "# 라벨 horizon\n",
    "FH = 1  # 1시간\n",
    "\n",
    "# event build\n",
    "FETCH_LIMIT = 200_000\n",
    "FORCE_REBUILD_EVENT_STREAM = False\n",
    "\n",
    "# B-rule(3% 이상 FAIL) 관련\n",
    "B_RATE_THRESHOLD = 0.03\n",
    "PROMO_WINDOW_HOURS = 12\n",
    "MAX_LEVEL = 4\n",
    "\n",
    "# shift 정의\n",
    "DAY_START = time(8, 30, 0)\n",
    "DAY_END   = time(20, 29, 59)\n",
    "\n",
    "# A-rule은 폐기(희소)\n",
    "USE_A_RULE = False\n",
    "\n",
    "# \"problem2/3/4가 equipment이면 알람 제외\"를 event 생성 단계에서 반영할지 여부\n",
    "# True면: level>=2에서 pv==\"equipment\"인 이벤트는 아예 생성/학습 라벨에서 제거됨\n",
    "SILENT_EXCLUDE_EQUIPMENT_LV2P = True\n",
    "\n",
    "# 모델 학습 split\n",
    "VAL_DAYS  = 7\n",
    "TEST_DAYS = 14\n",
    "MIN_TRAIN_DAYS = 21\n",
    "\n",
    "# =========================\n",
    "# (1) 정책: 수명가중 임계값(동적 min_prob)\n",
    "# =========================\n",
    "LIFE_THR_RULES = [\n",
    "    # (min_ratio_inclusive, min_prob)\n",
    "    (0.00, 0.90),\n",
    "    (0.50, 0.70),\n",
    "    (0.70, 0.55),\n",
    "    (0.90, 0.35),\n",
    "    (1.00, 0.25),\n",
    "]\n",
    "\n",
    "def min_prob_by_life_ratio(r: float) -> float:\n",
    "    if r is None or not np.isfinite(r):\n",
    "        return LIFE_THR_RULES[0][1]\n",
    "    r = float(r)\n",
    "    thr = LIFE_THR_RULES[0][1]\n",
    "    for ratio0, mp in LIFE_THR_RULES:\n",
    "        if r >= ratio0:\n",
    "            thr = mp\n",
    "    return float(thr)\n",
    "\n",
    "# =========================\n",
    "# (2) UTIL\n",
    "# =========================\n",
    "def _now() -> pd.Timestamp:\n",
    "    return pd.Timestamp.now()\n",
    "\n",
    "def _fmt_sec(sec: float) -> str:\n",
    "    if sec < 60:\n",
    "        return f\"{sec:.2f}s\"\n",
    "    if sec < 3600:\n",
    "        return f\"{sec/60:.2f}m\"\n",
    "    return f\"{sec/3600:.2f}h\"\n",
    "\n",
    "def _shift_of(ts: pd.Timestamp) -> Tuple[str, date]:\n",
    "    t = ts.time()\n",
    "    if DAY_START <= t <= DAY_END:\n",
    "        return \"DAY\", ts.date()\n",
    "    if t >= time(20,30,0):\n",
    "        return \"NIGHT\", ts.date()\n",
    "    return \"NIGHT\", (ts - pd.Timedelta(days=1)).date()\n",
    "\n",
    "def _get_cols(engine: Engine, schema: str, table: str) -> List[str]:\n",
    "    q = \"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema=:s AND table_name=:t\n",
    "    ORDER BY ordinal_position\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(text(q), {\"s\": schema, \"t\": table}).fetchall()\n",
    "    return [r[0] for r in rows]\n",
    "\n",
    "def _get_col_meta(engine: Engine, schema: str, table: str) -> Dict[str, dict]:\n",
    "    q = \"\"\"\n",
    "    SELECT column_name, is_nullable, data_type\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema=:s AND table_name=:t\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(text(q), {\"s\": schema, \"t\": table}).fetchall()\n",
    "    meta = {}\n",
    "    for c, is_nullable, data_type in rows:\n",
    "        meta[c] = {\n",
    "            \"notnull\": (str(is_nullable).upper() == \"NO\"),\n",
    "            \"data_type\": str(data_type).lower(),\n",
    "        }\n",
    "    return meta\n",
    "\n",
    "def _default_by_type(data_type: str):\n",
    "    dt = (data_type or \"\").lower()\n",
    "    if \"int\" in dt:\n",
    "        return 0\n",
    "    if dt in (\"numeric\", \"decimal\", \"real\", \"double precision\"):\n",
    "        return 0.0\n",
    "    if \"timestamp\" in dt:\n",
    "        return None\n",
    "    if dt == \"date\":\n",
    "        return None\n",
    "    if dt == \"boolean\":\n",
    "        return False\n",
    "    return \"\"\n",
    "\n",
    "def _safe_auc(y, p):\n",
    "    y = np.asarray(y)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        return np.nan\n",
    "    return roc_auc_score(y, p)\n",
    "\n",
    "def _confusion_counts(y_true, y_prob, thr):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_hat  = (np.asarray(y_prob, dtype=float) >= float(thr)).astype(int)\n",
    "    tp = int(((y_true == 1) & (y_hat == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_hat == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_hat == 0)).sum())\n",
    "    fn = int(((y_true == 1) & (y_hat == 0)).sum())\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def _prf(tp, fp, fn):\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "    return float(prec), float(rec), float(f1)\n",
    "\n",
    "# =========================\n",
    "# (2-1) [ADD] Plot helpers (Seaborn styled)\n",
    "# =========================\n",
    "def _sns_style():\n",
    "    try:\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def plot_roc_curves(yva, pva, yte, pte, val_auc: float, test_auc: float):\n",
    "    _sns_style()\n",
    "    fpr_v, tpr_v, _ = roc_curve(yva, pva)\n",
    "    fpr_t, tpr_t, _ = roc_curve(yte, pte)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(fpr_v, tpr_v, label=f\"VAL  AUC={val_auc:.4f}\")\n",
    "    plt.plot(fpr_t, tpr_t, label=f\"TEST AUC={test_auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (VAL / TEST)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_auc_learning_curve(evals_result: dict, best_iter: int):\n",
    "    _sns_style()\n",
    "    tr_auc = evals_result.get(\"train\", {}).get(\"auc\", None)\n",
    "    va_auc = evals_result.get(\"val\", {}).get(\"auc\", None)\n",
    "\n",
    "    if not tr_auc or not va_auc:\n",
    "        print(\"[WARN] evals_result에 auc 기록이 없습니다. lgb.record_evaluation(evals_result) 콜백을 확인하세요.\")\n",
    "        return\n",
    "\n",
    "    iters = np.arange(1, len(tr_auc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "    plt.plot(iters, tr_auc, label=\"train auc\")\n",
    "    plt.plot(iters, va_auc, label=\"val auc\")\n",
    "    if best_iter is not None and best_iter > 0:\n",
    "        plt.axvline(int(best_iter), linestyle=\"--\", label=f\"best_iter={best_iter}\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"AUC Learning Curve (train vs val)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =========================\n",
    "# (3) event_stream_v2 ensure + state\n",
    "# =========================\n",
    "def ensure_event_stream_v2(engine: Engine):\n",
    "    ddl_create = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS \"{EVT_SCHEMA}\".\"{EVT_TABLE}\" (\n",
    "        id BIGSERIAL PRIMARY KEY,\n",
    "        event_ts TIMESTAMP NOT NULL,\n",
    "        trigger_type TEXT NOT NULL,\n",
    "        level INTEGER NOT NULL,\n",
    "\n",
    "        problem_level INTEGER NOT NULL DEFAULT 1,\n",
    "        action_type TEXT NOT NULL DEFAULT 'UNKNOWN',\n",
    "\n",
    "        station TEXT NOT NULL,\n",
    "        remark TEXT NOT NULL,\n",
    "        shift_name TEXT NOT NULL,\n",
    "        shift_date DATE NOT NULL,\n",
    "\n",
    "        step_description TEXT,\n",
    "        step_description_nn TEXT NOT NULL DEFAULT '',\n",
    "\n",
    "        problem_value TEXT,\n",
    "        problem_group TEXT,\n",
    "\n",
    "        metric_fail_streak INTEGER NOT NULL DEFAULT 0,\n",
    "        metric_fail_rate DOUBLE PRECISION NOT NULL DEFAULT 0.0,\n",
    "\n",
    "        promo_count INTEGER NOT NULL DEFAULT 0,\n",
    "\n",
    "        base_key TEXT NOT NULL DEFAULT '',\n",
    "        trig_rule TEXT NOT NULL DEFAULT '',\n",
    "        a_consec INTEGER NOT NULL DEFAULT 0,\n",
    "        b_fail_rate DOUBLE PRECISION NOT NULL DEFAULT 0.0,\n",
    "        recurrence_count INTEGER NOT NULL DEFAULT 0,\n",
    "        created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n",
    "        updated_at TIMESTAMP NOT NULL DEFAULT NOW()\n",
    "    )\n",
    "    \"\"\"\n",
    "    ddl_uq = f\"\"\"\n",
    "    CREATE UNIQUE INDEX IF NOT EXISTS event_stream_v2_uq\n",
    "    ON \"{EVT_SCHEMA}\".\"{EVT_TABLE}\"\n",
    "    (event_ts, trigger_type, level, station, remark, shift_name, shift_date, step_description_nn)\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl_create))\n",
    "        conn.execute(text(ddl_uq))\n",
    "\n",
    "def ensure_state_table_v2(engine: Engine):\n",
    "    ddl_create = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS \"{ML_SCHEMA}\".\"{STATE_TABLE}\" (\n",
    "        \"key\" TEXT PRIMARY KEY,\n",
    "        cursor_end_ts TIMESTAMP NOT NULL,\n",
    "        cursor_group_i BIGINT NOT NULL DEFAULT 0,\n",
    "        updated_at TIMESTAMP NOT NULL DEFAULT NOW()\n",
    "    )\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl_create))\n",
    "\n",
    "@dataclass\n",
    "class CursorState:\n",
    "    end_ts: pd.Timestamp\n",
    "    grp_i: int\n",
    "\n",
    "def load_state(engine: Engine, key: str) -> Optional[CursorState]:\n",
    "    q = f\"\"\"\n",
    "    SELECT cursor_end_ts, cursor_group_i\n",
    "    FROM \"{ML_SCHEMA}\".\"{STATE_TABLE}\"\n",
    "    WHERE \"key\"=:k\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        r = conn.execute(text(q), {\"k\": key}).fetchone()\n",
    "    if not r or r[0] is None:\n",
    "        return None\n",
    "    return CursorState(pd.to_datetime(r[0]), int(r[1] or 0))\n",
    "\n",
    "def save_state(engine: Engine, key: str, st: CursorState):\n",
    "    q = f\"\"\"\n",
    "    INSERT INTO \"{ML_SCHEMA}\".\"{STATE_TABLE}\" (\"key\", cursor_end_ts, cursor_group_i, updated_at)\n",
    "    VALUES (:k, :ts, :gi, NOW())\n",
    "    ON CONFLICT (\"key\")\n",
    "    DO UPDATE SET cursor_end_ts=EXCLUDED.cursor_end_ts, cursor_group_i=EXCLUDED.cursor_group_i, updated_at=NOW()\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(q), {\"k\": key, \"ts\": st.end_ts.to_pydatetime(), \"gi\": int(st.grp_i)})\n",
    "\n",
    "def reset_event_stream_range(engine: Engine, d0: pd.Timestamp, d1: pd.Timestamp):\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(f'DELETE FROM \"{EVT_SCHEMA}\".\"{EVT_TABLE}\" WHERE event_ts >= :t0 AND event_ts < :t1'),\n",
    "            {\"t0\": d0.to_pydatetime(), \"t1\": (d1 + pd.Timedelta(days=1)).to_pydatetime()},\n",
    "        )\n",
    "        conn.execute(text(f'DELETE FROM \"{ML_SCHEMA}\".\"{STATE_TABLE}\" WHERE \"key\"=:k'), {\"k\": STATE_KEY})\n",
    "\n",
    "# =========================\n",
    "# (4) step_description -> problem1..4 mapping\n",
    "# =========================\n",
    "def load_step_problem_map(engine: Engine) -> Dict[str, Tuple[Optional[str],Optional[str],Optional[str],Optional[str]]]:\n",
    "    cols = set(_get_cols(engine, MAP_SCHEMA, MAP_TABLE))\n",
    "    need = {\"step_description\",\"problem1\",\"problem2\",\"problem3\",\"problem4\"}\n",
    "    missing = sorted(list(need - cols))\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[CELL10] step_description_problem missing required columns: {missing}\")\n",
    "\n",
    "    q = f\"\"\"\n",
    "    SELECT step_description, problem1, problem2, problem3, problem4\n",
    "    FROM \"{MAP_SCHEMA}\".\"{MAP_TABLE}\"\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(text(q), conn)\n",
    "\n",
    "    df[\"step_description\"] = df[\"step_description\"].astype(str).str.strip()\n",
    "    for c in [\"problem1\",\"problem2\",\"problem3\",\"problem4\"]:\n",
    "        df[c] = df[c].astype(str).str.strip().str.lower().replace({\"nan\": None, \"\": None})\n",
    "\n",
    "    mp: Dict[str, Tuple[Optional[str],Optional[str],Optional[str],Optional[str]]] = {}\n",
    "    for _, r in df.iterrows():\n",
    "        mp[r[\"step_description\"]] = (r[\"problem1\"], r[\"problem2\"], r[\"problem3\"], r[\"problem4\"])\n",
    "    return mp\n",
    "\n",
    "# =========================\n",
    "# (5) Raw fetch SQL (cursor)\n",
    "# =========================\n",
    "def build_cursor_sql() -> str:\n",
    "    q = f\"\"\"\n",
    "    WITH base AS (\n",
    "        SELECT\n",
    "            (end_day::date + end_time::time) AS end_ts,\n",
    "            \"group\"::bigint AS grp_i,\n",
    "            station::text AS station,\n",
    "            remark::text AS remark,\n",
    "            step_description::text AS step_description,\n",
    "            result::text AS result,\n",
    "            value AS value\n",
    "        FROM \"{SRC_SCHEMA}\".\"{SRC_TABLE}\"\n",
    "        WHERE\n",
    "            (end_day::date + end_time::time) >= :t0\n",
    "            AND (end_day::date + end_time::time) <  :t1\n",
    "            AND (\n",
    "                (end_day::date + end_time::time) > :c_ts\n",
    "                OR (\n",
    "                    (end_day::date + end_time::time) = :c_ts\n",
    "                    AND \"group\"::bigint > :c_gi\n",
    "                )\n",
    "            )\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM base\n",
    "    ORDER BY end_ts ASC, grp_i ASC\n",
    "    LIMIT :lim\n",
    "    \"\"\"\n",
    "    return q\n",
    "\n",
    "# =========================\n",
    "# (6) Event helpers\n",
    "# =========================\n",
    "SPAREPART_SET = {\"usb_a\",\"usb_c\",\"mini_b\",\"probe_pin\",\"passmark\",\"pd_board\",\"relay_board\"}\n",
    "\n",
    "def _problem_group(problem_value: Optional[str]) -> Optional[str]:\n",
    "    if problem_value is None:\n",
    "        return None\n",
    "    pv = str(problem_value).strip().lower()\n",
    "    if pv == \"\" or pv == \"nan\":\n",
    "        return None\n",
    "    if pv == \"product\":\n",
    "        return \"product\"\n",
    "    if pv in SPAREPART_SET:\n",
    "        return \"sparepart\"\n",
    "    if pv == \"equipment\":\n",
    "        return \"equipment\"\n",
    "    return \"equipment\"\n",
    "\n",
    "def _action_type(problem_group: Optional[str]) -> str:\n",
    "    if problem_group == \"sparepart\":\n",
    "        return \"SPAREPART_CHANGE\"\n",
    "    if problem_group == \"equipment\":\n",
    "        return \"EQUIPMENT_REVIEW\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def _promote_level(last_level: int) -> int:\n",
    "    return int(min(MAX_LEVEL, last_level + 1))\n",
    "\n",
    "def _norm_step_desc(step_desc) -> Tuple[Optional[str], str]:\n",
    "    step_desc = None if pd.isna(step_desc) else str(step_desc)\n",
    "    payload_step = None if step_desc is None else step_desc\n",
    "    step_desc_nn = \"\" if payload_step is None else str(payload_step)\n",
    "    return payload_step, step_desc_nn\n",
    "\n",
    "def _make_base_key(station: str, remark: str, shift_name: str, shift_date: date, step_desc_nn: str, trigger_type: str) -> str:\n",
    "    return f\"{station}|{remark}|{shift_name}|{shift_date.isoformat()}|{trigger_type}|{step_desc_nn}\"\n",
    "\n",
    "def _get_last_event(engine: Engine, trigger_type: str, station: str, remark: str, shift_name: str, shift_date: date) -> Optional[Tuple[pd.Timestamp,int,int]]:\n",
    "    q = f\"\"\"\n",
    "    SELECT event_ts, level, promo_count\n",
    "    FROM \"{EVT_SCHEMA}\".\"{EVT_TABLE}\"\n",
    "    WHERE trigger_type=:tt\n",
    "      AND station=:st AND remark=:rm\n",
    "      AND shift_name=:sn AND shift_date=:sd\n",
    "    ORDER BY event_ts DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        r = conn.execute(\n",
    "            text(q),\n",
    "            {\"tt\": trigger_type, \"st\": station, \"rm\": remark, \"sn\": shift_name, \"sd\": shift_date}\n",
    "        ).fetchone()\n",
    "    if not r:\n",
    "        return None\n",
    "    return (pd.to_datetime(r[0]), int(r[1]), int(r[2]))\n",
    "\n",
    "def _insert_events(engine: Engine, rows: List[dict]):\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    cols_in_table = _get_cols(engine, EVT_SCHEMA, EVT_TABLE)\n",
    "    meta = _get_col_meta(engine, EVT_SCHEMA, EVT_TABLE)\n",
    "\n",
    "    for r in rows:\n",
    "        event_ts = r.get(\"event_ts\")\n",
    "        for c, m in meta.items():\n",
    "            if c in (\"id\", \"updated_at\"):\n",
    "                continue\n",
    "            if not m[\"notnull\"]:\n",
    "                continue\n",
    "            if c not in r or r[c] is None:\n",
    "                dt = m[\"data_type\"]\n",
    "                if \"timestamp\" in dt:\n",
    "                    r[c] = event_ts\n",
    "                    continue\n",
    "                if dt == \"date\":\n",
    "                    if c == \"shift_date\":\n",
    "                        r[\"shift_date\"] = r.get(\"shift_date\") or DATE_FROM.date()\n",
    "                        continue\n",
    "                    r[c] = DATE_FROM.date()\n",
    "                    continue\n",
    "                if c == \"problem_level\":\n",
    "                    r[c] = int(r.get(\"level\", 1) or 1)\n",
    "                    continue\n",
    "                if c == \"action_type\":\n",
    "                    r[c] = str(r.get(\"action_type\") or \"UNKNOWN\")\n",
    "                    continue\n",
    "                if c == \"step_description_nn\":\n",
    "                    r[c] = \"\" if r.get(\"step_description\") is None else str(r.get(\"step_description\"))\n",
    "                    continue\n",
    "                if c == \"promo_count\":\n",
    "                    r[c] = int(r.get(\"promo_count\", 0) or 0)\n",
    "                    continue\n",
    "                if c == \"metric_fail_streak\":\n",
    "                    r[c] = int(r.get(\"metric_fail_streak\", 0) or 0)\n",
    "                    continue\n",
    "                if c == \"metric_fail_rate\":\n",
    "                    r[c] = float(r.get(\"metric_fail_rate\", 0.0) or 0.0)\n",
    "                    continue\n",
    "                if c == \"recurrence_count\":\n",
    "                    r[c] = int(r.get(\"recurrence_count\", r.get(\"promo_count\", 0)) or 0)\n",
    "                    continue\n",
    "\n",
    "                base = _default_by_type(dt)\n",
    "                r[c] = (event_ts if base is None else base)\n",
    "\n",
    "    use_cols = [c for c in cols_in_table if c in rows[0] and c not in (\"id\", \"updated_at\")]\n",
    "    col_sql = \", \".join([f'\"{c}\"' for c in use_cols])\n",
    "    val_sql = \", \".join([f\":{c}\" for c in use_cols])\n",
    "\n",
    "    ins = f\"\"\"\n",
    "    INSERT INTO \"{EVT_SCHEMA}\".\"{EVT_TABLE}\" ({col_sql}, updated_at)\n",
    "    VALUES ({val_sql}, NOW())\n",
    "    ON CONFLICT DO NOTHING\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ins), rows)\n",
    "\n",
    "# =========================\n",
    "# (7) Build event_stream_v2 (B-rule only)\n",
    "# =========================\n",
    "def build_event_stream_cursor_incremental(engine: Engine):\n",
    "    ensure_event_stream_v2(engine)\n",
    "    ensure_state_table_v2(engine)\n",
    "\n",
    "    if FORCE_REBUILD_EVENT_STREAM:\n",
    "        print(f\"[EVENT_STREAM] FORCE_REBUILD_EVENT_STREAM=True -> clear range + reset state\")\n",
    "        reset_event_stream_range(engine, DATE_FROM, DATE_TO)\n",
    "        save_state(engine, STATE_KEY, CursorState(end_ts=DATE_FROM - pd.Timedelta(seconds=1), grp_i=0))\n",
    "\n",
    "    st = load_state(engine, STATE_KEY)\n",
    "    if st is None:\n",
    "        st = CursorState(end_ts=DATE_FROM - pd.Timedelta(seconds=1), grp_i=0)\n",
    "        save_state(engine, STATE_KEY, st)\n",
    "\n",
    "    mp_map = load_step_problem_map(engine)\n",
    "\n",
    "    q = build_cursor_sql()\n",
    "    t0 = DATE_FROM\n",
    "    t1 = DATE_TO + pd.Timedelta(days=1)\n",
    "\n",
    "    cursor_ts = st.end_ts\n",
    "    cursor_gi = st.grp_i\n",
    "\n",
    "    total_rows = 0\n",
    "    total_events = 0\n",
    "    t_start = _now()\n",
    "\n",
    "    b_state: Dict[Tuple[str,str,str,date,str], Tuple[int,int,Optional[pd.Timestamp]]] = {}\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"t0\": t0.to_pydatetime(),\n",
    "            \"t1\": t1.to_pydatetime(),\n",
    "            \"c_ts\": cursor_ts.to_pydatetime(),\n",
    "            \"c_gi\": int(cursor_gi),\n",
    "            \"lim\": int(FETCH_LIMIT),\n",
    "        }\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql(text(q), conn, params=params)\n",
    "\n",
    "        if df is None or len(df) == 0:\n",
    "            break\n",
    "\n",
    "        df[\"end_ts\"] = pd.to_datetime(df[\"end_ts\"], errors=\"coerce\")\n",
    "        df[\"station\"] = df[\"station\"].astype(str)\n",
    "        df[\"remark\"] = df[\"remark\"].astype(str)\n",
    "        df[\"step_description\"] = df[\"step_description\"].astype(str).str.strip()\n",
    "        df[\"result\"] = df[\"result\"].astype(str).str.upper()\n",
    "\n",
    "        ev_rows: List[dict] = []\n",
    "\n",
    "        for _, rr in df.iterrows():\n",
    "            ts = rr[\"end_ts\"]\n",
    "            if pd.isna(ts):\n",
    "                continue\n",
    "\n",
    "            station = str(rr[\"station\"])\n",
    "            remark  = str(rr[\"remark\"])\n",
    "            step_desc_raw = str(rr[\"step_description\"]).strip()\n",
    "            result = str(rr[\"result\"]).upper()\n",
    "\n",
    "            shift_name, shift_date = _shift_of(ts)\n",
    "            key = (station, remark, shift_name, shift_date, step_desc_raw)\n",
    "\n",
    "            p1, p2, p3, p4 = mp_map.get(step_desc_raw, (None, None, None, None))\n",
    "\n",
    "            total_cnt, fail_cnt, last_trig_ts = b_state.get(key, (0, 0, None))\n",
    "            if last_trig_ts is not None and ts > last_trig_ts:\n",
    "                total_cnt, fail_cnt = (0, 0)\n",
    "                last_trig_ts = None\n",
    "\n",
    "            total_cnt += 1\n",
    "            if result == \"FAIL\":\n",
    "                fail_cnt += 1\n",
    "            rate = (fail_cnt / total_cnt) if total_cnt > 0 else 0.0\n",
    "\n",
    "            if rate > B_RATE_THRESHOLD:\n",
    "                last = _get_last_event(engine, \"B\", station, remark, shift_name, shift_date)\n",
    "                level = 1\n",
    "                promo_count = 0\n",
    "                if last is not None:\n",
    "                    last_ts, last_lv, last_pc = last\n",
    "                    if ts <= last_ts + pd.Timedelta(hours=PROMO_WINDOW_HOURS):\n",
    "                        level = _promote_level(last_lv)\n",
    "                        promo_count = last_pc + 1\n",
    "\n",
    "                pv = {1: p1, 2: p2, 3: p3, 4: p4}.get(level, p4)\n",
    "                pg = _problem_group(pv)\n",
    "\n",
    "                if pg == \"product\":\n",
    "                    b_state[key] = (int(total_cnt), int(fail_cnt), last_trig_ts)\n",
    "                    continue\n",
    "\n",
    "                if SILENT_EXCLUDE_EQUIPMENT_LV2P and int(level) >= 2 and (pv == \"equipment\" or pg == \"equipment\"):\n",
    "                    last_trig_ts = ts\n",
    "                    b_state[key] = (int(total_cnt), int(fail_cnt), last_trig_ts)\n",
    "                    continue\n",
    "\n",
    "                step_desc, step_desc_nn = _norm_step_desc(step_desc_raw)\n",
    "\n",
    "                base_key = _make_base_key(station, remark, shift_name, shift_date, step_desc_nn, \"B\")\n",
    "                trig_rule = \"B_rate>0.03\"\n",
    "                a_consec = 0\n",
    "                b_fail_rate = rate\n",
    "                recurrence_count = promo_count\n",
    "                created_at = ts.to_pydatetime()\n",
    "\n",
    "                ev_rows.append({\n",
    "                    \"event_ts\": ts.to_pydatetime(),\n",
    "                    \"trigger_type\": \"B\",\n",
    "                    \"level\": int(level),\n",
    "\n",
    "                    \"problem_level\": int(level),\n",
    "                    \"problem_value\": pv,\n",
    "                    \"action_type\": _action_type(pg),\n",
    "\n",
    "                    \"station\": station,\n",
    "                    \"remark\": remark,\n",
    "                    \"shift_name\": shift_name,\n",
    "                    \"shift_date\": shift_date,\n",
    "\n",
    "                    \"step_description\": step_desc,\n",
    "                    \"step_description_nn\": step_desc_nn,\n",
    "\n",
    "                    \"problem_group\": pg if pg in (\"sparepart\",\"equipment\") else \"equipment\",\n",
    "\n",
    "                    \"metric_fail_streak\": int(0),\n",
    "                    \"metric_fail_rate\": float(rate),\n",
    "                    \"promo_count\": int(promo_count),\n",
    "\n",
    "                    \"base_key\": base_key,\n",
    "                    \"trig_rule\": trig_rule,\n",
    "                    \"a_consec\": int(a_consec),\n",
    "                    \"b_fail_rate\": float(b_fail_rate),\n",
    "                    \"recurrence_count\": int(recurrence_count),\n",
    "                    \"created_at\": created_at,\n",
    "                })\n",
    "                total_events += 1\n",
    "                last_trig_ts = ts\n",
    "\n",
    "            b_state[key] = (int(total_cnt), int(fail_cnt), last_trig_ts)\n",
    "\n",
    "        _insert_events(engine, ev_rows)\n",
    "\n",
    "        total_rows += len(df)\n",
    "        last_ts = pd.to_datetime(df.iloc[-1][\"end_ts\"])\n",
    "        last_gi = int(df.iloc[-1][\"grp_i\"])\n",
    "        cursor_ts, cursor_gi = last_ts, last_gi\n",
    "        save_state(engine, STATE_KEY, CursorState(end_ts=cursor_ts, grp_i=cursor_gi))\n",
    "\n",
    "        print(f\"[BATCH] rows={len(df):,} | new_events={len(ev_rows):,} | cursor=({cursor_ts}, {cursor_gi}) | elapsed={_fmt_sec((_now()-t_start).total_seconds())}\")\n",
    "\n",
    "    print(f\"[DONE] event_stream_v2 build | total_rows={total_rows:,} | total_events={total_events:,} | elapsed={_fmt_sec((_now()-t_start).total_seconds())}\")\n",
    "\n",
    "# =========================\n",
    "# (8) Load events for labeling\n",
    "# =========================\n",
    "def load_event_stream(engine: Engine, trigger_type: str=\"B\", pg_filter: Optional[str]=\"sparepart\") -> pd.DataFrame:\n",
    "    q = f\"\"\"\n",
    "    SELECT event_ts, trigger_type, level, station, remark, problem_value, problem_group, step_description_nn\n",
    "    FROM \"{EVT_SCHEMA}\".\"{EVT_TABLE}\"\n",
    "    WHERE event_ts >= :t0 AND event_ts < :t1\n",
    "      AND trigger_type = :tt\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(\n",
    "            text(q), conn,\n",
    "            params={\"t0\": DATE_FROM.to_pydatetime(), \"t1\": (DATE_TO + pd.Timedelta(days=1)).to_pydatetime(), \"tt\": trigger_type}\n",
    "        )\n",
    "    df[\"event_ts\"] = pd.to_datetime(df[\"event_ts\"], errors=\"coerce\")\n",
    "    df[\"station\"] = df[\"station\"].astype(str)\n",
    "    df[\"remark\"] = df[\"remark\"].astype(str)\n",
    "    if \"problem_group\" in df.columns:\n",
    "        df[\"problem_group\"] = df[\"problem_group\"].astype(str).str.lower()\n",
    "    if pg_filter is not None and \"problem_group\" in df.columns:\n",
    "        df = df[df[\"problem_group\"] == str(pg_filter).lower()].copy()\n",
    "    return df.dropna(subset=[\"event_ts\"])\n",
    "\n",
    "def make_y_future_from_events(df_all: pd.DataFrame, ev_df: pd.DataFrame, future_hours: int) -> pd.Series:\n",
    "    if len(df_all) == 0:\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "    df_all2 = df_all.copy()\n",
    "    df_all2[\"end_ts\"] = pd.to_datetime(df_all2[\"end_ts\"], errors=\"coerce\")\n",
    "    df_all2 = df_all2.dropna(subset=[\"end_ts\"]).copy()\n",
    "\n",
    "    df_all2[\"station\"] = df_all2[\"station\"].astype(str)\n",
    "    df_all2[\"remark\"]  = df_all2[\"remark\"].astype(str)\n",
    "\n",
    "    groups = {}\n",
    "    for (st, rm), g in ev_df.groupby([\"station\", \"remark\"], dropna=False):\n",
    "        ev_ns = pd.to_datetime(g[\"event_ts\"], errors=\"coerce\").dropna().values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "        ev_ns.sort()\n",
    "        groups[(str(st), str(rm))] = ev_ns\n",
    "\n",
    "    horizon = int(future_hours) * 3600 * 1_000_000_000  # ns\n",
    "    y = np.zeros(len(df_all2), dtype=np.int8)\n",
    "\n",
    "    end_ns = df_all2[\"end_ts\"].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "\n",
    "    for i in range(len(df_all2)):\n",
    "        arr = groups.get((df_all2.iloc[i][\"station\"], df_all2.iloc[i][\"remark\"]))\n",
    "        if arr is None or arr.size == 0:\n",
    "            continue\n",
    "        t0 = end_ns[i]\n",
    "        t1 = t0 + horizon\n",
    "        j = np.searchsorted(arr, t0, side=\"left\")\n",
    "        if j < arr.size and arr[j] <= t1:\n",
    "            y[i] = 1\n",
    "\n",
    "    out = pd.Series(0, index=df_all.index, dtype=np.int8)\n",
    "    out.loc[df_all2.index] = y\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# (9) Split\n",
    "# =========================\n",
    "def split_by_days_auto(df: pd.DataFrame, val_days: int, test_days: int, min_train: int):\n",
    "    days = sorted(pd.to_datetime(df[\"end_day\"]).dt.date.unique())\n",
    "    need_min = val_days + test_days + min_train\n",
    "    if len(days) < need_min:\n",
    "        raise ValueError(f\"[SPLIT FAIL] unique_days={len(days)} < (min_train+val+test)={need_min}\")\n",
    "\n",
    "    train_days = len(days) - (val_days + test_days)\n",
    "    train_days = max(min_train, train_days)\n",
    "\n",
    "    train_set = set(days[:train_days])\n",
    "    val_set   = set(days[train_days:train_days+val_days])\n",
    "    test_set  = set(days[train_days+val_days:train_days+val_days+test_days])\n",
    "\n",
    "    tr = df[pd.to_datetime(df[\"end_day\"]).dt.date.isin(train_set)].copy()\n",
    "    va = df[pd.to_datetime(df[\"end_day\"]).dt.date.isin(val_set)].copy()\n",
    "    te = df[pd.to_datetime(df[\"end_day\"]).dt.date.isin(test_set)].copy()\n",
    "\n",
    "    info = {\n",
    "        \"unique_days\": len(days),\n",
    "        \"train_days\": train_days,\n",
    "        \"val_days\": val_days,\n",
    "        \"test_days\": test_days,\n",
    "        \"train_n\": len(tr),\n",
    "        \"val_n\": len(va),\n",
    "        \"test_n\": len(te),\n",
    "        \"train_pos\": int(tr[\"y_future\"].sum()),\n",
    "        \"val_pos\": int(va[\"y_future\"].sum()),\n",
    "        \"test_pos\": int(te[\"y_future\"].sum()),\n",
    "    }\n",
    "    return tr, va, te, info\n",
    "\n",
    "# =========================\n",
    "# (10) Track A 수명 테이블 로드 + life_ratio 산출\n",
    "# =========================\n",
    "def load_trackA_life(engine: Engine) -> pd.DataFrame:\n",
    "    cols = _get_cols(engine, LIFE_SCHEMA, LIFE_TABLE)\n",
    "    cols_l = [c.lower() for c in cols]\n",
    "\n",
    "    def pick(*cands):\n",
    "        for cand in cands:\n",
    "            if cand.lower() in cols_l:\n",
    "                return cols[cols_l.index(cand.lower())]\n",
    "        return None\n",
    "\n",
    "    c_station = pick(\"station\", \"station_name\", \"line\", \"equip\", \"equipment\", \"machine\")\n",
    "    c_sp      = pick(\"sparepart\", \"spare_part\", \"part\", \"part_name\")\n",
    "    c_p25     = pick(\"p25\", \"p_25\", \"life_p25\", \"life_amount_p25\", \"allow_cycle_p25\")\n",
    "    c_cc      = pick(\"cycle_count_since_last_change\", \"cycle_count\", \"cycle_cnt\", \"count_since_change\", \"life_cycle_count\")\n",
    "\n",
    "    sel_cols = [c for c in [c_station, c_sp, c_p25, c_cc] if c is not None]\n",
    "    if not sel_cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    q = f'SELECT {\", \".join([f\"\"\"\\\"{c}\\\"\"\"\" for c in sel_cols])} FROM \"{LIFE_SCHEMA}\".\"{LIFE_TABLE}\"'\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(text(q), conn)\n",
    "\n",
    "    if c_station is not None:\n",
    "        df[\"station__life\"] = df[c_station].astype(str)\n",
    "    else:\n",
    "        df[\"station__life\"] = \"\"\n",
    "\n",
    "    if c_sp is not None:\n",
    "        df[\"sparepart__life\"] = df[c_sp].astype(str).str.lower()\n",
    "    else:\n",
    "        df[\"sparepart__life\"] = \"\"\n",
    "\n",
    "    if c_p25 is not None:\n",
    "        df[\"p25__life\"] = pd.to_numeric(df[c_p25], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"p25__life\"] = np.nan\n",
    "\n",
    "    if c_cc is not None:\n",
    "        df[\"cycle__life\"] = pd.to_numeric(df[c_cc], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"cycle__life\"] = np.nan\n",
    "\n",
    "    df = df.dropna(subset=[\"p25__life\"]).copy()\n",
    "    df[\"life_ratio\"] = df[\"cycle__life\"] / df[\"p25__life\"]\n",
    "    df[\"life_ratio\"] = df[\"life_ratio\"].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    agg = df.groupby(\"station__life\").agg(\n",
    "        life_ratio_max=(\"life_ratio\", \"max\"),\n",
    "        life_ratio_min=(\"life_ratio\", \"min\"),\n",
    "        life_ratio_mean=(\"life_ratio\", \"mean\"),\n",
    "        p25_min=(\"p25__life\", \"min\"),\n",
    "        p25_max=(\"p25__life\", \"max\"),\n",
    "    ).reset_index().rename(columns={\"station__life\":\"station\"})\n",
    "\n",
    "    return agg\n",
    "\n",
    "def attach_life_ratio(df_all: pd.DataFrame, life_station_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df_all.copy()\n",
    "    out[\"station\"] = out[\"station\"].astype(str)\n",
    "    if life_station_df is None or len(life_station_df) == 0:\n",
    "        out[\"life_ratio_max\"] = 0.0\n",
    "        return out\n",
    "    out = out.merge(life_station_df[[\"station\",\"life_ratio_max\"]], on=\"station\", how=\"left\")\n",
    "    out[\"life_ratio_max\"] = pd.to_numeric(out[\"life_ratio_max\"], errors=\"coerce\").fillna(0.0)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# (11) 정책 적용 평가 (수명가중 임계값)\n",
    "# =========================\n",
    "def eval_with_life_weighted_threshold(df_split: pd.DataFrame, prob: np.ndarray, life_ratio_col: str=\"life_ratio_max\"):\n",
    "    y = df_split[\"y_future\"].astype(int).values\n",
    "    life = pd.to_numeric(df_split.get(life_ratio_col, 0.0), errors=\"coerce\").fillna(0.0).values\n",
    "\n",
    "    thr_vec = np.array([min_prob_by_life_ratio(r) for r in life], dtype=float)\n",
    "    yhat = (np.asarray(prob, dtype=float) >= thr_vec).astype(int)\n",
    "\n",
    "    tp = int(((y==1) & (yhat==1)).sum())\n",
    "    fp = int(((y==0) & (yhat==1)).sum())\n",
    "    tn = int(((y==0) & (yhat==0)).sum())\n",
    "    fn = int(((y==1) & (yhat==0)).sum())\n",
    "    prec, rec, f1 = _prf(tp, fp, fn)\n",
    "\n",
    "    alarms_total = int(tp + fp)\n",
    "    split_days = float(pd.to_datetime(df_split[\"end_day\"]).dt.date.nunique())\n",
    "    hours = split_days * 24.0\n",
    "    alarms_per_hour = alarms_total / hours if hours > 0 else np.nan\n",
    "    alarms_per_day  = alarms_total / split_days if split_days > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"precision\": prec, \"recall\": rec, \"f1\": f1,\n",
    "        \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "        \"alarms_total\": alarms_total,\n",
    "        \"alarms/hour\": float(alarms_per_hour),\n",
    "        \"alarms/day\": float(alarms_per_day),\n",
    "        \"split_days\": split_days,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# (12) MAIN: CELL10 실행\n",
    "# =========================\n",
    "def run_cell10_trackA_weighted(engine: Engine):\n",
    "    print(\"=\"*110)\n",
    "    print(\"[CELL10 v3] B-rule LGBM + TrackA(수명가중 임계값) | FH=1\")\n",
    "    print(f\"DATE_FROM={DATE_FROM} | DATE_TO={DATE_TO} | FETCH_LIMIT={FETCH_LIMIT:,}\")\n",
    "    print(f\"FORCE_REBUILD_EVENT_STREAM={FORCE_REBUILD_EVENT_STREAM} | SILENT_EXCLUDE_EQUIPMENT_LV2P={SILENT_EXCLUDE_EQUIPMENT_LV2P}\")\n",
    "    print(f\"LIFE_THR_RULES={LIFE_THR_RULES}\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    # 1) build events\n",
    "    t0 = _now()\n",
    "    build_event_stream_cursor_incremental(engine)\n",
    "    print(f\"[EVENT_STREAM] build done | elapsed={_fmt_sec((_now()-t0).total_seconds())}\")\n",
    "\n",
    "    # 2) load feature_store\n",
    "    q_fs = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM \"{FS_SCHEMA}\".\"{FS_TABLE}\"\n",
    "    WHERE end_day::date >= :d0 AND end_day::date <= :d1\n",
    "    \"\"\"\n",
    "    t1 = _now()\n",
    "    with engine.connect() as conn:\n",
    "        df_all = pd.read_sql(text(q_fs), conn, params={\"d0\": DATE_FROM.date(), \"d1\": DATE_TO.date()})\n",
    "\n",
    "    # end_ts 정규화\n",
    "    if \"end_ts\" not in df_all.columns:\n",
    "        df_all[\"end_ts\"] = pd.to_datetime(df_all[\"end_day\"], errors=\"coerce\") + pd.to_timedelta(df_all[\"end_time\"].astype(str), errors=\"coerce\")\n",
    "    else:\n",
    "        df_all[\"end_ts\"] = pd.to_datetime(df_all[\"end_ts\"], errors=\"coerce\")\n",
    "\n",
    "    df_all = df_all.dropna(subset=[\"end_ts\"]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"[LOAD] feature_store rows={len(df_all):,} cols={df_all.shape[1]} | elapsed={_fmt_sec((_now()-t1).total_seconds())}\")\n",
    "\n",
    "    # 3) load events for labeling (B + sparepart)\n",
    "    ev_df = load_event_stream(engine, trigger_type=\"B\", pg_filter=\"sparepart\")\n",
    "    print(f\"[LOAD] events rows={len(ev_df):,} | label_trigger=B | pg_filter=sparepart\")\n",
    "\n",
    "    # 4) Track A life load + attach\n",
    "    life_station_df = load_trackA_life(engine)\n",
    "    df_all = attach_life_ratio(df_all, life_station_df)\n",
    "    print(f\"[TRACK-A] life_station rows={len(life_station_df):,} | attached col=life_ratio_max\")\n",
    "\n",
    "    # 5) feature columns\n",
    "    drop_cols = {\"y\",\"y_future\",\"end_day\",\"end_time\",\"end_ts\",\"station\",\"remark\",\"shift_name\",\"shift_date\",\"barcode_information\",\"grp\",\"group\",\"bin\"}\n",
    "    feature_cols = [c for c in df_all.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(df_all[c])]\n",
    "    if \"life_ratio_max\" in df_all.columns and \"life_ratio_max\" not in feature_cols:\n",
    "        feature_cols.append(\"life_ratio_max\")\n",
    "\n",
    "    print(f\"[FEATURE] used={len(feature_cols)}\")\n",
    "    if len(feature_cols):\n",
    "        print(\"[FEATURE] first =\", feature_cols[:12])\n",
    "\n",
    "    # 6) label 생성 (FH=1)\n",
    "    df_all2 = df_all.copy()\n",
    "    df_all2[\"y_future\"] = make_y_future_from_events(df_all2, ev_df, future_hours=FH).astype(int)\n",
    "    y_mean = float(df_all2[\"y_future\"].mean())\n",
    "    print(f\"[LABEL] FH={FH} y_mean={y_mean:.6f}\")\n",
    "\n",
    "    # 7) split\n",
    "    tr, va, te, info = split_by_days_auto(df_all2, val_days=VAL_DAYS, test_days=TEST_DAYS, min_train=MIN_TRAIN_DAYS)\n",
    "    print(\"[SPLIT]\", info)\n",
    "\n",
    "    def XY(d):\n",
    "        X = d[feature_cols].fillna(0.0)\n",
    "        y = d[\"y_future\"].astype(int)\n",
    "        return X, y\n",
    "\n",
    "    Xtr, ytr = XY(tr)\n",
    "    Xva, yva = XY(va)\n",
    "    Xte, yte = XY(te)\n",
    "\n",
    "    if int(ytr.sum()) < 50:\n",
    "        raise RuntimeError(f\"train_pos too small: {int(ytr.sum())}\")\n",
    "\n",
    "    # 8) train LGBM\n",
    "    print(\"=\"*110)\n",
    "    print(\"[TRAIN] LightGBM (B-rule)\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    dtr = lgb.Dataset(Xtr, label=ytr)\n",
    "    dva = lgb.Dataset(Xva, label=yva, reference=dtr)\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"binary\",\n",
    "        metric=\"auc\",\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=63,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.9,\n",
    "        bagging_freq=1,\n",
    "        seed=42,\n",
    "        verbosity=-1,\n",
    "    )\n",
    "\n",
    "    # --- [ADD] evals_result 기록 ---\n",
    "    evals_result = {}\n",
    "\n",
    "    t2 = _now()\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtr,\n",
    "        valid_sets=[dtr, dva],\n",
    "        valid_names=[\"train\",\"val\"],\n",
    "        num_boost_round=2000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50, verbose=True),\n",
    "            lgb.record_evaluation(evals_result),\n",
    "        ]\n",
    "    )\n",
    "    print(f\"[TRAIN] done | best_iter={int(model.best_iteration)} | elapsed={_fmt_sec((_now()-t2).total_seconds())}\")\n",
    "\n",
    "    pva = model.predict(Xva, num_iteration=model.best_iteration)\n",
    "    pte = model.predict(Xte, num_iteration=model.best_iteration)\n",
    "\n",
    "    # 9) 기본 지표(AUC/PR)\n",
    "    val_auc = _safe_auc(yva, pva)\n",
    "    test_auc = _safe_auc(yte, pte)\n",
    "    val_pr  = average_precision_score(yva, pva) if yva.sum() > 0 else np.nan\n",
    "    test_pr = average_precision_score(yte, pte) if yte.sum() > 0 else np.nan\n",
    "\n",
    "    print(f\"[METRIC] AUC val={val_auc:.6f} | test={test_auc:.6f}\")\n",
    "    print(f\"[METRIC] PR  val={val_pr:.6f} | test={test_pr:.6f}\")\n",
    "\n",
    "    # --- [ADD] AUC 그래프 출력 (seaborn styled) ---\n",
    "    plot_auc_learning_curve(evals_result, best_iter=int(model.best_iteration))\n",
    "    plot_roc_curves(yva.values, pva, yte.values, pte, float(val_auc), float(test_auc))\n",
    "\n",
    "    # 10) 정책 평가: 수명가중 임계값 적용\n",
    "    print(\"=\"*110)\n",
    "    print(\"[POLICY EVAL] life_weighted min_prob (row-wise thr)\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    out_va = eval_with_life_weighted_threshold(va, pva, life_ratio_col=\"life_ratio_max\")\n",
    "    out_te = eval_with_life_weighted_threshold(te, pte, life_ratio_col=\"life_ratio_max\")\n",
    "\n",
    "    print(\"[VAL ]\",\n",
    "          f\"P/R/F1={out_va['precision']:.4f}/{out_va['recall']:.4f}/{out_va['f1']:.4f}\",\n",
    "          f\"| CM tn={out_va['tn']:,} fp={out_va['fp']:,} fn={out_va['fn']:,} tp={out_va['tp']:,}\",\n",
    "          f\"| alarms_total={out_va['alarms_total']:,} alarms/hour={out_va['alarms/hour']:.3f} alarms/day={out_va['alarms/day']:.1f}\")\n",
    "\n",
    "    print(\"[TEST]\",\n",
    "          f\"P/R/F1={out_te['precision']:.4f}/{out_te['recall']:.4f}/{out_te['f1']:.4f}\",\n",
    "          f\"| CM tn={out_te['tn']:,} fp={out_te['fp']:,} fn={out_te['fn']:,} tp={out_te['tp']:,}\",\n",
    "          f\"| alarms_total={out_te['alarms_total']:,} alarms/hour={out_te['alarms/hour']:.3f} alarms/day={out_te['alarms/day']:.1f}\")\n",
    "\n",
    "    # 11) 요약 테이블\n",
    "    summary = pd.DataFrame([{\n",
    "        \"FH\": FH,\n",
    "        \"y_mean\": y_mean,\n",
    "        \"val_auc\": float(val_auc) if val_auc==val_auc else np.nan,\n",
    "        \"test_auc\": float(test_auc) if test_auc==test_auc else np.nan,\n",
    "        \"val_pr_auc\": float(val_pr) if val_pr==val_pr else np.nan,\n",
    "        \"test_pr_auc\": float(test_pr) if test_pr==test_pr else np.nan,\n",
    "\n",
    "        \"VAL_precision\": out_va[\"precision\"],\n",
    "        \"VAL_recall\": out_va[\"recall\"],\n",
    "        \"VAL_f1\": out_va[\"f1\"],\n",
    "        \"VAL_alarms/hour\": out_va[\"alarms/hour\"],\n",
    "        \"VAL_alarms/day\": out_va[\"alarms/day\"],\n",
    "\n",
    "        \"TEST_precision\": out_te[\"precision\"],\n",
    "        \"TEST_recall\": out_te[\"recall\"],\n",
    "        \"TEST_f1\": out_te[\"f1\"],\n",
    "        \"TEST_alarms/hour\": out_te[\"alarms/hour\"],\n",
    "        \"TEST_alarms/day\": out_te[\"alarms/day\"],\n",
    "    }])\n",
    "\n",
    "    print(\"\\n[SUMMARY]\")\n",
    "    display(summary)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"life_station_df\": life_station_df,\n",
    "        \"summary\": summary,\n",
    "        \"split_info\": info,\n",
    "        \"evals_result\": evals_result,  # [ADD] 학습곡선 원자료 반환\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# EXEC\n",
    "# =========================\n",
    "res = run_cell10_trackA_weighted(ENGINE)"
   ],
   "id": "31874fde94235a67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =========================\n",
    "# [CELL] SAVE 통합(레거시 노트북 호환)\n",
    "# - res 없어도 globals에서 LGBM 모델 자동 탐색\n",
    "# - 서버 무한 접속 재시도\n",
    "# - policy_json: {\"A\":\"사용 가능 횟수\",\"B\":\"상황별 임계값(row-wise)\"}\n",
    "# - uq_ml_model_one_active_per_target 회피: 기존 active False 후 insert\n",
    "# - updated_date unique 충돌 회피: python datetime.now()로 매번 갱신\n",
    "# =========================\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError, IntegrityError\n",
    "\n",
    "# -------------------------\n",
    "# 0) DB / TABLE\n",
    "# -------------------------\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"100.105.75.47\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"leejangwoo1!\",\n",
    "}\n",
    "ML_SCHEMA = \"h_machine_learning\"\n",
    "T_MODEL   = \"3_machine_learning_model\"\n",
    "TARGET_KEY = \"sparepart\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) ENGINE 무한 접속\n",
    "# -------------------------\n",
    "def _make_engine():\n",
    "    pw = urllib.parse.quote_plus(DB_CONFIG[\"password\"])\n",
    "    url = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{pw}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "    return create_engine(\n",
    "        url,\n",
    "        pool_pre_ping=True,\n",
    "        pool_recycle=30,\n",
    "        pool_size=5,\n",
    "        max_overflow=10,\n",
    "        pool_timeout=30,\n",
    "    )\n",
    "\n",
    "def get_engine_forever():\n",
    "    backoff = 1.0\n",
    "    while True:\n",
    "        try:\n",
    "            eng = _make_engine()\n",
    "            with eng.connect() as conn:\n",
    "                conn.execute(text(\"SELECT 1\"))\n",
    "            print(\"[OK] DB connection ready\")\n",
    "            return eng\n",
    "        except Exception as e:\n",
    "            print(f\"[RETRY] DB connect failed: {type(e).__name__}: {e}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff = min(backoff * 1.5, 30.0)\n",
    "\n",
    "ENGINE = get_engine_forever()\n",
    "\n",
    "# -------------------------\n",
    "# 2) globals에서 LightGBM 모델 자동 찾기\n",
    "# -------------------------\n",
    "def _is_lgbm_model(obj):\n",
    "    if obj is None:\n",
    "        return False\n",
    "    name = type(obj).__name__.lower()\n",
    "    mod  = type(obj).__module__.lower()\n",
    "    # Booster / LGBMClassifier / LGBMRegressor 등\n",
    "    if \"lightgbm\" in mod:\n",
    "        return True\n",
    "    if \"booster\" in name and hasattr(obj, \"predict\"):\n",
    "        return True\n",
    "    # sklearn wrapper인데 booster_를 갖는 경우\n",
    "    if hasattr(obj, \"booster_\") and hasattr(obj.booster_, \"predict\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def pick_model_from_globals(g):\n",
    "    # 1) 흔한 변수명 우선\n",
    "    for k in [\"model\", \"best_model\", \"lgbm_model\", \"booster\", \"clf\"]:\n",
    "        if k in g and _is_lgbm_model(g[k]):\n",
    "            return g[k], f\"globals['{k}']\"\n",
    "\n",
    "    # 2) models(dict/list) 구조 대응\n",
    "    if \"models\" in g:\n",
    "        m = g[\"models\"]\n",
    "        if isinstance(m, dict):\n",
    "            # value 중 첫번째 모델\n",
    "            for kk, vv in m.items():\n",
    "                if _is_lgbm_model(vv):\n",
    "                    return vv, f\"models['{kk}']\"\n",
    "        if isinstance(m, (list, tuple)):\n",
    "            for i, vv in enumerate(m):\n",
    "                if _is_lgbm_model(vv):\n",
    "                    return vv, f\"models[{i}]\"\n",
    "\n",
    "    # 3) globals 전체 스캔\n",
    "    for k, v in g.items():\n",
    "        if _is_lgbm_model(v):\n",
    "            return v, f\"globals['{k}']\"\n",
    "\n",
    "    return None, None\n",
    "\n",
    "model, model_src = pick_model_from_globals(globals())\n",
    "if model is None:\n",
    "    raise NameError(\n",
    "        \"[STOP] 현재 노트북 메모리(globals)에 LightGBM 모델 객체가 없습니다.\\n\"\n",
    "        \"=> 학습 셀(모델 만드는 셀)을 먼저 실행해서 model/models 변수가 생성되게 해주세요.\"\n",
    "    )\n",
    "\n",
    "print(f\"[OK] picked model from {model_src} | type={type(model)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) feature_cols 자동 확보\n",
    "# -------------------------\n",
    "def get_feature_cols(model):\n",
    "    # 1) 노트북에 feature_cols/FEATURE_COLS 있으면 그거 우선\n",
    "    for k in [\"feature_cols\", \"FEATURE_COLS\", \"FEATURES\", \"feature_columns\"]:\n",
    "        if k in globals() and isinstance(globals()[k], (list, tuple)) and len(globals()[k]) > 0:\n",
    "            return list(globals()[k]), f\"globals['{k}']\"\n",
    "\n",
    "    # 2) Booster.feature_name()\n",
    "    try:\n",
    "        if hasattr(model, \"feature_name\"):\n",
    "            cols = list(model.feature_name())\n",
    "            if cols:\n",
    "                return cols, \"model.feature_name()\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) wrapper.booster_.feature_name()\n",
    "    try:\n",
    "        if hasattr(model, \"booster_\") and hasattr(model.booster_, \"feature_name\"):\n",
    "            cols = list(model.booster_.feature_name())\n",
    "            if cols:\n",
    "                return cols, \"model.booster_.feature_name()\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return [], \"fallback([])\"\n",
    "\n",
    "feature_cols, feat_src = get_feature_cols(model)\n",
    "print(f\"[OK] feature_cols={len(feature_cols)} from {feat_src}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) auc_test / split info 자동 확보 (있으면 저장)\n",
    "# -------------------------\n",
    "def pick_first_number(*vals):\n",
    "    for v in vals:\n",
    "        if v is None:\n",
    "            continue\n",
    "        try:\n",
    "            fv = float(v)\n",
    "            if np.isfinite(fv):\n",
    "                return fv\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "test_auc = None\n",
    "val_auc = None\n",
    "\n",
    "# 흔한 변수명/데이터프레임에서 찾기\n",
    "for k in [\"test_auc\", \"AUC_TEST\", \"auc_test\"]:\n",
    "    if k in globals():\n",
    "        test_auc = pick_first_number(globals()[k], test_auc)\n",
    "\n",
    "for k in [\"val_auc\", \"AUC_VAL\", \"auc_val\"]:\n",
    "    if k in globals():\n",
    "        val_auc = pick_first_number(globals()[k], val_auc)\n",
    "\n",
    "# summary_df/res_df 같은 DF에서 test_auc 찾기\n",
    "for k in [\"summary_df\", \"res_df\", \"summary\", \"result_df\"]:\n",
    "    if k in globals() and isinstance(globals()[k], pd.DataFrame) and len(globals()[k]) > 0:\n",
    "        df = globals()[k]\n",
    "        if test_auc is None and \"test_auc\" in df.columns:\n",
    "            test_auc = pick_first_number(df.iloc[0][\"test_auc\"])\n",
    "        if val_auc is None and \"val_auc\" in df.columns:\n",
    "            val_auc = pick_first_number(df.iloc[0][\"val_auc\"])\n",
    "\n",
    "# -------------------------\n",
    "# 5) 메타(이름/버전) 자동 구성 (원하면 여기만 수정)\n",
    "# -------------------------\n",
    "FH = int(globals().get(\"FH\", 1))\n",
    "DATE_FROM = globals().get(\"DATE_FROM\", None)\n",
    "DATE_TO   = globals().get(\"DATE_TO\", None)\n",
    "\n",
    "def _to_date(x):\n",
    "    try:\n",
    "        return pd.to_datetime(x).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "date_from = _to_date(DATE_FROM) if DATE_FROM is not None else None\n",
    "date_to   = _to_date(DATE_TO) if DATE_TO is not None else None\n",
    "\n",
    "model_name = globals().get(\"model_name\", None) or f\"CELL10v3_B_rule_LGBM_FH{FH}\"\n",
    "if date_from and date_to:\n",
    "    model_version = globals().get(\"model_version\", None) or f\"{date_from}_{date_to}_v3\"\n",
    "else:\n",
    "    model_version = globals().get(\"model_version\", None) or f\"v3_FH{FH}\"\n",
    "\n",
    "# best_iteration\n",
    "def get_best_iter(model):\n",
    "    for attr in [\"best_iteration_\", \"best_iteration\"]:\n",
    "        if hasattr(model, attr):\n",
    "            try:\n",
    "                v = getattr(model, attr)\n",
    "                if v:\n",
    "                    return int(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        if hasattr(model, \"current_iteration\"):\n",
    "            return int(model.current_iteration())\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if hasattr(model, \"booster_\") and hasattr(model.booster_, \"current_iteration\"):\n",
    "            return int(model.booster_.current_iteration())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "best_iteration = get_best_iter(model)\n",
    "print(f\"[INFO] best_iteration={best_iteration}, auc_test={test_auc}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) policy_json (요청 반영)\n",
    "# -------------------------\n",
    "policy_json_obj = {\"A\": \"사용 가능 횟수\", \"B\": \"상황별 임계값(row-wise)\"}\n",
    "\n",
    "# label_config / metrics / train_config (최소)\n",
    "label_config_obj = globals().get(\"label_config\", None)\n",
    "if not isinstance(label_config_obj, dict):\n",
    "    label_config_obj = {\"future_hours\": FH, \"label_trigger\": \"B\", \"problem_group_filter\": \"sparepart\"}\n",
    "\n",
    "train_config_obj = {\"best_iteration\": best_iteration}\n",
    "metrics_obj = {\"val_auc\": val_auc, \"test_auc\": test_auc}\n",
    "\n",
    "# -------------------------\n",
    "# 7) blob + sha256\n",
    "# -------------------------\n",
    "model_blob = pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "model_sha256 = hashlib.sha256(model_blob).hexdigest()\n",
    "\n",
    "# -------------------------\n",
    "# 8) 테이블 컬럼 조회 후, 있는 컬럼만 INSERT\n",
    "# -------------------------\n",
    "def get_table_cols(engine, schema, table):\n",
    "    q = text(\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = :s AND table_name = :t\n",
    "        ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    with engine.connect() as c:\n",
    "        rows = c.execute(q, {\"s\": schema, \"t\": table}).fetchall()\n",
    "    return [r[0] for r in rows]\n",
    "\n",
    "cols = get_table_cols(ENGINE, ML_SCHEMA, T_MODEL)\n",
    "cols_set = set(cols)\n",
    "print(f\"[INFO] {ML_SCHEMA}.{T_MODEL} columns={len(cols)}\")\n",
    "\n",
    "# json dumps\n",
    "feature_cols_json = json.dumps(list(feature_cols), ensure_ascii=False)\n",
    "label_config_json = json.dumps(label_config_obj, ensure_ascii=False)\n",
    "policy_json_str   = json.dumps(policy_json_obj, ensure_ascii=False)\n",
    "metrics_json_str  = json.dumps(metrics_obj, ensure_ascii=False)\n",
    "train_config_json = json.dumps(train_config_obj, ensure_ascii=False)\n",
    "\n",
    "# split days (있으면)\n",
    "split_info = globals().get(\"split_info\", {}) if isinstance(globals().get(\"split_info\", {}), dict) else {}\n",
    "data_days  = split_info.get(\"unique_days\", None) or split_info.get(\"data_days\", None)\n",
    "train_days = split_info.get(\"train_days\", None)\n",
    "val_days   = split_info.get(\"val_days\", None)\n",
    "test_days  = split_info.get(\"test_days\", None)\n",
    "\n",
    "params_all = {\n",
    "    \"updated_date\": datetime.now(),\n",
    "    \"created_at\": datetime.now(),\n",
    "    \"updated_at\": datetime.now(),\n",
    "    \"model_blob\": model_blob,\n",
    "    \"model_type\": \"lightgbm\",\n",
    "    \"best_iteration\": int(best_iteration) if best_iteration is not None else None,\n",
    "    \"auc_test\": float(test_auc) if test_auc is not None else None,\n",
    "    \"model_name\": str(model_name),\n",
    "    \"model_version\": str(model_version),\n",
    "    \"target_key\": str(TARGET_KEY),\n",
    "    \"is_active\": True,\n",
    "    \"feature_cols_json\": feature_cols_json,\n",
    "    \"label_config_json\": label_config_json,\n",
    "    \"policy_json\": policy_json_str,\n",
    "    \"metrics_json\": metrics_json_str,\n",
    "    \"train_config_json\": train_config_json,\n",
    "    \"date_from\": date_from,\n",
    "    \"date_to\": date_to,\n",
    "    \"data_days\": int(data_days) if data_days is not None else None,\n",
    "    \"train_days\": int(train_days) if train_days is not None else None,\n",
    "    \"val_days\": int(val_days) if val_days is not None else None,\n",
    "    \"test_days\": int(test_days) if test_days is not None else None,\n",
    "    \"model_sha256\": model_sha256,\n",
    "}\n",
    "\n",
    "insert_cols = []\n",
    "values_expr = []\n",
    "bind_params = {}\n",
    "\n",
    "def add_col(col, expr=None):\n",
    "    if col in cols_set:\n",
    "        insert_cols.append(col)\n",
    "        values_expr.append(expr if expr is not None else f\":{col}\")\n",
    "        bind_params[col] = params_all[col]\n",
    "\n",
    "# base\n",
    "add_col(\"updated_date\")\n",
    "add_col(\"model_blob\")\n",
    "add_col(\"model_type\")\n",
    "add_col(\"best_iteration\")\n",
    "add_col(\"auc_test\")\n",
    "add_col(\"model_name\")\n",
    "add_col(\"model_version\")\n",
    "add_col(\"target_key\")\n",
    "add_col(\"is_active\")\n",
    "\n",
    "# jsonb cast\n",
    "if \"feature_cols_json\" in cols_set:\n",
    "    insert_cols.append(\"feature_cols_json\")\n",
    "    values_expr.append(\"CAST(:feature_cols_json AS jsonb)\")\n",
    "    bind_params[\"feature_cols_json\"] = params_all[\"feature_cols_json\"]\n",
    "if \"label_config_json\" in cols_set:\n",
    "    insert_cols.append(\"label_config_json\")\n",
    "    values_expr.append(\"CAST(:label_config_json AS jsonb)\")\n",
    "    bind_params[\"label_config_json\"] = params_all[\"label_config_json\"]\n",
    "if \"policy_json\" in cols_set:\n",
    "    insert_cols.append(\"policy_json\")\n",
    "    values_expr.append(\"CAST(:policy_json AS jsonb)\")\n",
    "    bind_params[\"policy_json\"] = params_all[\"policy_json\"]\n",
    "if \"metrics_json\" in cols_set:\n",
    "    insert_cols.append(\"metrics_json\")\n",
    "    values_expr.append(\"CAST(:metrics_json AS jsonb)\")\n",
    "    bind_params[\"metrics_json\"] = params_all[\"metrics_json\"]\n",
    "if \"train_config_json\" in cols_set:\n",
    "    insert_cols.append(\"train_config_json\")\n",
    "    values_expr.append(\"CAST(:train_config_json AS jsonb)\")\n",
    "    bind_params[\"train_config_json\"] = params_all[\"train_config_json\"]\n",
    "\n",
    "# range/split\n",
    "add_col(\"date_from\")\n",
    "add_col(\"date_to\")\n",
    "add_col(\"data_days\")\n",
    "add_col(\"train_days\")\n",
    "add_col(\"val_days\")\n",
    "add_col(\"test_days\")\n",
    "add_col(\"model_sha256\")\n",
    "\n",
    "# timestamps\n",
    "add_col(\"created_at\")\n",
    "add_col(\"updated_at\")\n",
    "\n",
    "returning_col = \"id\" if \"id\" in cols_set else (\"updated_date\" if \"updated_date\" in cols_set else None)\n",
    "\n",
    "q_deactivate = text(f\"\"\"\n",
    "UPDATE \"{ML_SCHEMA}\".\"{T_MODEL}\"\n",
    "SET is_active = FALSE,\n",
    "    updated_at = NOW()\n",
    "WHERE target_key = :target_key\n",
    "  AND is_active = TRUE\n",
    "\"\"\")\n",
    "\n",
    "q_insert = text(f\"\"\"\n",
    "INSERT INTO \"{ML_SCHEMA}\".\"{T_MODEL}\"\n",
    "({\", \".join([f'\"{c}\"' for c in insert_cols])})\n",
    "VALUES\n",
    "({\", \".join(values_expr)})\n",
    "{f'RETURNING \"{returning_col}\"' if returning_col else \"\"}\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------\n",
    "# 9) 저장 실행 (무한 재시도)\n",
    "# -------------------------\n",
    "backoff = 1.0\n",
    "while True:\n",
    "    try:\n",
    "        with ENGINE.begin() as conn:\n",
    "            # 기존 active off (target_key당 active 1개 제약 회피)\n",
    "            res_up = conn.execute(q_deactivate, {\"target_key\": TARGET_KEY})\n",
    "            print(f\"[OK] deactivated active models: {res_up.rowcount} rows (target_key={TARGET_KEY})\")\n",
    "\n",
    "            # updated_date unique 충돌 회피: 매번 새 시간\n",
    "            bind_params[\"updated_date\"] = datetime.now()\n",
    "            bind_params[\"created_at\"] = datetime.now()\n",
    "            bind_params[\"updated_at\"] = datetime.now()\n",
    "\n",
    "            ret = conn.execute(q_insert, bind_params)\n",
    "            key = ret.scalar() if returning_col else None\n",
    "\n",
    "        print(f\"[OK] saved model -> {ML_SCHEMA}.{T_MODEL} | {returning_col}={key}\")\n",
    "        print(f\"     model_name={model_name} | version={model_version} | auc_test={bind_params.get('auc_test')}\")\n",
    "        print(f\"     sha256={model_sha256[:16]}... | policy_json={policy_json_obj}\")\n",
    "        break\n",
    "\n",
    "    except OperationalError as e:\n",
    "        print(f\"[RETRY] OperationalError: {e}\")\n",
    "    except IntegrityError as e:\n",
    "        print(f\"[RETRY] IntegrityError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RETRY] Exception: {type(e).__name__}: {e}\")\n",
    "\n",
    "    time.sleep(backoff)\n",
    "    backoff = min(backoff * 1.5, 30.0)"
   ],
   "id": "529b912598644b90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d3dd721814a27a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
